"""
Enhanced GRPO (Group Relative Policy Optimization) Trainer
=========================================================

MS Swift CoZ GRPOë¥¼ ì°¸ì¡°í•˜ì—¬ êµ¬í˜„ëœ ê°œì„ ëœ GRPO íŠ¸ë ˆì´ë„ˆì…ë‹ˆë‹¤.
LoRA ë²„ì „ê³¼ ì „ì²´ í•™ìŠµ ë²„ì „ì„ ëª¨ë‘ ì§€ì›í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
1. LoRA/QLoRA ì§€ì› (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
2. ì „ì²´ íŒŒë¼ë¯¸í„° í•™ìŠµ ì§€ì› (Full Fine-tuning)
3. DeepSpeed ZeRO ì§€ì› (ëŒ€ê·œëª¨ ëª¨ë¸)
4. vLLM ê°€ì† ì§€ì›
5. ë‹¤ì¤‘ ë³´ìƒ í•¨ìˆ˜ ì§€ì›
6. ê³ ê¸‰ GRPO ì•Œê³ ë¦¬ì¦˜

Author: AI Assistant (Based on MS Swift CoZ GRPO)
Date: 2025-01-22
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.nn.utils import clip_grad_norm_
from typing import List, Dict, Tuple, Optional, Any, Union, Literal
import logging
import numpy as np
import copy
from dataclasses import dataclass, field
import json
from pathlib import Path
import warnings

# LoRA ê´€ë ¨ ì„í¬íŠ¸
try:
    from peft import (
        LoraConfig, 
        get_peft_model, 
        TaskType,
        PeftModel,
        prepare_model_for_kbit_training
    )
    PEFT_AVAILABLE = True
except ImportError:
    PEFT_AVAILABLE = False
    warnings.warn("PEFT not available. LoRA training will be disabled.")

# DeepSpeed ê´€ë ¨ ì„í¬íŠ¸
try:
    import deepspeed
    DEEPSPEED_AVAILABLE = True
except ImportError:
    DEEPSPEED_AVAILABLE = False
    warnings.warn("DeepSpeed not available. Multi-GPU training may be limited.")

logger = logging.getLogger(__name__)

@dataclass
class EnhancedGRPOConfig:
    """
    ê°œì„ ëœ GRPO í•™ìŠµ ì„¤ì • í´ë˜ìŠ¤
    
    MS Swiftì˜ ì„¤ì •ì„ ì°¸ì¡°í•˜ì—¬ LoRAì™€ ì „ì²´ í•™ìŠµì„ ëª¨ë‘ ì§€ì›í•©ë‹ˆë‹¤.
    """
    # ê¸°ë³¸ í•™ìŠµ ì„¤ì •
    learning_rate: float = 1e-5
    num_train_epochs: int = 1
    per_device_train_batch_size: int = 4
    per_device_eval_batch_size: int = 4
    gradient_accumulation_steps: int = 1
    max_grad_norm: float = 1.0
    warmup_ratio: float = 0.05
    
    # GRPO íŠ¹í™” ì„¤ì •
    group_size: int = 4
    num_generations: int = 4
    grpo_epochs: int = 2
    gamma: float = 0.99
    kl_beta: float = 0.01
    clip_epsilon: float = 0.2
    entropy_coeff: float = 0.01
    temperature: float = 0.9
    top_p: float = 0.9
    top_k: int = 50
    
    # í•™ìŠµ íƒ€ì… ì„¤ì • (MS Swift ìŠ¤íƒ€ì¼)
    train_type: Literal["full", "lora", "qlora"] = "lora"
    
    # LoRA ì„¤ì • (MS Swift ê¸°ë³¸ê°’)
    lora_rank: int = 8
    lora_alpha: int = 32
    lora_dropout: float = 0.1
    target_modules: Union[str, List[str]] = "all-linear"
    
    # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì„¤ì •
    model_name: str = "microsoft/DialoGPT-medium"
    torch_dtype: str = "bfloat16"
    
    # ë°ì´í„° ì„¤ì •
    max_length: int = 2048
    max_completion_length: int = 1024
    
    # í‰ê°€ ë° ì €ì¥ ì„¤ì •
    eval_steps: int = 100
    save_steps: int = 100
    save_total_limit: int = 2
    logging_steps: int = 5
    
    # ì¶œë ¥ ì„¤ì •
    output_dir: str = "output"
    log_completions: bool = True
    
    # DeepSpeed ì„¤ì •
    deepspeed: Optional[str] = None  # "zero2", "zero3" ë“±
    
    # vLLM ì„¤ì •
    use_vllm: bool = False
    vllm_gpu_memory_utilization: float = 0.7
    vllm_max_model_len: int = 4096
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device: str = "auto"
    
    # ë³´ìƒ í•¨ìˆ˜ ì„¤ì •
    reward_funcs: List[str] = field(default_factory=lambda: ["clip_similarity", "image_quality"])
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    system_prompt: Optional[str] = None

class EnhancedGRPOTrainer:
    """
    MS Swift CoZ GRPOë¥¼ ì°¸ì¡°í•œ ê°œì„ ëœ GRPO íŠ¸ë ˆì´ë„ˆ
    
    ì£¼ìš” ê°œì„ ì‚¬í•­:
    1. LoRA/QLoRA ì§€ì›ìœ¼ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ
    2. ì „ì²´ íŒŒë¼ë¯¸í„° í•™ìŠµ ì§€ì›
    3. DeepSpeed ZeRO ì§€ì›ìœ¼ë¡œ ëŒ€ê·œëª¨ ëª¨ë¸ í•™ìŠµ ê°€ëŠ¥
    4. vLLM ê°€ì† ì§€ì›
    5. ë‹¤ì¤‘ ë³´ìƒ í•¨ìˆ˜ ì§€ì›
    6. MS Swift ìŠ¤íƒ€ì¼ì˜ ì„¤ì • ì‹œìŠ¤í…œ
    """
    
    def __init__(self, 
                 vlm_model,
                 config: EnhancedGRPOConfig):
        """
        Enhanced GRPO Trainer ì´ˆê¸°í™”
        
        Args:
            vlm_model: í•™ìŠµí•  VLM ëª¨ë¸
            config (EnhancedGRPOConfig): ê°œì„ ëœ GRPO í•™ìŠµ ì„¤ì •
        """
        self.config = config
        self.original_model = vlm_model
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self._setup_device()
        
        # ëª¨ë¸ ì„¤ì • (LoRA ë˜ëŠ” ì „ì²´ í•™ìŠµ)
        self._setup_model()
        
        # ì°¸ì¡° ëª¨ë¸ ì„¤ì •
        self.vlm_ref = None
        
        # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
        self._setup_optimizer()
        
        # DeepSpeed ì„¤ì •
        if self.config.deepspeed and DEEPSPEED_AVAILABLE:
            self._setup_deepspeed()
        
        # í•™ìŠµ í†µê³„
        self.training_stats = {
            'iteration': 0,
            'epoch': 0,
            'total_samples': 0,
            'avg_reward': 0.0,
            'policy_loss': 0.0,
            'kl_divergence': 0.0,
            'entropy': 0.0,
            'grad_norm': 0.0
        }
        
        logger.info(f"ğŸš€ Enhanced GRPO Trainer initialized")
        logger.info(f"   - Train Type: {config.train_type}")
        logger.info(f"   - Model: {config.model_name}")
        logger.info(f"   - Device: {self.device}")
        if config.train_type in ["lora", "qlora"]:
            logger.info(f"   - LoRA Rank: {config.lora_rank}")
            logger.info(f"   - LoRA Alpha: {config.lora_alpha}")
    
    def _setup_device(self):
        """ë””ë°”ì´ìŠ¤ ì„¤ì •"""
        if self.config.device == "auto":
            if torch.backends.mps.is_available():
                self.device = torch.device("mps")
                logger.info("ğŸ Using Apple Silicon MPS")
            elif torch.cuda.is_available():
                self.device = torch.device("cuda")
                logger.info("ğŸš€ Using CUDA GPU")
            else:
                self.device = torch.device("cpu")
                logger.info("ğŸ’» Using CPU")
        else:
            self.device = torch.device(self.config.device)
    
    def _setup_model(self):
        """ëª¨ë¸ ì„¤ì • (LoRA ë˜ëŠ” ì „ì²´ í•™ìŠµ)"""
        if self.config.train_type == "full":
            # ì „ì²´ íŒŒë¼ë¯¸í„° í•™ìŠµ
            self.vlm = self.original_model.to(self.device)
            logger.info("ğŸ”¥ Full parameter training enabled")
            
        elif self.config.train_type in ["lora", "qlora"]:
            if not PEFT_AVAILABLE:
                raise ImportError("PEFT is required for LoRA training. Please install: pip install peft")
            
            # LoRA ì„¤ì •
            if isinstance(self.config.target_modules, str):
                if self.config.target_modules == "all-linear":
                    # MS Swift ìŠ¤íƒ€ì¼: ëª¨ë“  ì„ í˜• ë ˆì´ì–´ì— LoRA ì ìš©
                    target_modules = self._get_all_linear_modules()
                else:
                    target_modules = [self.config.target_modules]
            else:
                target_modules = self.config.target_modules
            
            lora_config = LoraConfig(
                task_type=TaskType.CAUSAL_LM,
                r=self.config.lora_rank,
                lora_alpha=self.config.lora_alpha,
                lora_dropout=self.config.lora_dropout,
                target_modules=target_modules,
                bias="none"
            )
            
            # QLoRAë¥¼ ìœ„í•œ ëª¨ë¸ ì¤€ë¹„ (ì–‘ìí™”ëœ ëª¨ë¸ì¸ ê²½ìš°)
            if self.config.train_type == "qlora":
                self.original_model = prepare_model_for_kbit_training(self.original_model)
            
            # LoRA ëª¨ë¸ ìƒì„±
            self.vlm = get_peft_model(self.original_model, lora_config)
            self.vlm = self.vlm.to(self.device)
            
            # í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ì¶œë ¥
            trainable_params = sum(p.numel() for p in self.vlm.parameters() if p.requires_grad)
            total_params = sum(p.numel() for p in self.vlm.parameters())
            
            logger.info(f"ğŸ¯ LoRA training enabled")
            logger.info(f"   - Trainable params: {trainable_params:,}")
            logger.info(f"   - Total params: {total_params:,}")
            logger.info(f"   - Trainable ratio: {100 * trainable_params / total_params:.2f}%")
        
        else:
            raise ValueError(f"Unsupported train_type: {self.config.train_type}")
    
    def _get_all_linear_modules(self) -> List[str]:
        """
        ëª¨ë“  ì„ í˜• ë ˆì´ì–´ ëª¨ë“ˆ ì´ë¦„ì„ ì°¾ëŠ” í•¨ìˆ˜
        MS Swiftì˜ "all-linear" ì˜µì…˜ì„ êµ¬í˜„
        """
        linear_modules = []
        for name, module in self.original_model.named_modules():
            if isinstance(module, nn.Linear):
                # ëª¨ë“ˆ ì´ë¦„ì—ì„œ ë§ˆì§€ë§‰ ë¶€ë¶„ë§Œ ì¶”ì¶œ
                module_name = name.split('.')[-1]
                if module_name not in linear_modules:
                    linear_modules.append(module_name)
        
        # ì¼ë°˜ì ì¸ Transformer ëª¨ë¸ì˜ ì„ í˜• ë ˆì´ì–´ë“¤
        common_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
        
        # ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” ëª¨ë“ˆë§Œ ë°˜í™˜
        return [module for module in common_modules if module in linear_modules] or linear_modules
    
    def _setup_optimizer(self):
        """ì˜µí‹°ë§ˆì´ì € ì„¤ì •"""
        # í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë§Œ ì„ íƒ
        trainable_params = [p for p in self.vlm.parameters() if p.requires_grad]
        
        self.optimizer = optim.AdamW(
            trainable_params,
            lr=self.config.learning_rate,
            weight_decay=0.01,
            betas=(0.9, 0.999),
            eps=1e-8
        )
        
        logger.info(f"ğŸ”§ Optimizer configured with {len(trainable_params)} trainable parameters")
    
    def _setup_deepspeed(self):
        """DeepSpeed ì„¤ì •"""
        deepspeed_config = {
            "train_batch_size": self.config.per_device_train_batch_size * self.config.gradient_accumulation_steps,
            "gradient_accumulation_steps": self.config.gradient_accumulation_steps,
            "optimizer": {
                "type": "AdamW",
                "params": {
                    "lr": self.config.learning_rate,
                    "weight_decay": 0.01
                }
            },
            "fp16": {
                "enabled": self.config.torch_dtype == "float16"
            },
            "bf16": {
                "enabled": self.config.torch_dtype == "bfloat16"
            }
        }
        
        # ZeRO ì„¤ì •
        if self.config.deepspeed == "zero2":
            deepspeed_config["zero_optimization"] = {
                "stage": 2,
                "allgather_partitions": True,
                "allgather_bucket_size": 2e8,
                "overlap_comm": True,
                "reduce_scatter": True,
                "reduce_bucket_size": 2e8,
                "contiguous_gradients": True
            }
        elif self.config.deepspeed == "zero3":
            deepspeed_config["zero_optimization"] = {
                "stage": 3,
                "offload_optimizer": {
                    "device": "cpu",
                    "pin_memory": True
                },
                "offload_param": {
                    "device": "cpu",
                    "pin_memory": True
                },
                "overlap_comm": True,
                "contiguous_gradients": True,
                "sub_group_size": 1e9,
                "reduce_bucket_size": "auto",
                "stage3_prefetch_bucket_size": "auto",
                "stage3_param_persistence_threshold": "auto",
                "stage3_max_live_parameters": 1e9,
                "stage3_max_reuse_distance": 1e9
            }
        
        logger.info(f"âš¡ DeepSpeed {self.config.deepspeed} configured")
    
    def enhance_prompts_batch(self, prompts: List[str]) -> Tuple[List[str], torch.Tensor]:
        """
        ë°°ì¹˜ í”„ë¡¬í”„íŠ¸ ê°œì„  ë° ë¡œê·¸ í™•ë¥  ê³„ì‚°
        
        MS Swiftì˜ GRPO êµ¬í˜„ì„ ì°¸ì¡°í•˜ì—¬ íš¨ìœ¨ì ì¸ ë°°ì¹˜ ì²˜ë¦¬ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.
        
        Args:
            prompts (List[str]): ì…ë ¥ í”„ë¡¬í”„íŠ¸ë“¤
            
        Returns:
            Tuple[List[str], torch.Tensor]: (ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ë“¤, ë¡œê·¸ í™•ë¥ ë“¤)
        """
        enhanced_prompts = []
        log_probs = []
        
        self.vlm.eval()
        
        with torch.no_grad():
            for prompt in prompts:
                try:
                    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì ìš©
                    if self.config.system_prompt:
                        full_prompt = f"{self.config.system_prompt}\n\nUser: {prompt}\nAssistant:"
                    else:
                        full_prompt = f"Improve this prompt for better image generation: {prompt}\nImproved prompt:"
                    
                    # í† í¬ë‚˜ì´ì§•
                    inputs = self.vlm.tokenizer.encode(
                        full_prompt,
                        return_tensors="pt",
                        max_length=self.config.max_length,
                        truncation=True
                    ).to(self.device)
                    
                    # ìƒì„± ì„¤ì •
                    generation_config = {
                        'max_new_tokens': self.config.max_completion_length,
                        'temperature': self.config.temperature,
                        'top_p': self.config.top_p,
                        'top_k': self.config.top_k,
                        'do_sample': True,
                        'pad_token_id': self.vlm.tokenizer.eos_token_id,
                        'repetition_penalty': 1.1,
                        'return_dict_in_generate': True,
                        'output_scores': True
                    }
                    
                    # í…ìŠ¤íŠ¸ ìƒì„±
                    outputs = self.vlm.generate(inputs, **generation_config)
                    
                    # ìƒì„±ëœ í…ìŠ¤íŠ¸ ë””ì½”ë”©
                    generated_text = self.vlm.tokenizer.decode(
                        outputs.sequences[0][inputs.shape[1]:],
                        skip_special_tokens=True
                    ).strip()
                    
                    # ë¡œê·¸ í™•ë¥  ê³„ì‚°
                    log_prob = self._calculate_log_prob_from_scores(outputs.scores)
                    
                    enhanced_prompts.append(generated_text or prompt)
                    log_probs.append(log_prob)
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ Failed to enhance prompt '{prompt}': {e}")
                    enhanced_prompts.append(prompt)
                    log_probs.append(torch.tensor(0.0, device=self.device))
        
        return enhanced_prompts, torch.stack(log_probs)
    
    def _calculate_log_prob_from_scores(self, scores: Tuple[torch.Tensor]) -> torch.Tensor:
        """ìƒì„± ì ìˆ˜ì—ì„œ ë¡œê·¸ í™•ë¥  ê³„ì‚°"""
        if not scores:
            return torch.tensor(0.0, device=self.device)
        
        # ê° ìŠ¤í…ì˜ ë¡œê·¸ í™•ë¥ ì„ í•©ì‚°
        total_log_prob = torch.tensor(0.0, device=self.device)
        for score in scores:
            probs = F.softmax(score[0], dim=-1)
            max_prob = torch.max(probs)
            total_log_prob += torch.log(max_prob + 1e-8)
        
        return total_log_prob / len(scores)  # í‰ê·  ë¡œê·¸ í™•ë¥ 
    
    def calculate_grpo_loss(self, 
                           log_probs: torch.Tensor,
                           ref_log_probs: torch.Tensor,
                           rewards: torch.Tensor,
                           advantages: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        GRPO ì†ì‹¤ ê³„ì‚°
        
        MS Swiftì˜ GRPO êµ¬í˜„ì„ ì°¸ì¡°í•˜ì—¬ ê·¸ë£¹ ê¸°ë°˜ ìƒëŒ€ì  ì •ì±… ìµœì í™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            log_probs: í˜„ì¬ ì •ì±…ì˜ ë¡œê·¸ í™•ë¥ 
            ref_log_probs: ì°¸ì¡° ì •ì±…ì˜ ë¡œê·¸ í™•ë¥ 
            rewards: ë³´ìƒ ê°’ë“¤
            advantages: ì–´ë“œë°´í‹°ì§€ ê°’ë“¤
            
        Returns:
            Dict[str, torch.Tensor]: ì†ì‹¤ êµ¬ì„± ìš”ì†Œë“¤
        """
        # ì •ì±… ë¹„ìœ¨ ê³„ì‚°
        ratio = torch.exp(log_probs - ref_log_probs)
        
        # í´ë¦¬í•‘ëœ ì„œë¡œê²Œì´íŠ¸ ì†ì‹¤ (PPO ìŠ¤íƒ€ì¼)
        surr1 = ratio * advantages
        surr2 = torch.clamp(ratio, 1 - self.config.clip_epsilon, 1 + self.config.clip_epsilon) * advantages
        policy_loss = -torch.min(surr1, surr2).mean()
        
        # KL ë°œì‚° í˜ë„í‹°
        kl_divergence = (log_probs - ref_log_probs).mean()
        kl_penalty = self.config.kl_beta * kl_divergence
        
        # ì—”íŠ¸ë¡œí”¼ ë³´ë„ˆìŠ¤ (ì •ì±… íƒìƒ‰ ì¥ë ¤)
        entropy = -(log_probs * torch.exp(log_probs)).mean()
        entropy_bonus = self.config.entropy_coeff * entropy
        
        # ì´ ì†ì‹¤
        total_loss = policy_loss + kl_penalty - entropy_bonus
        
        return {
            'total_loss': total_loss,
            'policy_loss': policy_loss,
            'kl_divergence': kl_divergence,
            'kl_penalty': kl_penalty,
            'entropy': entropy,
            'entropy_bonus': entropy_bonus
        }
    
    def calculate_group_advantages(self, rewards: torch.Tensor) -> torch.Tensor:
        """
        ê·¸ë£¹ ê¸°ë°˜ ì–´ë“œë°´í‹°ì§€ ê³„ì‚° (GRPOì˜ í•µì‹¬)
        
        MS Swiftì˜ GRPO êµ¬í˜„ì„ ì°¸ì¡°í•˜ì—¬ ê·¸ë£¹ ë‚´ ìƒëŒ€ì  ì„±ëŠ¥ì„ ê¸°ë°˜ìœ¼ë¡œ 
        ì–´ë“œë°´í‹°ì§€ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        
        Args:
            rewards (torch.Tensor): ê·¸ë£¹ ë‚´ ë³´ìƒë“¤
            
        Returns:
            torch.Tensor: ê³„ì‚°ëœ ì–´ë“œë°´í‹°ì§€ë“¤
        """
        # ê·¸ë£¹ ë‚´ í‰ê·  ë³´ìƒ ê³„ì‚°
        group_mean_reward = rewards.mean()
        
        # ê·¸ë£¹ ë‚´ ìƒëŒ€ì  ì–´ë“œë°´í‹°ì§€ ê³„ì‚°
        advantages = rewards - group_mean_reward
        
        # ì–´ë“œë°´í‹°ì§€ ì •ê·œí™” (ì•ˆì •ì„± í–¥ìƒ)
        if advantages.std() > 1e-8:
            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        return advantages
    
    def train_step(self, prompts: List[str]) -> Dict[str, float]:
        """
        GRPO í•™ìŠµ ìŠ¤í… ìˆ˜í–‰
        
        Args:
            prompts (List[str]): í•™ìŠµìš© í”„ë¡¬í”„íŠ¸ë“¤
            
        Returns:
            Dict[str, float]: í•™ìŠµ í†µê³„
        """
        self.vlm.train()
        
        # 1. ì°¸ì¡° ëª¨ë¸ ì—…ë°ì´íŠ¸
        self._update_reference_model()
        
        # 2. í”„ë¡¬í”„íŠ¸ ê°œì„  ë° ë³´ìƒ ê³„ì‚°
        enhanced_prompts, log_probs = self.enhance_prompts_batch(prompts)
        ref_log_probs = self._calculate_reference_log_probs(prompts, enhanced_prompts)
        rewards = self._calculate_rewards_batch(enhanced_prompts, prompts)
        
        # 3. ê·¸ë£¹ ì–´ë“œë°´í‹°ì§€ ê³„ì‚°
        advantages = self.calculate_group_advantages(rewards)
        
        # 4. GRPO ì—í¬í¬ í•™ìŠµ
        epoch_stats = []
        for epoch in range(self.config.grpo_epochs):
            loss_dict = self.calculate_grpo_loss(log_probs, ref_log_probs, rewards, advantages)
            
            # ì—­ì „íŒŒ
            self.optimizer.zero_grad()
            loss_dict['total_loss'].backward()
            
            # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
            grad_norm = clip_grad_norm_(self.vlm.parameters(), self.config.max_grad_norm)
            
            # ì˜µí‹°ë§ˆì´ì € ìŠ¤í…
            self.optimizer.step()
            
            # í†µê³„ ìˆ˜ì§‘
            epoch_stats.append({
                'total_loss': loss_dict['total_loss'].item(),
                'policy_loss': loss_dict['policy_loss'].item(),
                'kl_divergence': loss_dict['kl_divergence'].item(),
                'entropy': loss_dict['entropy'].item(),
                'grad_norm': grad_norm.item() if isinstance(grad_norm, torch.Tensor) else grad_norm
            })
        
        # í‰ê·  í†µê³„ ê³„ì‚°
        avg_stats = {
            'avg_reward': rewards.mean().item(),
            'total_loss': np.mean([s['total_loss'] for s in epoch_stats]),
            'policy_loss': np.mean([s['policy_loss'] for s in epoch_stats]),
            'kl_divergence': np.mean([s['kl_divergence'] for s in epoch_stats]),
            'entropy': np.mean([s['entropy'] for s in epoch_stats]),
            'grad_norm': np.mean([s['grad_norm'] for s in epoch_stats])
        }
        
        # í•™ìŠµ í†µê³„ ì—…ë°ì´íŠ¸
        self.training_stats.update(avg_stats)
        self.training_stats['iteration'] += 1
        self.training_stats['total_samples'] += len(prompts)
        
        return avg_stats
    
    def _update_reference_model(self):
        """ì°¸ì¡° ëª¨ë¸ ì—…ë°ì´íŠ¸ (í˜„ì¬ ì •ì±…ì˜ ë³µì‚¬ë³¸)"""
        if self.vlm_ref is None:
            self.vlm_ref = copy.deepcopy(self.vlm)
        else:
            # ê¸°ì¡´ ì°¸ì¡° ëª¨ë¸ì„ í˜„ì¬ ì •ì±…ìœ¼ë¡œ ì—…ë°ì´íŠ¸
            self.vlm_ref.load_state_dict(self.vlm.state_dict())
        
        self.vlm_ref.eval()
        for param in self.vlm_ref.parameters():
            param.requires_grad = False
    
    def _calculate_reference_log_probs(self, prompts: List[str], enhanced_prompts: List[str]) -> torch.Tensor:
        """ì°¸ì¡° ëª¨ë¸ë¡œ ë¡œê·¸ í™•ë¥  ê³„ì‚°"""
        if self.vlm_ref is None:
            return torch.zeros(len(prompts), device=self.device)
        
        ref_log_probs = []
        
        with torch.no_grad():
            for prompt, enhanced in zip(prompts, enhanced_prompts):
                try:
                    # ì°¸ì¡° ëª¨ë¸ë¡œ ë¡œê·¸ í™•ë¥  ê³„ì‚°
                    full_prompt = f"Improve this prompt: {prompt}\nImproved:"
                    inputs = self.vlm_ref.tokenizer.encode(full_prompt, return_tensors="pt").to(self.device)
                    target = self.vlm_ref.tokenizer.encode(enhanced, return_tensors="pt").to(self.device)
                    
                    outputs = self.vlm_ref(inputs)
                    log_prob = F.log_softmax(outputs.logits[0, -1], dim=-1)[target[0, 0]]
                    ref_log_probs.append(log_prob)
                    
                except Exception as e:
                    logger.warning(f"Reference log prob calculation failed: {e}")
                    ref_log_probs.append(torch.tensor(0.0, device=self.device))
        
        return torch.stack(ref_log_probs)
    
    def _calculate_rewards_batch(self, enhanced_prompts: List[str], original_prompts: List[str]) -> torch.Tensor:
        """ë°°ì¹˜ ë³´ìƒ ê³„ì‚° (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” CLIP ë“± ì‚¬ìš©)"""
        rewards = []
        
        for enhanced, original in zip(enhanced_prompts, original_prompts):
            # ê°„ë‹¨í•œ ë³´ìƒ í•¨ìˆ˜ (ì‹¤ì œë¡œëŠ” CLIP, ì´ë¯¸ì§€ í’ˆì§ˆ ë“± ì‚¬ìš©)
            reward = len(enhanced.split()) / 10.0  # ë‹¨ì–´ ìˆ˜ ê¸°ë°˜ ê°„ë‹¨í•œ ë³´ìƒ
            if len(enhanced) > len(original):
                reward += 0.5  # ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ì— ë³´ë„ˆìŠ¤
            rewards.append(reward)
        
        return torch.tensor(rewards, device=self.device, dtype=torch.float32)
    
    def save_model(self, output_dir: str):
        """
        ëª¨ë¸ ì €ì¥ (LoRA ì–´ëŒ‘í„° ë˜ëŠ” ì „ì²´ ëª¨ë¸)
        
        Args:
            output_dir (str): ì €ì¥ ë””ë ‰í† ë¦¬
        """
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        if self.config.train_type in ["lora", "qlora"]:
            # LoRA ì–´ëŒ‘í„°ë§Œ ì €ì¥
            self.vlm.save_pretrained(output_path)
            logger.info(f"ğŸ’¾ LoRA adapter saved to {output_path}")
        else:
            # ì „ì²´ ëª¨ë¸ ì €ì¥
            self.vlm.save_pretrained(output_path)
            logger.info(f"ğŸ’¾ Full model saved to {output_path}")
        
        # ì„¤ì • íŒŒì¼ ì €ì¥
        config_path = output_path / "grpo_config.json"
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(self.config.__dict__, f, indent=2, ensure_ascii=False)
        
        # í•™ìŠµ í†µê³„ ì €ì¥
        stats_path = output_path / "training_stats.json"
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(self.training_stats, f, indent=2, ensure_ascii=False)
    
    def load_model(self, model_path: str):
        """
        ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ
        
        Args:
            model_path (str): ëª¨ë¸ ê²½ë¡œ
        """
        model_path = Path(model_path)
        
        if self.config.train_type in ["lora", "qlora"]:
            # LoRA ì–´ëŒ‘í„° ë¡œë“œ
            self.vlm = PeftModel.from_pretrained(self.original_model, model_path)
            logger.info(f"ğŸ“¥ LoRA adapter loaded from {model_path}")
        else:
            # ì „ì²´ ëª¨ë¸ ë¡œë“œ
            self.vlm.load_state_dict(torch.load(model_path / "pytorch_model.bin"))
            logger.info(f"ğŸ“¥ Full model loaded from {model_path}")
        
        self.vlm = self.vlm.to(self.device)
    
    def get_training_stats(self) -> Dict[str, Any]:
        """í•™ìŠµ í†µê³„ ë°˜í™˜"""
        return self.training_stats.copy()
    
    def print_model_info(self):
        """ëª¨ë¸ ì •ë³´ ì¶œë ¥"""
        total_params = sum(p.numel() for p in self.vlm.parameters())
        trainable_params = sum(p.numel() for p in self.vlm.parameters() if p.requires_grad)
        
        print("ğŸ” Model Information:")
        print(f"   - Train Type: {self.config.train_type}")
        print(f"   - Total Parameters: {total_params:,}")
        print(f"   - Trainable Parameters: {trainable_params:,}")
        print(f"   - Trainable Ratio: {100 * trainable_params / total_params:.2f}%")
        
        if self.config.train_type in ["lora", "qlora"]:
            print(f"   - LoRA Rank: {self.config.lora_rank}")
            print(f"   - LoRA Alpha: {self.config.lora_alpha}")
            print(f"   - Target Modules: {self.config.target_modules}")


# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    # ì„¤ì • ìƒì„±
    config = EnhancedGRPOConfig(
        train_type="lora",
        lora_rank=8,
        lora_alpha=32,
        learning_rate=1e-5,
        num_generations=4
    )
    
    print("âœ… Enhanced GRPO Trainer with LoRA/Full support ready!")
    print(f"   - Train Type: {config.train_type}")
    print(f"   - LoRA Rank: {config.lora_rank}")
    print(f"   - Learning Rate: {config.learning_rate}") 