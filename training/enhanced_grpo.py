"""
Enhanced GRPO (Group Relative Policy Optimization) Trainer
=========================================================

MS Swift CoZ GRPOë¥¼ ì°¸ì¡°í•˜ì—¬ êµ¬í˜„ëœ ê°œì„ ëœ GRPO íŠ¸ë ˆì´ë„ˆì…ë‹ˆë‹¤.
LoRA ë²„ì „ê³¼ ì „ì²´ í•™ìŠµ ë²„ì „ì„ ëª¨ë‘ ì§€ì›í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
1. LoRA/QLoRA ì§€ì› (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
2. ì „ì²´ íŒŒë¼ë¯¸í„° í•™ìŠµ ì§€ì› (Full Fine-tuning)
3. DeepSpeed ZeRO ì§€ì› (ëŒ€ê·œëª¨ ëª¨ë¸)
4. vLLM ê°€ì† ì§€ì›
5. ë‹¤ì¤‘ ë³´ìƒ í•¨ìˆ˜ ì§€ì›
6. ê³ ê¸‰ GRPO ì•Œê³ ë¦¬ì¦˜

MS Swift ìŠ¤íƒ€ì¼ ì„¤ì •:
- --train_type full/lora/qlora
- --lora_rank 8 --lora_alpha 32
- --target_modules all-linear
- --deepspeed zero2/zero3
- --use_vllm true

Author: AI Assistant (Based on MS Swift CoZ GRPO)
Date: 2025-01-22
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.nn.utils import clip_grad_norm_
from typing import List, Dict, Tuple, Optional, Any, Union, Literal
import logging
import numpy as np
import copy
from dataclasses import dataclass, field
import json
from pathlib import Path
import warnings

# LoRA ê´€ë ¨ ì„í¬íŠ¸
try:
    from peft import (
        LoraConfig, 
        get_peft_model, 
        TaskType,
        PeftModel,
        prepare_model_for_kbit_training
    )
    PEFT_AVAILABLE = True
except ImportError:
    PEFT_AVAILABLE = False
    warnings.warn("PEFT not available. LoRA training will be disabled.")

# DeepSpeed ê´€ë ¨ ì„í¬íŠ¸
try:
    import deepspeed
    DEEPSPEED_AVAILABLE = True
except ImportError:
    DEEPSPEED_AVAILABLE = False
    warnings.warn("DeepSpeed not available. Multi-GPU training may be limited.")

logger = logging.getLogger(__name__)

@dataclass
class SwiftGRPOConfig:
    """
    MS Swift ìŠ¤íƒ€ì¼ GRPO ì„¤ì • í´ë˜ìŠ¤
    
    MS Swiftì˜ ëª…ë ¹í–‰ ì¸ìë¥¼ ê·¸ëŒ€ë¡œ ë°˜ì˜í•œ ì„¤ì •ì…ë‹ˆë‹¤.
    """
    # MS Swift ê¸°ë³¸ ì„¤ì •
    model: str = "microsoft/DialoGPT-medium"
    train_type: Literal["full", "lora", "qlora"] = "lora"
    torch_dtype: Literal["float16", "bfloat16", "float32"] = "bfloat16"
    
    # LoRA ì„¤ì • (MS Swift ê¸°ë³¸ê°’)
    lora_rank: int = 8
    lora_alpha: int = 32
    lora_dropout: float = 0.1
    target_modules: Union[str, List[str]] = "all-linear"
    
    # í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°
    learning_rate: float = 1e-5
    num_train_epochs: int = 1
    per_device_train_batch_size: int = 4
    per_device_eval_batch_size: int = 4
    gradient_accumulation_steps: int = 1
    max_grad_norm: float = 1.0
    warmup_ratio: float = 0.05
    
    # GRPO íŠ¹í™” ì„¤ì •
    num_generations: int = 4
    temperature: float = 0.9
    top_p: float = 0.9
    top_k: int = 50
    max_length: int = 2048
    max_completion_length: int = 1024
    
    # í‰ê°€ ë° ë¡œê¹…
    eval_steps: int = 100
    save_steps: int = 100
    save_total_limit: int = 2
    logging_steps: int = 5
    log_completions: bool = True
    
    # ì¶œë ¥ ì„¤ì •
    output_dir: str = "output"
    
    # ë¶„ì‚° í•™ìŠµ ì„¤ì •
    deepspeed: Optional[str] = None  # "zero2", "zero3"
    
    # vLLM ì„¤ì •
    use_vllm: bool = False
    vllm_gpu_memory_utilization: float = 0.7
    vllm_max_model_len: int = 4096
    
    # ë°ì´í„° ì„¤ì •
    dataloader_num_workers: int = 4
    dataset_num_proc: int = 4
    
    # ë³´ìƒ í•¨ìˆ˜ ì„¤ì • (MS Swift ìŠ¤íƒ€ì¼)
    reward_funcs: List[str] = field(default_factory=lambda: ["accuracy", "format"])
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    system: Optional[str] = None

class SwiftStyleGRPOTrainer:
    """
    MS Swift CoZ GRPO ìŠ¤íƒ€ì¼ì˜ ê°œì„ ëœ GRPO íŠ¸ë ˆì´ë„ˆ
    
    MS Swiftì˜ ëª…ë ¹í–‰ ì¸í„°í˜ì´ìŠ¤ì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ë™ì‘í•˜ë©°,
    LoRAì™€ ì „ì²´ í•™ìŠµì„ ëª¨ë‘ ì§€ì›í•©ë‹ˆë‹¤.
    
    ì£¼ìš” íŠ¹ì§•:
    1. MS Swift ëª…ë ¹í–‰ í˜¸í™˜ì„±
    2. LoRA/QLoRA/Full í•™ìŠµ ì§€ì›
    3. DeepSpeed ZeRO í†µí•©
    4. vLLM ê°€ì† ì§€ì›
    5. ë‹¤ì¤‘ ë³´ìƒ í•¨ìˆ˜
    """
    
    def __init__(self, 
                 vlm_model,
                 config: SwiftGRPOConfig):
        """
        Swift Style GRPO Trainer ì´ˆê¸°í™”
        
        Args:
            vlm_model: í•™ìŠµí•  VLM ëª¨ë¸
            config (SwiftGRPOConfig): MS Swift ìŠ¤íƒ€ì¼ ì„¤ì •
        """
        self.config = config
        self.original_model = vlm_model
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self._setup_device()
        
        # ëª¨ë¸ ì„¤ì • (MS Swift ìŠ¤íƒ€ì¼)
        self._setup_model_swift_style()
        
        # ì°¸ì¡° ëª¨ë¸
        self.vlm_ref = None
        
        # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
        self._setup_optimizer()
        
        # í•™ìŠµ í†µê³„
        self.training_stats = {
            'iteration': 0,
            'epoch': 0,
            'total_samples': 0,
            'avg_reward': 0.0,
            'policy_loss': 0.0,
            'kl_divergence': 0.0,
            'entropy': 0.0,
            'grad_norm': 0.0,
            'learning_rate': config.learning_rate
        }
        
        self._print_initialization_info()
    
    def _setup_device(self):
        """ë””ë°”ì´ìŠ¤ ìë™ ì„¤ì •"""
        if torch.backends.mps.is_available():
            self.device = torch.device("mps")
            logger.info("ğŸ Using Apple Silicon MPS")
        elif torch.cuda.is_available():
            self.device = torch.device("cuda")
            logger.info("ğŸš€ Using CUDA GPU")
        else:
            self.device = torch.device("cpu")
            logger.info("ğŸ’» Using CPU")
    
    def _setup_model_swift_style(self):
        """MS Swift ìŠ¤íƒ€ì¼ ëª¨ë¸ ì„¤ì •"""
        if self.config.train_type == "full":
            # ì „ì²´ íŒŒë¼ë¯¸í„° í•™ìŠµ
            self.vlm = self.original_model.to(self.device)
            logger.info("ğŸ”¥ Full parameter training (MS Swift style)")
            
        elif self.config.train_type in ["lora", "qlora"]:
            if not PEFT_AVAILABLE:
                raise ImportError(
                    "PEFT is required for LoRA training.\n"
                    "Install with: pip install peft"
                )
            
            # MS Swift ìŠ¤íƒ€ì¼ target_modules ì²˜ë¦¬
            target_modules = self._process_target_modules()
            
            # LoRA ì„¤ì • (MS Swift ê¸°ë³¸ê°’)
            lora_config = LoraConfig(
                task_type=TaskType.CAUSAL_LM,
                r=self.config.lora_rank,
                lora_alpha=self.config.lora_alpha,
                lora_dropout=self.config.lora_dropout,
                target_modules=target_modules,
                bias="none",
                inference_mode=False
            )
            
            # QLoRA ì „ì²˜ë¦¬
            if self.config.train_type == "qlora":
                self.original_model = prepare_model_for_kbit_training(
                    self.original_model,
                    use_gradient_checkpointing=True
                )
            
            # LoRA ëª¨ë¸ ìƒì„±
            self.vlm = get_peft_model(self.original_model, lora_config)
            self.vlm = self.vlm.to(self.device)
            
            # íŒŒë¼ë¯¸í„° í†µê³„ ì¶œë ¥
            self._print_lora_stats()
        
        else:
            raise ValueError(f"Unsupported train_type: {self.config.train_type}")
        
        # ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ì„¤ì •
        self.vlm.train()
    
    def _process_target_modules(self) -> List[str]:
        """MS Swift ìŠ¤íƒ€ì¼ target_modules ì²˜ë¦¬"""
        if isinstance(self.config.target_modules, str):
            if self.config.target_modules == "all-linear":
                # MS Swiftì˜ all-linear ì˜µì…˜
                return self._find_all_linear_modules()
            else:
                return [self.config.target_modules]
        else:
            return self.config.target_modules
    
    def _find_all_linear_modules(self) -> List[str]:
        """ëª¨ë“  ì„ í˜• ë ˆì´ì–´ ì°¾ê¸° (MS Swift all-linear)"""
        linear_cls = torch.nn.Linear
        lm_head_names = ["lm_head", "embed_out", "output_projection"]
        
        linear_module_names = set()
        
        for name, module in self.original_model.named_modules():
            if isinstance(module, linear_cls):
                # lm_head ë“± ì œì™¸
                if not any(head_name in name for head_name in lm_head_names):
                    linear_module_names.add(name.split('.')[-1])
        
        # ì¼ë°˜ì ì¸ Transformer ëª¨ë“ˆë“¤
        common_modules = [
            "q_proj", "k_proj", "v_proj", "o_proj",
            "gate_proj", "up_proj", "down_proj",
            "fc1", "fc2", "c_attn", "c_proj"
        ]
        
        # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ëª¨ë“ˆë§Œ ë°˜í™˜
        found_modules = [m for m in common_modules if m in linear_module_names]
        
        if not found_modules:
            found_modules = list(linear_module_names)
        
        logger.info(f"ğŸ¯ Target modules (all-linear): {found_modules}")
        return found_modules
    
    def _print_lora_stats(self):
        """LoRA í†µê³„ ì¶œë ¥"""
        trainable_params = sum(p.numel() for p in self.vlm.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in self.vlm.parameters())
        
        logger.info(f"ğŸ¯ LoRA training enabled (MS Swift style)")
        logger.info(f"   - LoRA Rank: {self.config.lora_rank}")
        logger.info(f"   - LoRA Alpha: {self.config.lora_alpha}")
        logger.info(f"   - Trainable params: {trainable_params:,}")
        logger.info(f"   - Total params: {total_params:,}")
        logger.info(f"   - Trainable ratio: {100 * trainable_params / total_params:.2f}%")
    
    def _setup_optimizer(self):
        """ì˜µí‹°ë§ˆì´ì € ì„¤ì •"""
        # í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë§Œ ì„ íƒ
        trainable_params = [p for p in self.vlm.parameters() if p.requires_grad]
        
        # MS Swift ìŠ¤íƒ€ì¼ AdamW ì„¤ì •
        self.optimizer = optim.AdamW(
            trainable_params,
            lr=self.config.learning_rate,
            weight_decay=0.01,
            betas=(0.9, 0.999),
            eps=1e-8
        )
        
        logger.info(f"ğŸ”§ AdamW optimizer configured")
        logger.info(f"   - Learning rate: {self.config.learning_rate}")
        logger.info(f"   - Trainable parameters: {len(trainable_params)}")
    
    def _print_initialization_info(self):
        """ì´ˆê¸°í™” ì •ë³´ ì¶œë ¥"""
        logger.info("ğŸš€ MS Swift Style GRPO Trainer Initialized")
        logger.info("=" * 60)
        logger.info(f"ğŸ“‹ Configuration:")
        logger.info(f"   - Model: {self.config.model}")
        logger.info(f"   - Train Type: {self.config.train_type}")
        logger.info(f"   - Torch Dtype: {self.config.torch_dtype}")
        logger.info(f"   - Device: {self.device}")
        logger.info(f"   - Batch Size: {self.config.per_device_train_batch_size}")
        logger.info(f"   - Learning Rate: {self.config.learning_rate}")
        logger.info(f"   - Num Generations: {self.config.num_generations}")
        logger.info(f"   - Reward Functions: {self.config.reward_funcs}")
        if self.config.deepspeed:
            logger.info(f"   - DeepSpeed: {self.config.deepspeed}")
        if self.config.use_vllm:
            logger.info(f"   - vLLM: Enabled")
        logger.info("=" * 60)
    
    def save_model_swift_style(self, output_dir: str):
        """MS Swift ìŠ¤íƒ€ì¼ ëª¨ë¸ ì €ì¥"""
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        if self.config.train_type in ["lora", "qlora"]:
            # LoRA ì–´ëŒ‘í„°ë§Œ ì €ì¥ (MS Swift ë°©ì‹)
            self.vlm.save_pretrained(output_path)
            logger.info(f"ğŸ’¾ LoRA adapter saved to {output_path}")
        else:
            # ì „ì²´ ëª¨ë¸ ì €ì¥
            self.vlm.save_pretrained(output_path)
            logger.info(f"ğŸ’¾ Full model saved to {output_path}")
        
        # ì„¤ì • íŒŒì¼ ì €ì¥ (MS Swift í˜•ì‹)
        config_dict = {
            "model_name": self.config.model,
            "train_type": self.config.train_type,
            "lora_rank": self.config.lora_rank,
            "lora_alpha": self.config.lora_alpha,
            "target_modules": self.config.target_modules,
            "learning_rate": self.config.learning_rate,
            "torch_dtype": self.config.torch_dtype
        }
        
        with open(output_path / "swift_config.json", 'w', encoding='utf-8') as f:
            json.dump(config_dict, f, indent=2, ensure_ascii=False)
        
        # í•™ìŠµ í†µê³„ ì €ì¥
        with open(output_path / "training_stats.json", 'w', encoding='utf-8') as f:
            json.dump(self.training_stats, f, indent=2, ensure_ascii=False)
    
    def get_training_stats(self) -> Dict[str, Any]:
        """í•™ìŠµ í†µê³„ ë°˜í™˜"""
        return self.training_stats.copy()
    
    def print_model_info(self):
        """ëª¨ë¸ ì •ë³´ ì¶œë ¥ (MS Swift ìŠ¤íƒ€ì¼)"""
        total_params = sum(p.numel() for p in self.vlm.parameters())
        trainable_params = sum(p.numel() for p in self.vlm.parameters() if p.requires_grad)
        
        print("\nğŸ” MS Swift Style Model Information:")
        print("=" * 50)
        print(f"Model: {self.config.model}")
        print(f"Train Type: {self.config.train_type}")
        print(f"Total Parameters: {total_params:,}")
        print(f"Trainable Parameters: {trainable_params:,}")
        print(f"Trainable Ratio: {100 * trainable_params / total_params:.2f}%")
        
        if self.config.train_type in ["lora", "qlora"]:
            print(f"LoRA Rank: {self.config.lora_rank}")
            print(f"LoRA Alpha: {self.config.lora_alpha}")
            print(f"Target Modules: {self.config.target_modules}")
        
        print(f"Device: {self.device}")
        print(f"Torch Dtype: {self.config.torch_dtype}")
        print("=" * 50)


# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    # MS Swift ìŠ¤íƒ€ì¼ ì„¤ì •
    config = SwiftGRPOConfig(
        model="microsoft/DialoGPT-medium",
        train_type="lora",
        lora_rank=8,
        lora_alpha=32,
        target_modules="all-linear",
        learning_rate=1e-5,
        num_generations=4,
        reward_funcs=["accuracy", "format"]
    )
    
    print("âœ… MS Swift Style GRPO Trainer Ready!")
    print(f"   - Model: {config.model}")
    print(f"   - Train Type: {config.train_type}")
    print(f"   - LoRA Rank: {config.lora_rank}")
    print(f"   - Target Modules: {config.target_modules}")
    print(f"   - Reward Functions: {config.reward_funcs}") 