"""
GRPO (Group Relative Policy Optimization) Trainer
================================================

GRPO ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í•œ í•µì‹¬ í•™ìŠµ ëª¨ë“ˆì…ë‹ˆë‹¤.
VLM í”„ë¡¬í”„íŠ¸ ê°œì„ ì„ ìœ„í•œ ê°•í™”í•™ìŠµ íŠ¸ë ˆì´ë„ˆì…ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
1. GRPO ì •ì±… ì—…ë°ì´íŠ¸
2. ê·¸ë£¹ ê¸°ë°˜ ì–´ë“œë°´í‹°ì§€ ê³„ì‚°
3. KL ë°œì‚° í˜ë„í‹°
4. ì°¸ì¡° ëª¨ë¸ ê´€ë¦¬

GRPO vs PPO ì°¨ì´ì :
- PPO: ê°œë³„ ìƒ˜í”Œ ê¸°ë°˜ ì–´ë“œë°´í‹°ì§€
- GRPO: ê·¸ë£¹ ë‚´ ìƒëŒ€ì  ì–´ë“œë°´í‹°ì§€ (ë” ì•ˆì •ì )

Author: AI Assistant
Date: 2025-01-22
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.distributions import Categorical
from dataclasses import dataclass
from typing import Dict, List, Any, Tuple, Optional
import numpy as np
import logging
from transformers import AutoTokenizer

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class GRPOConfig:
    """GRPO í•™ìŠµ ì„¤ì • í´ë˜ìŠ¤"""
    learning_rate: float = 1e-5
    group_size: int = 4
    num_iterations: int = 20
    grpo_epochs: int = 2
    gamma: float = 0.99           # í• ì¸ íŒ©í„°
    kl_beta: float = 0.01         # KL ë°œì‚° í˜ë„í‹° ê³„ìˆ˜
    clip_epsilon: float = 0.2     # í´ë¦¬í•‘ ë²”ìœ„
    entropy_coeff: float = 0.01   # ì—”íŠ¸ë¡œí”¼ ë³´ë„ˆìŠ¤ ê³„ìˆ˜
    max_grad_norm: float = 1.0    # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
    epsilon_std: float = 1e-8     # í‘œì¤€í¸ì°¨ ì •ê·œí™”ìš© ì—¡ì‹¤ë¡ 
    
    # í† í° ìƒì„± íŒŒë¼ë¯¸í„°
    max_new_tokens: int = 20      # ìµœëŒ€ ìƒì„± í† í° ìˆ˜
    vocab_size: int = 50000       # ì–´íœ˜ í¬ê¸°
    max_sequence_length: int = 100 # ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
    temperature: float = 0.8
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device: str = "auto"

class TokenPolicyNetwork(nn.Module):
    """í† í°ë³„ ì •ì±… ë„¤íŠ¸ì›Œí¬ - ë‹¤ìŒ í† í°ì„ ì„ íƒí•˜ëŠ” ëª¨ë¸"""
    
    def __init__(self, vocab_size: int = 50000, embed_dim: int = 256, hidden_dim: int = 512):
        super().__init__()
        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        
        # í† í° ì„ë² ë”©
        self.token_embedding = nn.Embedding(vocab_size, embed_dim)
        self.position_embedding = nn.Embedding(100, embed_dim)  # ìµœëŒ€ 100 í† í°
        
        # íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=embed_dim,
                nhead=8,
                dim_feedforward=hidden_dim,
                dropout=0.1,
                batch_first=True
            ),
            num_layers=4
        )
        
        # ì¶œë ¥ í—¤ë“œ
        self.output_head = nn.Linear(embed_dim, vocab_size)
        
    def forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None):
        """
        Args:
            input_ids: [batch_size, seq_len] - í† í° ID ì‹œí€€ìŠ¤
            attention_mask: [batch_size, seq_len] - ì–´í…ì…˜ ë§ˆìŠ¤í¬
            
        Returns:
            Categorical: ë‹¤ìŒ í† í°ì— ëŒ€í•œ í™•ë¥  ë¶„í¬
        """
        batch_size, seq_len = input_ids.shape
        
        # í† í° + ìœ„ì¹˜ ì„ë² ë”©
        token_embeds = self.token_embedding(input_ids)
        positions = torch.arange(seq_len, device=input_ids.device).unsqueeze(0).expand(batch_size, -1)
        pos_embeds = self.position_embedding(positions)
        
        # ì„ë² ë”© í•©ì„±
        embeddings = token_embeds + pos_embeds
        
        # íŠ¸ëœìŠ¤í¬ë¨¸ ì²˜ë¦¬
        if attention_mask is not None:
            # íŒ¨ë”© ë§ˆìŠ¤í¬ ìƒì„± (True = ë§ˆìŠ¤í‚¹)
            padding_mask = (attention_mask == 0)
        else:
            padding_mask = None
        
        transformer_output = self.transformer(embeddings, src_key_padding_mask=padding_mask)
        
        # ë§ˆì§€ë§‰ í† í°ì˜ ì¶œë ¥ì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ í† í° ì˜ˆì¸¡
        last_token_output = transformer_output[:, -1, :]  # [batch_size, embed_dim]
        logits = self.output_head(last_token_output)      # [batch_size, vocab_size]
        
        # í™•ë¥  ë¶„í¬ ë°˜í™˜
        return Categorical(logits=logits)

class GRPOTrainer:
    """í† í°ë³„ ìˆœì°¨ ìƒì„± ê¸°ë°˜ GRPO íŠ¸ë ˆì´ë„ˆ"""
    
    def __init__(self, vlm_model, sd_generator, clip_reward, config: GRPOConfig):
        self.vlm = vlm_model
        self.sd_generator = sd_generator  # ë™ê²°ëœ SD3 íŒŒì´í”„ë¼ì¸
        self.clip_reward = clip_reward    # ë™ê²°ëœ CLIP ë³´ìƒ ëª¨ë¸
        self.config = config
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        if config.device == "auto":
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = torch.device(config.device)
        
        # í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
        self.tokenizer = AutoTokenizer.from_pretrained("gpt2")
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # ì •ì±… ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™”
        self.policy_network = TokenPolicyNetwork(
            vocab_size=len(self.tokenizer),
            embed_dim=256,
            hidden_dim=512
        ).to(self.device)
        
        # ì˜µí‹°ë§ˆì´ì €
        self.optimizer = optim.Adam(self.policy_network.parameters(), lr=config.learning_rate)
        
        # í•™ìŠµ í†µê³„
        self.training_stats = {
            'iteration': 0,
            'total_samples': 0,
            'avg_reward': 0.0,
            'policy_loss': 0.0,
            'entropy': 0.0,
            'kl_div': 0.0
        }
        
        logger.info(f"ğŸš€ GRPO Trainer initialized with device: {self.device}")
        logger.info(f"ğŸ“Š Policy network parameters: {sum(p.numel() for p in self.policy_network.parameters())}")
        logger.info(f"ğŸ“ Tokenizer vocab size: {len(self.tokenizer)}")
    
    def collect_group_data(self, prompts: List[str]) -> Dict[str, Any]:
        """
        ê·¸ë£¹ ë°ì´í„° ìˆ˜ì§‘ - í† í°ë³„ ìˆœì°¨ ìƒì„± ë°©ì‹
        
        ê° í”„ë¡¬í”„íŠ¸ì— ëŒ€í•´:
        1. í† í°ë³„ë¡œ ìˆœì°¨ ìƒì„±
        2. ê° ìŠ¤í…ì—ì„œ state = user_prompt + ì§€ê¸ˆê¹Œì§€_ìƒì„±ëœ_í† í°ë“¤
        3. action = ë‹¤ìŒ_í† í°_ì„ íƒ
        4. ì™„ì„±ëœ í”„ë¡¬í”„íŠ¸ë¡œ ì´ë¯¸ì§€ ìƒì„± ë° ë³´ìƒ ê³„ì‚°
        """
        logger.info(f"ğŸ”„ Collecting group data for {len(prompts)} prompts...")
        
        group_data = {
            'original_prompts': [],
            'generated_sequences': [],  # ìƒì„±ëœ ì „ì²´ ì‹œí€€ìŠ¤
            'states': [],              # ê° ìŠ¤í…ì˜ ìƒíƒœ
            'actions': [],             # ê° ìŠ¤í…ì˜ ì•¡ì…˜ (í† í°)
            'log_probs_old': [],       # ê° ìŠ¤í…ì˜ ë¡œê·¸ í™•ë¥ 
            'rewards': [],             # ê° ì‹œí€€ìŠ¤ì˜ ìµœì¢… ë³´ìƒ
            'episode_lengths': []      # ê° ì—í”¼ì†Œë“œ ê¸¸ì´
        }
        
        self.policy_network.eval()
        
        for prompt in prompts:
            logger.debug(f"ğŸ“ Processing prompt: '{prompt[:50]}...'")
            
            # 1. í”„ë¡¬í”„íŠ¸ í† í¬ë‚˜ì´ì§•
            initial_tokens = self.tokenizer.encode(prompt, return_tensors="pt").to(self.device)
            
            # 2. í† í°ë³„ ìˆœì°¨ ìƒì„±
            episode_states = []
            episode_actions = []
            episode_log_probs = []
            
            current_sequence = initial_tokens.clone()
            
            with torch.no_grad():
                for step in range(self.config.max_new_tokens):
                    # í˜„ì¬ ìƒíƒœ: user_prompt + ì§€ê¸ˆê¹Œì§€_ìƒì„±ëœ_í† í°ë“¤
                    current_state = current_sequence.clone()
                    
                    # ì •ì±… ë„¤íŠ¸ì›Œí¬ë¡œ ë‹¤ìŒ í† í° ë¶„í¬ ê³„ì‚°
                    policy_dist = self.policy_network(current_sequence)
                    
                    # ë‹¤ìŒ í† í° ìƒ˜í”Œë§
                    next_token = policy_dist.sample()
                    log_prob = policy_dist.log_prob(next_token)
                    
                    # ë°ì´í„° ì €ì¥
                    episode_states.append(current_state.squeeze())
                    episode_actions.append(next_token)
                    episode_log_probs.append(log_prob)
                    
                    # ì‹œí€€ìŠ¤ ì—…ë°ì´íŠ¸
                    current_sequence = torch.cat([current_sequence, next_token.unsqueeze(0).unsqueeze(0)], dim=1)
                    
                    # EOS í† í°ì´ë©´ ì¤‘ë‹¨
                    if next_token.item() == self.tokenizer.eos_token_id:
                        break
            
            # 3. ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ ë””ì½”ë”©
            generated_sequence = current_sequence.squeeze()
            generated_text = self.tokenizer.decode(generated_sequence, skip_special_tokens=True)
            
            # 4. í™˜ê²½ ì‹¤í–‰: ë™ê²°ëœ í…ìŠ¤íŠ¸â†’ì´ë¯¸ì§€ íŒŒì´í”„ë¼ì¸
            try:
                # SD3ë¡œ ì´ë¯¸ì§€ ìƒì„±
                generated_image = self.sd_generator.generate_image(generated_text)
                
                # CLIPìœ¼ë¡œ ë³´ìƒ ê³„ì‚°
                reward = self.clip_reward.calculate_reward(
                    image=generated_image,
                    text=generated_text,
                    original_prompt=prompt
                )
            except Exception as e:
                logger.warning(f"âš ï¸ Image generation/reward failed: {e}")
                reward = 0.0
            
            # 5. ê·¸ë£¹ ë°ì´í„°ì— ì¶”ê°€
            group_data['original_prompts'].append(prompt)
            group_data['generated_sequences'].append(generated_sequence)
            group_data['states'].extend(episode_states)
            group_data['actions'].extend(episode_actions)
            group_data['log_probs_old'].extend(episode_log_probs)
            
            # ê° ìŠ¤í…ì— ë™ì¼í•œ ìµœì¢… ë³´ìƒ í• ë‹¹ (í• ì¸ ì ìš© ì˜ˆì •)
            episode_length = len(episode_states)
            group_data['rewards'].extend([reward] * episode_length)
            group_data['episode_lengths'].append(episode_length)
            
            logger.debug(f"âœ… Generated: '{generated_text[:50]}...', Reward: {reward:.4f}, Length: {episode_length}")
        
        logger.info(f"ğŸ“Š Collected {len(group_data['states'])} steps from {len(prompts)} episodes")
        logger.info(f"ğŸ“Š Average reward: {np.mean(group_data['rewards']):.4f}")
        
        return group_data
    
    def _calculate_advantages_and_returns(self, group_data: Dict[str, Any]):
        """
        í• ì¸ëœ ë¦¬í„´ê³¼ ì–´ë“œë°´í‹°ì§€ ê³„ì‚° (ì—í”¼ì†Œë“œë³„)
        """
        logger.debug("ğŸ”„ Calculating discounted returns and advantages...")
        
        returns = []
        advantages = []
        
        start_idx = 0
        for episode_length in group_data['episode_lengths']:
            # ì—í”¼ì†Œë“œ ë³´ìƒ ì¶”ì¶œ
            episode_rewards = group_data['rewards'][start_idx:start_idx + episode_length]
            
            # í• ì¸ëœ ë¦¬í„´ ê³„ì‚° (ì—­ìˆœ)
            episode_returns = []
            discounted_return = 0.0
            
            for reward in reversed(episode_rewards):
                discounted_return = reward + self.config.gamma * discounted_return
                episode_returns.insert(0, discounted_return)
            
            returns.extend(episode_returns)
            start_idx += episode_length
        
        # ê·¸ë£¹ ì •ê·œí™”ë¥¼ ìœ„í•œ ì–´ë“œë°´í‹°ì§€ ê³„ì‚°
        returns_array = np.array(returns, dtype=np.float32)
        
        if len(returns_array) > 1:
            group_mean = np.mean(returns_array)
            group_std = np.std(returns_array)
            advantages_array = (returns_array - group_mean) / (group_std + self.config.epsilon_std)
        else:
            advantages_array = np.array([0.0])
        
        # í…ì„œë¡œ ë³€í™˜
        group_data['returns'] = [torch.tensor(ret, dtype=torch.float32, device=self.device) for ret in returns]
        group_data['advantages'] = [torch.tensor(adv, dtype=torch.float32, device=self.device) for adv in advantages_array]
        
        logger.debug(f"ğŸ“Š Returns: mean={np.mean(returns_array):.4f}, std={np.std(returns_array):.4f}")
        logger.debug(f"ğŸ“Š Advantages: mean={np.mean(advantages_array):.4f}, std={np.std(advantages_array):.4f}")
    
    def grpo_update(self, group_data: Dict[str, Any]) -> Dict[str, float]:
        """
        GRPO ì—…ë°ì´íŠ¸ - í† í°ë³„ ì •ì±… ê°œì„ 
        """
        logger.info("ğŸ”„ Starting GRPO update...")
        
        # 1. ì–´ë“œë°´í‹°ì§€ ê³„ì‚°
        self._calculate_advantages_and_returns(group_data)
        
        # 2. ì°¸ì¡° ëª¨ë¸ ìƒì„±
        policy_ref = TokenPolicyNetwork(
            vocab_size=len(self.tokenizer),
            embed_dim=256,
            hidden_dim=512
        ).to(self.device)
        policy_ref.load_state_dict(self.policy_network.state_dict())
        policy_ref.eval()
        
        # 3. ì—¬ëŸ¬ ì—í¬í¬ ì—…ë°ì´íŠ¸
        total_policy_loss = 0.0
        total_entropy = 0.0
        total_kl_div = 0.0
        
        for epoch in range(self.config.grpo_epochs):
            metrics = self._grpo_epoch_update(group_data, policy_ref)
            total_policy_loss += metrics['policy_loss']
            total_entropy += metrics['entropy']
            total_kl_div += metrics['kl_div']
        
        # 4. í‰ê·  ë©”íŠ¸ë¦­
        avg_metrics = {
            'policy_loss': total_policy_loss / self.config.grpo_epochs,
            'entropy': total_entropy / self.config.grpo_epochs,
            'kl_div': total_kl_div / self.config.grpo_epochs,
            'avg_reward': np.mean([r for episode_rewards in group_data['rewards'] for r in episode_rewards] if isinstance(group_data['rewards'][0], list) else group_data['rewards'])
        }
        
        # 5. í†µê³„ ì—…ë°ì´íŠ¸
        self.training_stats.update(avg_metrics)
        self.training_stats['iteration'] += 1
        self.training_stats['total_samples'] += len(group_data['states'])
        
        logger.info(f"âœ… GRPO update complete:")
        logger.info(f"  Policy Loss: {avg_metrics['policy_loss']:.4f}")
        logger.info(f"  Entropy: {avg_metrics['entropy']:.4f}")
        logger.info(f"  KL Div: {avg_metrics['kl_div']:.4f}")
        logger.info(f"  Avg Reward: {avg_metrics['avg_reward']:.4f}")
        
        return avg_metrics
    
    def _grpo_epoch_update(self, group_data: Dict[str, Any], policy_ref: TokenPolicyNetwork) -> Dict[str, float]:
        """
        ë‹¨ì¼ ì—í¬í¬ GRPO ì—…ë°ì´íŠ¸
        """
        self.optimizer.zero_grad()
        
        total_policy_loss = 0.0
        total_entropy = 0.0
        total_kl_div = 0.0
        batch_size = len(group_data['states'])
        
        for i in range(batch_size):
            # í˜„ì¬ ìŠ¤í… ë°ì´í„°
            state = group_data['states'][i].unsqueeze(0)  # [1, seq_len]
            action = group_data['actions'][i]
            old_log_prob = group_data['log_probs_old'][i]
            advantage = group_data['advantages'][i]
            
            # í˜„ì¬ ì •ì±… ë¶„í¬
            current_policy_dist = self.policy_network(state)
            current_log_prob = current_policy_dist.log_prob(action)
            
            # ì°¸ì¡° ì •ì±… ë¶„í¬
            with torch.no_grad():
                ref_policy_dist = policy_ref(state)
            
            # PPO/GRPO ì†ì‹¤ ê³„ì‚°
            log_ratio = current_log_prob - old_log_prob.detach()
            ratio = torch.exp(log_ratio)
            
            surr1 = ratio * advantage
            surr2 = torch.clamp(ratio, 1 - self.config.clip_epsilon, 1 + self.config.clip_epsilon) * advantage
            policy_loss_i = -torch.min(surr1, surr2)
            
            # ì—”íŠ¸ë¡œí”¼ ë° KL ë°œì‚°
            entropy_i = current_policy_dist.entropy()
            kl_div_i = torch.distributions.kl_divergence(ref_policy_dist, current_policy_dist)
            
            total_policy_loss += policy_loss_i
            total_entropy += entropy_i
            total_kl_div += kl_div_i
        
        # í‰ê·  ë° ì´ ì†ì‹¤
        avg_policy_loss = total_policy_loss / batch_size
        avg_entropy = total_entropy / batch_size
        avg_kl_div = total_kl_div / batch_size
        
        total_loss = avg_policy_loss + self.config.kl_beta * avg_kl_div - self.config.entropy_coeff * avg_entropy
        
        # ì—­ì „íŒŒ
        total_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.policy_network.parameters(), self.config.max_grad_norm)
        self.optimizer.step()
        
        return {
            'policy_loss': float(avg_policy_loss.detach()),
            'entropy': float(avg_entropy.detach()),
            'kl_div': float(avg_kl_div.detach()),
            'total_loss': float(total_loss.detach())
        }
    
    def get_training_stats(self) -> Dict[str, Any]:
        """í˜„ì¬ í•™ìŠµ í†µê³„ ë°˜í™˜"""
        return self.training_stats.copy()
    
    def save_checkpoint(self, checkpoint_path: str):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        try:
            checkpoint = {
                'policy_network_state_dict': self.policy_network.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'tokenizer': self.tokenizer,
                'config': self.config,
                'training_stats': self.training_stats
            }
            torch.save(checkpoint, checkpoint_path)
            logger.info(f"ğŸ’¾ Checkpoint saved: {checkpoint_path}")
        except Exception as e:
            logger.error(f"âŒ Failed to save checkpoint: {e}")
    
    def load_checkpoint(self, checkpoint_path: str):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        try:
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            self.policy_network.load_state_dict(checkpoint['policy_network_state_dict'])
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            self.training_stats = checkpoint['training_stats']
            logger.info(f"ğŸ“‚ Checkpoint loaded: {checkpoint_path}")
        except Exception as e:
            logger.error(f"âŒ Failed to load checkpoint: {e}")

# í…ŒìŠ¤íŠ¸ìš© Mock í´ë˜ìŠ¤ë“¤
if __name__ == "__main__":
    class MockVLM:
        def enhance_prompt(self, prompt):
            return f"enhanced {prompt}"
    
    class MockSDGenerator:
        def generate_image(self, prompt):
            return f"image_for_{prompt[:20]}"
    
    class MockCLIPReward:
        def calculate_reward(self, image, text, original_prompt):
            return np.random.uniform(0.3, 0.8)
    
    # í…ŒìŠ¤íŠ¸
    config = GRPOConfig()
    trainer = GRPOTrainer(
        vlm_model=MockVLM(),
        sd_generator=MockSDGenerator(),
        clip_reward=MockCLIPReward(),
        config=config
    )
    
    test_prompts = ["a beautiful sunset", "a cat in the garden"]
    group_data = trainer.collect_group_data(test_prompts)
    metrics = trainer.grpo_update(group_data)
    print(f"Test metrics: {metrics}") 