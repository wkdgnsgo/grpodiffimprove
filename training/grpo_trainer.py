"""
GRPO Trainer for QWEN Model (Enhanced with EasyR1 Implementation)
================================================================

QWEN ëª¨ë¸ì„ GRPO ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ í•™ìŠµì‹œí‚¤ëŠ” íŠ¸ë ˆì´ë„ˆì…ë‹ˆë‹¤.
EasyR1ì˜ êµ¬í˜„ì„ ì°¸ì¡°í•´ì„œ ê°œì„ ëœ GRPO ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©í•©ë‹ˆë‹¤.

í•µì‹¬ ê°œì„ ì‚¬í•­:
- ì •í™•í•œ GRPO advantage ê³„ì‚° (ê·¸ë£¹ ë‚´ ì •ê·œí™”)
- ì ì‘í˜• KL Controller
- ì´ì¤‘ í´ë¦¬í•‘ policy loss
- ìˆ˜ì¹˜ì  ì•ˆì •ì„± ê°œì„ 

ê¸°ë°˜ ì½”ë“œ: EasyR1 verl/trainer/core_algos.py

Author: AI Assistant
Date: 2025-01-22
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.distributions import Categorical
from typing import List, Dict, Tuple, Optional, Any, Literal
import numpy as np
import logging
import copy
import os
import json
from datetime import datetime
from dataclasses import dataclass
from collections import defaultdict
from abc import ABC, abstractmethod

logger = logging.getLogger(__name__)

# ===============================
# KL Controllers (from EasyR1)
# ===============================

class KLController(ABC):
    """KL coefficient controller base class"""
    kl_coef: float

    @abstractmethod
    def update(self, current_kl: float, n_steps: int):
        """Update kl_coef according to current KL."""
        ...

class AdaptiveKLController(KLController):
    """Adaptive KL controller from EasyR1"""

    def __init__(self, init_kl_coef: float, target_kl: float, horizon: float):
        self.kl_coef = init_kl_coef
        self.target = target_kl
        self.horizon = horizon

    def update(self, current_kl: float, n_steps: int):
        target = self.target
        proportional_error = np.clip(current_kl / target - 1, -0.2, 0.2)
        mult = 1 + proportional_error * n_steps / self.horizon
        self.kl_coef *= mult

class FixedKLController(KLController):
    """Fixed KL controller from EasyR1"""

    def __init__(self, init_kl_coef: float):
        self.kl_coef = init_kl_coef

    def update(self, current_kl: float, n_steps: int):
        pass

# ===============================
# Enhanced GRPO Config
# ===============================

@dataclass
class GRPOConfig:
    """Enhanced GRPO í•™ìŠµ ì„¤ì • (EasyR1 ê¸°ë°˜)"""
    # í•™ìŠµ íŒŒë¼ë¯¸í„°
    learning_rate: float = 1e-6  # ë” ì•ˆì •ì ì¸ í•™ìŠµë¥ 
    group_size: int = 4              # ê·¸ë£¹ë‹¹ rollout ìˆ˜
    num_iterations: int = 100        # ì „ì²´ í•™ìŠµ iteration
    grpo_epochs: int = 3             # ê° iterationë‹¹ ìµœì í™” epoch
    
    # GRPO í•˜ì´í¼íŒŒë¼ë¯¸í„°
    gamma: float = 0.99              # í• ì¸ íŒ©í„°
    
    # KL ì œì–´
    kl_type: str = "adaptive"        # "adaptive" or "fixed"
    kl_coef: float = 0.02            # ì´ˆê¸° KL ê³„ìˆ˜
    kl_target: float = 0.01          # ëª©í‘œ KL (adaptiveìš©)
    kl_horizon: float = 1000         # KL ì ì‘ horizon
    kl_penalty: str = "kl"          # "kl", "abs", "mse", "low_var_kl", "full"
    
    # Policy í´ë¦¬í•‘ (ì´ì¤‘ í´ë¦¬í•‘)
    clip_ratio_low: float = 0.2      # í•˜í•œ í´ë¦¬í•‘
    clip_ratio_high: float = 0.2     # ìƒí•œ í´ë¦¬í•‘  
    clip_ratio_dual: float = 4.0     # ì´ì¤‘ í´ë¦¬í•‘
    
    # ê¸°íƒ€
    entropy_coeff: float = 0.01      # ì—”íŠ¸ë¡œí”¼ ë³´ë„ˆìŠ¤
    max_grad_norm: float = 1.0       # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
    
    # í† í° ìƒì„± íŒŒë¼ë¯¸í„°
    max_new_tokens: int = 10         # placeholderì— ì¶”ê°€í•  ìµœëŒ€ í† í° ìˆ˜
    temperature: float = 0.8         # ìƒ˜í”Œë§ ì˜¨ë„
    
    # ì‹œìŠ¤í…œ ì„¤ì •
    device: str = "cuda"
    epsilon_std: float = 1e-6        # ìˆ˜ì¹˜ ì•ˆì •ì„±
    loss_avg_mode: str = "token"     # "token" or "seq"
    
    # ì €ì¥ ì„¤ì •
    save_training_data: bool = True  # í•™ìŠµ ë°ì´í„° ì €ì¥ ì—¬ë¶€
    save_dir: str = "training_results"  # ì €ì¥ ë””ë ‰í† ë¦¬

# ===============================
# Utility Functions (from EasyR1)
# ===============================

def masked_mean(values: torch.Tensor, mask: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:
    """Compute masked mean"""
    return (values * mask).sum() / (mask.sum() + eps)

def masked_whiten(values: torch.Tensor, mask: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:
    """Whiten values with mask"""
    values_masked = values * mask
    mean = masked_mean(values_masked, mask, eps)
    var = masked_mean((values_masked - mean) ** 2, mask, eps)
    return (values_masked - mean) / (var.sqrt() + eps)

def average_loss(values: torch.Tensor, mask: torch.Tensor, mode: str = "token", eps: float = 1e-8) -> torch.Tensor:
    """Average the loss"""
    if mode == "token":
        return masked_mean(values, mask, eps=eps)
    elif mode == "seq":
        return ((values * mask).sum(-1) / (mask.sum(-1) + eps)).mean()
    else:
        raise NotImplementedError(f"Unknown mode: {mode}.")

# ===============================
# Enhanced Prompt Environment
# ===============================

class PromptEnvironment:
    """
    í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ ìƒì„± í™˜ê²½ (EasyR1 ê¸°ë°˜)
    """
    
    def __init__(self, qwen_model, sd3_generator, clip_calculator, config: GRPOConfig):
        self.qwen_model = qwen_model
        self.sd3_generator = sd3_generator
        self.clip_calculator = clip_calculator
        self.config = config
        
        # ì–´íœ˜ ì„¤ì •
        self.tokenizer = qwen_model.tokenizer
        self.vocab_size = len(self.tokenizer)
        
        # ìœ ìš©í•œ í† í°ë“¤ì„ í•„í„°ë§
        self.useful_token_ids = []
        for token_id in range(min(10000, self.vocab_size)):
            token_text = self.tokenizer.decode([token_id]).strip()
            
            if (len(token_text) >= 2 and  
                token_text.isalpha() and  
                not token_text.startswith('<') and  
                not token_text.startswith('[') and
                token_id not in [self.tokenizer.pad_token_id, self.tokenizer.eos_token_id]):
                self.useful_token_ids.append(token_id)
        
        self.action_space_size = len(self.useful_token_ids)
        
        # ì €ì¥ ê´€ë ¨ ë³€ìˆ˜ ì´ˆê¸°í™”
        self.current_iteration = 0
        self.current_rollout = 0
        self.current_step = 0
        
        logger.info(f"ğŸ® Enhanced Environment initialized with {self.action_space_size} useful tokens")
    
    def set_iteration_info(self, iteration: int, rollout: int):
        """í˜„ì¬ iterationê³¼ rollout ì •ë³´ ì„¤ì •"""
        self.current_iteration = iteration
        self.current_rollout = rollout
        self.current_step = 0
    
    def reset(self, user_prompt: str) -> torch.Tensor:
        """í™˜ê²½ ë¦¬ì…‹"""
        self.user_prompt = user_prompt
        base_placeholder = ", high quality, detailed"
        self.current_prompt = user_prompt + base_placeholder
        
        self.current_token_ids = self.tokenizer.encode(
            self.current_prompt, 
            add_special_tokens=False,
            return_tensors="pt"
        ).squeeze(0)
        
        self.current_step = 0
        state = self._get_state()
        return state
    
    def _get_state(self) -> torch.Tensor:
        """í˜„ì¬ ìƒíƒœ ë°˜í™˜ (í–¥ìƒëœ ìˆ˜ì¹˜ì  ì•ˆì •ì„±)"""
        max_state_tokens = 20
        if len(self.current_token_ids) > max_state_tokens:
            state_token_ids = self.current_token_ids[-max_state_tokens:]
        else:
            state_token_ids = self.current_token_ids
        
        device = next(self.qwen_model.model.parameters()).device
        state_token_ids = state_token_ids.to(device)
        
        with torch.no_grad():
            embeddings = self.qwen_model.model.get_input_embeddings()(state_token_ids.unsqueeze(0))
            state = embeddings.mean(dim=1).squeeze(0)
            
            # ìˆ˜ì¹˜ì  ì•ˆì •ì„±ì„ ìœ„í•œ ì •ê·œí™”
            state = F.normalize(state, dim=-1, eps=self.config.epsilon_std)
        
        return state
    
    def step(self, action: int) -> Tuple[torch.Tensor, float, bool]:
        """ì•¡ì…˜ ì‹¤í–‰ (ê°œì„ ëœ ë³´ìƒ ê³„ì‚°)"""
        if action < len(self.useful_token_ids):
            selected_token_id = self.useful_token_ids[action]
        else:
            selected_token_id = self.useful_token_ids[0]
        
        selected_token_text = self.tokenizer.decode([selected_token_id])
        self.current_prompt += " " + selected_token_text.strip()
        
        self.current_token_ids = self.tokenizer.encode(
            self.current_prompt,
            add_special_tokens=False,
            return_tensors="pt"
        ).squeeze(0)
        
        next_state = self._get_state()
        
        current_new_tokens = len(self.current_token_ids) - len(self.tokenizer.encode(
            self.user_prompt, add_special_tokens=False
        ))
        done = current_new_tokens >= self.config.max_new_tokens
        
        if done:
            reward = self._calculate_reward()
        else:
            reward = 0.0
        
        self.current_step += 1
        return next_state, reward, done
    
    def _save_step_data(self, original_image, enhanced_image, reward: float):
        """ìŠ¤í… ë°ì´í„° ì €ì¥"""
        if not self.config.save_training_data:
            return
        
        try:
            step_dir = os.path.join(
                self.config.save_dir,
                f"iteration_{self.current_iteration:03d}",
                f"rollout_{self.current_rollout:02d}",
                f"step_{self.current_step:02d}"
            )
            os.makedirs(step_dir, exist_ok=True)
            
            with open(os.path.join(step_dir, "original_prompt.txt"), "w", encoding="utf-8") as f:
                f.write(self.user_prompt)
            
            with open(os.path.join(step_dir, "enhanced_prompt.txt"), "w", encoding="utf-8") as f:
                f.write(self.current_prompt)
            
            if original_image is not None:
                original_image.save(os.path.join(step_dir, "original_image.png"))
            
            if enhanced_image is not None:
                enhanced_image.save(os.path.join(step_dir, "enhanced_image.png"))
            
            metadata = {
                "iteration": self.current_iteration,
                "rollout": self.current_rollout,
                "step": self.current_step,
                "original_prompt": self.user_prompt,
                "enhanced_prompt": self.current_prompt,
                "reward": reward,
                "timestamp": datetime.now().isoformat(),
                "tokens_added": len(self.current_token_ids) - len(self.tokenizer.encode(self.user_prompt, add_special_tokens=False))
            }
            
            with open(os.path.join(step_dir, "metadata.json"), "w", encoding="utf-8") as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)
            
            logger.debug(f"ğŸ’¾ Saved step data to {step_dir}")
            
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to save step data: {e}")
    
    def _calculate_reward(self) -> float:
        """CLIPì„ ì‚¬ìš©í•œ ë³´ìƒ ê³„ì‚°"""
        try:
            logger.debug(f"ğŸ–¼ï¸ Generating original image with: '{self.user_prompt}'")
            original_image = self.sd3_generator.generate_image(self.user_prompt)
            
            logger.debug(f"ğŸ–¼ï¸ Generating enhanced image with: '{self.current_prompt[:50]}...'")
            enhanced_image = self.sd3_generator.generate_image(self.current_prompt)
            
            if original_image is None and enhanced_image is None:
                return 0.0
            
            if enhanced_image is not None:
                reward = self.clip_calculator.calculate_reward(self.user_prompt, enhanced_image)
            else:
                reward = 0.0
            
            self._save_step_data(original_image, enhanced_image, reward)
            
            logger.debug(f"ğŸ¯ Reward: {reward:.3f} for '{self.user_prompt}' -> '{self.current_prompt[:50]}...'")
            return reward
            
        except Exception as e:
            logger.warning(f"âš ï¸ Reward calculation failed: {e}")
            return 0.0
    
    def get_action_space_size(self) -> int:
        return self.action_space_size
    
    def get_state_dimension(self) -> int:
        return self.qwen_model.model.config.hidden_size

# ===============================
# Enhanced GRPO Trainer
# ===============================

class GRPOTrainer:
    """
    í–¥ìƒëœ GRPO íŠ¸ë ˆì´ë„ˆ (EasyR1 ê¸°ë°˜)
    """
    
    def __init__(self, qwen_model, sd3_generator, clip_calculator, config: GRPOConfig):
        self.qwen_model = qwen_model
        self.sd3_generator = sd3_generator
        self.clip_calculator = clip_calculator
        self.config = config
        
        # í™˜ê²½ ì´ˆê¸°í™”
        self.env = PromptEnvironment(qwen_model, sd3_generator, clip_calculator, config)
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        if config.device == "auto":
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = torch.device(config.device)
        
        # ì •ì±… ë„¤íŠ¸ì›Œí¬ (ìˆ˜ì¹˜ì  ì•ˆì •ì„± ê°œì„ )
        qwen_dtype = next(self.qwen_model.model.parameters()).dtype
        self.action_head = nn.Linear(
            self.env.get_state_dimension(),
            self.env.get_action_space_size(),
            dtype=qwen_dtype
        ).to(self.device)
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        nn.init.xavier_uniform_(self.action_head.weight)
        nn.init.zeros_(self.action_head.bias)
        
        # ì˜µí‹°ë§ˆì´ì € (ê°œì„ ëœ ì„¤ì •)
        self.optimizer = optim.AdamW(
            list(self.qwen_model.model.parameters()) + list(self.action_head.parameters()),
            lr=config.learning_rate,
            eps=1e-8,
            weight_decay=1e-6,
            betas=(0.9, 0.95)  # ë” ì•ˆì •ì ì¸ ë² íƒ€ ê°’
        )
        
        # KL Controller ì´ˆê¸°í™”
        self.kl_controller = self._get_kl_controller()
        
        # Reference model (frozen)
        self.ref_model = copy.deepcopy(self.qwen_model.model)
        self.ref_model.eval()
        for param in self.ref_model.parameters():
            param.requires_grad = False
        
        # í•™ìŠµ í†µê³„
        self.iteration_rewards = []
        self.iteration_policy_losses = []
        self.iteration_entropies = []
        self.iteration_kl_divs = []
        self.current_iteration = 0
        
        logger.info(f"ğŸš€ Enhanced GRPO Trainer initialized with KL type: {config.kl_type}")
    
    def _get_kl_controller(self) -> KLController:
        """KL Controller ìƒì„±"""
        if self.config.kl_type == "fixed":
            return FixedKLController(init_kl_coef=self.config.kl_coef)
        elif self.config.kl_type == "adaptive":
            return AdaptiveKLController(
                init_kl_coef=self.config.kl_coef,
                target_kl=self.config.kl_target,
                horizon=self.config.kl_horizon,
            )
        else:
            raise ValueError(f"Unknown kl type: {self.config.kl_type}.")
    
    def get_action_distribution(self, state: torch.Tensor) -> Categorical:
        """ìƒíƒœì—ì„œ ì•¡ì…˜ í™•ë¥  ë¶„í¬ ê³„ì‚° (ìˆ˜ì¹˜ì  ì•ˆì •ì„± ê°œì„ )"""
        state = state.to(dtype=self.action_head.weight.dtype)
        action_logits = self.action_head(state)
        
        # NaN ë° ë¬´í•œê°’ ì²˜ë¦¬
        if torch.isnan(action_logits).any() or torch.isinf(action_logits).any():
            logger.warning("âš ï¸ NaN or Inf detected in action logits, using uniform distribution")
            action_logits = torch.zeros_like(action_logits)
        
        # ë¡œì§“ í´ë¦¬í•‘ (ë” ê°•í•œ í´ë¦¬í•‘)
        action_logits = torch.clamp(action_logits, min=-20.0, max=20.0)
        
        return Categorical(logits=action_logits)
    
    @torch.no_grad()
    def compute_grpo_outcome_advantage(
        self, token_level_rewards: torch.Tensor, response_mask: torch.Tensor, 
        index: torch.Tensor, eps: float = 1e-6
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        EasyR1 ìŠ¤íƒ€ì¼ GRPO advantage ê³„ì‚°
        ê°™ì€ í”„ë¡¬í”„íŠ¸ì˜ ì—¬ëŸ¬ rolloutì„ ê·¸ë£¹í™”í•´ì„œ ì •ê·œí™”
        """
        scores = token_level_rewards.sum(dim=-1)
        id2score = defaultdict(list)
        id2mean, id2std = {}, {}

        bsz = scores.shape[0]
        for i in range(bsz):
            id2score[index[i].item()].append(scores[i])

        for idx in id2score:
            if len(id2score[idx]) > 1:
                id2mean[idx] = torch.mean(torch.stack(id2score[idx]))
                id2std[idx] = torch.std(torch.stack(id2score[idx]))
            else:
                # ë‹¨ì¼ ìƒ˜í”Œì¸ ê²½ìš° 0ìœ¼ë¡œ ì„¤ì •
                id2mean[idx] = scores[0].clone()  # ì„ì‹œê°’
                id2std[idx] = torch.tensor(1.0, device=scores.device)

        for i in range(bsz):
            if len(id2score[index[i].item()]) > 1:
                scores[i] = (scores[i] - id2mean[index[i].item()]) / (id2std[index[i].item()] + eps)
            else:
                scores[i] = 0.0  # ë‹¨ì¼ ìƒ˜í”Œì€ advantage 0

        returns = scores.unsqueeze(-1) * response_mask
        return returns, returns
    
    def compute_kl(self, log_probs: torch.Tensor, ref_log_probs: torch.Tensor) -> torch.Tensor:
        """KL divergence ê³„ì‚° (EasyR1 ìŠ¤íƒ€ì¼)"""
        log_probs, ref_log_probs = log_probs.float(), ref_log_probs.float()
        
        if self.config.kl_penalty == "kl":
            return log_probs - ref_log_probs
        elif self.config.kl_penalty == "abs":
            return (log_probs - ref_log_probs).abs()
        elif self.config.kl_penalty == "mse":
            return 0.5 * (log_probs - ref_log_probs).square()
        elif self.config.kl_penalty == "low_var_kl":
            kl = (ref_log_probs - log_probs).clamp(-20.0, 20.0)
            kld = (kl.exp() - kl - 1).contiguous()
            return torch.clamp(kld, min=-10.0, max=10.0)
        elif self.config.kl_penalty == "full":
            return F.kl_div(ref_log_probs, log_probs, log_target=True, reduction="none").sum(-1)
        else:
            raise NotImplementedError(f"Unknown KL penalty: {self.config.kl_penalty}.")
    
    def compute_policy_loss(
        self, old_log_probs: torch.Tensor, log_probs: torch.Tensor,
        advantages: torch.Tensor, response_mask: torch.Tensor
    ) -> Tuple[torch.Tensor, Dict[str, float]]:
        """
        EasyR1 ìŠ¤íƒ€ì¼ ì´ì¤‘ í´ë¦¬í•‘ policy loss ê³„ì‚°
        """
        negative_approx_kl = log_probs - old_log_probs
        # KL í´ë¦¬í•‘ìœ¼ë¡œ ìˆ˜ì¹˜ì  ì•ˆì •ì„± í™•ë³´
        negative_approx_kl = torch.clamp(negative_approx_kl, -20.0, 20.0)
        ratio = torch.exp(negative_approx_kl)
        
        # í´ë¦¬í•‘ëœ ratio ê³„ì‚°
        clipped_ratio = torch.exp(
            torch.clamp(
                negative_approx_kl, 
                np.log(1.0 - self.config.clip_ratio_low), 
                np.log(1.0 + self.config.clip_ratio_high)
            )
        )

        # Metrics ê³„ì‚°
        metrics = {"ppo_kl": masked_mean(-negative_approx_kl, response_mask).item()}
        metrics["entropy_loss"] = average_loss(-log_probs, response_mask, mode=self.config.loss_avg_mode).item()

        # 3ê°€ì§€ loss ê³„ì‚°
        pg_loss = -advantages * ratio
        pg_loss2 = -advantages * clipped_ratio
        pg_loss3 = -advantages * self.config.clip_ratio_dual

        # ì´ì¤‘ í´ë¦¬í•‘ ì ìš©
        clipped_pg_loss_higher = torch.max(pg_loss, pg_loss2)
        metrics["pg_clipfrac_higher"] = masked_mean((pg_loss < pg_loss2).float(), response_mask).item()
        
        clipped_pg_loss_lower = torch.min(clipped_pg_loss_higher, pg_loss3)
        final_pg_loss = torch.where(advantages < 0, clipped_pg_loss_lower, clipped_pg_loss_higher)
        metrics["pg_clipfrac_lower"] = masked_mean(
            (clipped_pg_loss_higher > pg_loss3).float() * (advantages < 0).float(), 
            response_mask
        ).item()

        final_pg_loss = average_loss(final_pg_loss, response_mask, mode=self.config.loss_avg_mode)
        
        return final_pg_loss, metrics

    def collect_group_trajectories(self, user_prompts: List[str]) -> Dict[str, Any]:
        """
        ê·¸ë£¹ ê¶¤ì  ìˆ˜ì§‘ (CartPole ì½”ë“œ ê¸°ë°˜)
        """
        logger.info(f"ğŸ”„ Collecting trajectories for {len(user_prompts)} prompts...")
        
        group_states_list = []
        group_actions_list = []
        group_log_probs_old_list = []
        group_rewards_list = []
        
        episode_rewards_in_iter = []
        episode_lengths_in_iter = []
        
        # í‰ê°€ ëª¨ë“œë¡œ rollout
        self.qwen_model.model.eval()
        self.action_head.eval()
        
        for rollout_idx, user_prompt in enumerate(user_prompts):
            rollout_states = []
            rollout_actions = []
            rollout_log_probs = []
            rollout_rewards = []
            
            # í˜„ì¬ iterationê³¼ rollout ì •ë³´ ì„¤ì •
            self.env.set_iteration_info(getattr(self, 'current_iteration', 0), rollout_idx)
            
            # í™˜ê²½ ë¦¬ì…‹
            state = self.env.reset(user_prompt)
            episode_reward = 0.0
            episode_steps = 0
            done = False
            
            while not done and episode_steps < self.config.max_new_tokens:
                state_tensor = state.to(self.device)
                
                # ì•¡ì…˜ ì„ íƒ
                with torch.no_grad():
                    action_dist = self.get_action_distribution(state_tensor)
                    action_tensor = action_dist.sample()
                    log_prob = action_dist.log_prob(action_tensor)
                
                # í™˜ê²½ì—ì„œ ìŠ¤í… ì‹¤í–‰
                next_state, reward, done = self.env.step(action_tensor.item())
                
                # ë°ì´í„° ì €ì¥
                rollout_states.append(state_tensor)
                rollout_actions.append(action_tensor)
                rollout_log_probs.append(log_prob)
                rollout_rewards.append(reward)
                
                state = next_state
                episode_reward += reward
                episode_steps += 1
            
            # ì™„ë£Œëœ rollout ì €ì¥
            if rollout_states:
                group_states_list.append(torch.stack(rollout_states))
                group_actions_list.append(torch.stack(rollout_actions).squeeze())
                group_log_probs_old_list.append(torch.stack(rollout_log_probs).squeeze())
                group_rewards_list.append(rollout_rewards)
            
            episode_rewards_in_iter.append(episode_reward)
            episode_lengths_in_iter.append(episode_steps)
        
        # í›ˆë ¨ ëª¨ë“œë¡œ ë³µì›
        self.qwen_model.model.train()
        self.action_head.train()
        
        return {
            'states': group_states_list,
            'actions': group_actions_list,
            'log_probs_old': group_log_probs_old_list,
            'rewards': group_rewards_list,
            'episode_rewards': episode_rewards_in_iter,
            'episode_lengths': episode_lengths_in_iter
        }
    
    def calculate_group_advantages(self, group_data: Dict[str, Any]) -> List[torch.Tensor]:
        """
        ê·¸ë£¹ ìƒëŒ€ì  ì–´ë“œë°´í‹°ì§€ ê³„ì‚° (CartPole ì½”ë“œ ê¸°ë°˜)
        """
        group_rewards_list = group_data['rewards']
        
        # 1ë‹¨ê³„: í• ì¸ëœ ë¦¬í„´ ê³„ì‚°
        all_raw_advantages = []
        temp_raw_advantages_tensors = []
        
        for rollout_rewards in group_rewards_list:
            rollout_len = len(rollout_rewards)
            rollout_advantages = torch.zeros(rollout_len, dtype=torch.float32, device=self.device)
            
            if rollout_len > 0:
                # í• ì¸ëœ ë¦¬í„´ ê³„ì‚°
                discounted_return = 0.0
                for t in reversed(range(rollout_len)):
                    discounted_return = rollout_rewards[t] + self.config.gamma * discounted_return
                    rollout_advantages[t] = discounted_return
                
                temp_raw_advantages_tensors.append(rollout_advantages)
                all_raw_advantages.extend(rollout_advantages.cpu().numpy())
            else:
                temp_raw_advantages_tensors.append(torch.empty((0,), device=self.device))
        
        # 2ë‹¨ê³„: ê·¸ë£¹ ì •ê·œí™”
        if len(all_raw_advantages) > 1:
            group_mean = np.mean(all_raw_advantages)
            group_std = np.std(all_raw_advantages)
        else:
            group_mean = 0.0
            group_std = 1.0
        
        # 3ë‹¨ê³„: ì •ê·œí™”ëœ ì–´ë“œë°´í‹°ì§€ ê³„ì‚°
        group_advantages_list = []
        for raw_advantages in temp_raw_advantages_tensors:
            if raw_advantages.nelement() > 0:
                normalized_advantages = (raw_advantages - group_mean) / (group_std + self.config.epsilon_std)
            else:
                normalized_advantages = raw_advantages
            group_advantages_list.append(normalized_advantages)
        
        return group_advantages_list
    
    def update_grpo(self, group_data: Dict[str, Any], group_advantages: List[torch.Tensor]) -> Tuple[float, float, float]:
        """
        GRPO ì—…ë°ì´íŠ¸ (CartPole ì½”ë“œ ê¸°ë°˜)
        """
        # ë°ì´í„° ì—°ê²°
        states = torch.cat(group_data['states'], dim=0).to(self.device)
        actions = torch.cat(group_data['actions'], dim=0).to(self.device)
        log_probs_old = torch.cat(group_data['log_probs_old'], dim=0).to(self.device)
        advantages = torch.cat(group_advantages, dim=0).to(self.device)
        
        # ìƒìˆ˜ë¡œ ê³ ì •
        advantages = advantages.detach()
        log_probs_old = log_probs_old.detach()
        
        # ì°¸ì¡° ëª¨ë¸ ìƒì„± (í˜„ì¬ ëª¨ë¸ ìƒíƒœ ë³µì‚¬)
        action_head_ref = copy.deepcopy(self.action_head)
        action_head_ref.eval()
        
        total_policy_objective = 0.0
        total_kl_div = 0.0
        total_entropy = 0.0
        
        for epoch in range(self.config.grpo_epochs):
            # í˜„ì¬ ì •ì±…ìœ¼ë¡œ í™•ë¥  ê³„ì‚°
            action_dist = self.get_action_distribution(states)
            log_probs_new = action_dist.log_prob(actions)
            entropy = action_dist.entropy().mean()
            
            # ë¹„ìœ¨ ê³„ì‚°
            ratio = torch.exp(log_probs_new - log_probs_old)
            
            # í´ë¦¬í•‘ëœ ì„œë¡œê²Œì´íŠ¸ ëª©ì  í•¨ìˆ˜
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 1.0 - self.config.grpo_clip_epsilon, 1.0 + self.config.grpo_clip_epsilon) * advantages
            clipped_surrogate_objective = torch.min(surr1, surr2).mean()
            
            # KL ë°œì‚° ê³„ì‚°
            with torch.no_grad():
                action_dist_ref = Categorical(logits=action_head_ref(states))
                log_probs_ref = action_dist_ref.log_prob(actions)
            
            log_ratio_ref_curr = log_probs_ref - log_probs_new.detach()
            kl_div_estimate = torch.exp(log_ratio_ref_curr) - log_ratio_ref_curr - 1
            kl_div_estimate_mean = torch.relu(kl_div_estimate.mean())
            
            # ì „ì²´ ì†ì‹¤
            policy_loss = -clipped_surrogate_objective + self.config.grpo_kl_beta * kl_div_estimate_mean - self.config.entropy_coeff * entropy
            
            # ìµœì í™”
            self.optimizer.zero_grad()
            policy_loss.backward()
            torch.nn.utils.clip_grad_norm_(
                list(self.qwen_model.model.parameters()) + list(self.action_head.parameters()),
                max_norm=1.0
            )
            self.optimizer.step()
            
            # í†µê³„ ìˆ˜ì§‘
            total_policy_objective += clipped_surrogate_objective.item()
            total_kl_div += kl_div_estimate_mean.item()
            total_entropy += entropy.item()
        
        # í‰ê·  ê³„ì‚°
        avg_policy_objective = total_policy_objective / self.config.grpo_epochs
        avg_kl_div = total_kl_div / self.config.grpo_epochs
        avg_entropy = total_entropy / self.config.grpo_epochs
        
        return avg_policy_objective, avg_kl_div, avg_entropy
    
    def train_iteration(self, user_prompts: List[str]) -> Dict[str, float]:
        """
        í•˜ë‚˜ì˜ GRPO í•™ìŠµ iteration ì‹¤í–‰
        """
        # 1. ê¶¤ì  ìˆ˜ì§‘
        group_data = self.collect_group_trajectories(user_prompts)
        
        # 2. ì–´ë“œë°´í‹°ì§€ ê³„ì‚°
        group_advantages = self.calculate_group_advantages(group_data)
        
        # 3. GRPO ì—…ë°ì´íŠ¸
        avg_policy_obj, avg_kl, avg_entropy = self.update_grpo(group_data, group_advantages)
        
        # 4. í†µê³„ ê¸°ë¡
        avg_reward = np.mean(group_data['episode_rewards']) if group_data['episode_rewards'] else 0.0
        avg_length = np.mean(group_data['episode_lengths']) if group_data['episode_lengths'] else 0.0
        
        self.iteration_rewards.append(avg_reward)
        self.iteration_policy_losses.append(avg_policy_obj)
        self.iteration_entropies.append(avg_entropy)
        self.iteration_kl_divs.append(avg_kl)
        
        return {
            'avg_reward': avg_reward,
            'avg_length': avg_length,
            'policy_objective': avg_policy_obj,
            'kl_divergence': avg_kl,
            'entropy': avg_entropy
        } 