"""
GRPO Trainer for QWEN Model
===========================

QWEN ëª¨ë¸ì„ GRPO ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ í•™ìŠµì‹œí‚¤ëŠ” íŠ¸ë ˆì´ë„ˆì…ë‹ˆë‹¤.

í•µì‹¬ ì•„ì´ë””ì–´:
- State: user_prompt + placeholder + í˜„ì¬ê¹Œì§€ ìƒì„±ëœ í† í°ë“¤
- Action: placeholderì— ì¶”ê°€í•  ë‹¤ìŒ ë‹¨ì–´/í† í° ì„ íƒ
- Environment: SD3ë¡œ ì´ë¯¸ì§€ ìƒì„±
- Reward: CLIP(original_user_prompt, generated_image)
- Reference Model: í•™ìŠµ ì „ QWEN ëª¨ë¸

ê¸°ë°˜ ì½”ë“œ: grpo-cartpole.ipynb

Author: AI Assistant
Date: 2025-01-22
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.distributions import Categorical
from typing import List, Dict, Tuple, Optional, Any
import numpy as np
import logging
import copy
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class GRPOConfig:
    """GRPO í•™ìŠµ ì„¤ì •"""
    # í•™ìŠµ íŒŒë¼ë¯¸í„°
    learning_rate: float = 1e-5
    group_size: int = 4              # ê·¸ë£¹ë‹¹ rollout ìˆ˜
    num_iterations: int = 100        # ì „ì²´ í•™ìŠµ iteration
    grpo_epochs: int = 3             # ê° iterationë‹¹ ìµœì í™” epoch
    
    # GRPO í•˜ì´í¼íŒŒë¼ë¯¸í„°
    gamma: float = 0.99              # í• ì¸ íŒ©í„°
    grpo_kl_beta: float = 0.01       # KL ë°œì‚° í˜ë„í‹°
    grpo_clip_epsilon: float = 0.2   # í´ë¦¬í•‘ íŒŒë¼ë¯¸í„°
    entropy_coeff: float = 0.01      # ì—”íŠ¸ë¡œí”¼ ë³´ë„ˆìŠ¤
    
    # í† í° ìƒì„± íŒŒë¼ë¯¸í„°
    max_new_tokens: int = 10         # placeholderì— ì¶”ê°€í•  ìµœëŒ€ í† í° ìˆ˜
    temperature: float = 0.8         # ìƒ˜í”Œë§ ì˜¨ë„
    
    # ì‹œìŠ¤í…œ ì„¤ì •
    device: str = "cuda"
    epsilon_std: float = 1e-8        # ìˆ˜ì¹˜ ì•ˆì •ì„±

class PromptEnvironment:
    """
    í”„ë¡¬í”„íŠ¸ ìƒì„± í™˜ê²½
    
    - State: user_prompt + placeholder + generated_tokens
    - Action: ë‹¤ìŒ ë‹¨ì–´ ì„ íƒ
    - Reward: CLIP similarity
    """
    
    def __init__(self, qwen_model, sd3_generator, clip_calculator, config: GRPOConfig):
        self.qwen_model = qwen_model
        self.sd3_generator = sd3_generator
        self.clip_calculator = clip_calculator
        self.config = config
        
        # ì–´íœ˜ ì„¤ì •
        self.tokenizer = qwen_model.tokenizer
        self.vocab_size = len(self.tokenizer)
        
        # ììœ ë¡œìš´ í† í° ìƒì„±ì„ ìœ„í•œ ì–´íœ˜ ì„¤ì •
        # ì „ì²´ vocabularyì—ì„œ ì¼ë¶€ë¥¼ ì„ íƒ (ë„ˆë¬´ í¬ë©´ ë©”ëª¨ë¦¬ ë¬¸ì œ)
        self.vocab_size = len(self.tokenizer)
        
        # ìœ ìš©í•œ í† í°ë“¤ì„ í•„í„°ë§ (íŠ¹ìˆ˜ í† í°, ë„ˆë¬´ ì§§ì€ í† í° ì œì™¸)
        self.useful_token_ids = []
        for token_id in range(min(10000, self.vocab_size)):  # ì²˜ìŒ 10k í† í°ë§Œ ì‚¬ìš©
            token_text = self.tokenizer.decode([token_id]).strip()
            
            # í•„í„°ë§ ì¡°ê±´
            if (len(token_text) >= 2 and  # 2ê¸€ì ì´ìƒ
                token_text.isalpha() and  # ì•ŒíŒŒë²³ë§Œ
                not token_text.startswith('<') and  # íŠ¹ìˆ˜ í† í° ì œì™¸
                not token_text.startswith('[') and
                token_id not in [self.tokenizer.pad_token_id, self.tokenizer.eos_token_id]):
                self.useful_token_ids.append(token_id)
        
        self.action_space_size = len(self.useful_token_ids)
        
        logger.info(f"ğŸ® Environment initialized with {self.action_space_size} useful tokens (unrestricted vocabulary)")
    
    def reset(self, user_prompt: str) -> torch.Tensor:
        """
        í™˜ê²½ ë¦¬ì…‹ - ìƒˆë¡œìš´ user promptë¡œ ì‹œì‘
        
        Returns:
            ì´ˆê¸° state (user_prompt + base_placeholderì˜ ì„ë² ë”©)
        """
        self.user_prompt = user_prompt
        
        # ê¸°ë³¸ placeholder ì¶”ê°€
        base_placeholder = ", high quality, detailed"
        self.current_prompt = user_prompt + base_placeholder
        
        # í˜„ì¬ ìƒíƒœë¥¼ í† í° IDë¡œ ë³€í™˜ (CPUì—ì„œ ì²˜ë¦¬)
        self.current_token_ids = self.tokenizer.encode(
            self.current_prompt, 
            add_special_tokens=False,
            return_tensors="pt"
        ).squeeze(0)
        
        # StateëŠ” í˜„ì¬ í”„ë¡¬í”„íŠ¸ì˜ ë§ˆì§€ë§‰ ëª‡ í† í°ì˜ ì„ë² ë”©
        state = self._get_state()
        
        return state
    
    def _get_state(self) -> torch.Tensor:
        """
        í˜„ì¬ ìƒíƒœ ë°˜í™˜ (í˜„ì¬ í”„ë¡¬í”„íŠ¸ì˜ ì„ë² ë”©)
        """
        # ë§ˆì§€ë§‰ ëª‡ í† í°ë§Œ ì‚¬ìš© (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
        max_state_tokens = 20
        if len(self.current_token_ids) > max_state_tokens:
            state_token_ids = self.current_token_ids[-max_state_tokens:]
        else:
            state_token_ids = self.current_token_ids
        
        # í† í° IDë¥¼ ì˜¬ë°”ë¥¸ ì¥ì¹˜ë¡œ ì´ë™
        device = next(self.qwen_model.model.parameters()).device
        state_token_ids = state_token_ids.to(device)
        
        # í† í° ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
        with torch.no_grad():
            embeddings = self.qwen_model.model.get_input_embeddings()(state_token_ids.unsqueeze(0))
            # í‰ê·  í’€ë§ìœ¼ë¡œ ê³ ì • í¬ê¸° state ìƒì„±
            state = embeddings.mean(dim=1).squeeze(0)  # [hidden_size]
        
        return state
    
    def step(self, action: int) -> Tuple[torch.Tensor, float, bool]:
        """
        ì•¡ì…˜ ì‹¤í–‰
        
        Args:
            action: ì„ íƒëœ í’ˆì§ˆ í† í° ì¸ë±ìŠ¤
            
        Returns:
            next_state, reward, done
        """
        # ì•¡ì…˜ì„ í† í° IDë¡œ ë³€í™˜
        if action < len(self.useful_token_ids):
            selected_token_id = self.useful_token_ids[action]
        else:
            selected_token_id = self.useful_token_ids[0]  # í´ë°±
        
        # í† í°ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€
        selected_token_text = self.tokenizer.decode([selected_token_id])
        self.current_prompt += " " + selected_token_text.strip()
        
        # í† í° ID ì—…ë°ì´íŠ¸ (CPUì—ì„œ ì²˜ë¦¬)
        self.current_token_ids = self.tokenizer.encode(
            self.current_prompt,
            add_special_tokens=False,
            return_tensors="pt"
        ).squeeze(0)
        
        # ìƒˆë¡œìš´ ìƒíƒœ ê³„ì‚°
        next_state = self._get_state()
        
        # ì—í”¼ì†Œë“œ ì¢…ë£Œ ì¡°ê±´
        current_new_tokens = len(self.current_token_ids) - len(self.tokenizer.encode(
            self.user_prompt, add_special_tokens=False
        ))
        done = current_new_tokens >= self.config.max_new_tokens
        
        # ë³´ìƒ ê³„ì‚° (ì—í”¼ì†Œë“œ ëì—ì„œë§Œ)
        if done:
            reward = self._calculate_reward()
        else:
            reward = 0.0  # ì¤‘ê°„ ìŠ¤í…ì—ì„œëŠ” ë³´ìƒ ì—†ìŒ
        
        return next_state, reward, done
    
    def _calculate_reward(self) -> float:
        """
        CLIPì„ ì‚¬ìš©í•œ ë³´ìƒ ê³„ì‚°
        
        ì¤‘ìš”: ì›ë³¸ user_promptì™€ ìƒì„±ëœ ì´ë¯¸ì§€ ê°„ì˜ ìœ ì‚¬ë„ë§Œ ê³„ì‚°!
        """
        try:
            # SD3ë¡œ ì´ë¯¸ì§€ ìƒì„± (í˜„ì¬ í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©)
            image = self.sd3_generator.generate_image(self.current_prompt)
            
            if image is None:
                return 0.0
            
            # CLIP ë³´ìƒ ê³„ì‚° (ì›ë³¸ user_prompt ì‚¬ìš©!)
            reward = self.clip_calculator.calculate_reward(self.user_prompt, image)
            
            logger.debug(f"ğŸ¯ Reward: {reward:.3f} for '{self.user_prompt}' -> '{self.current_prompt[:50]}...'")
            return reward
            
        except Exception as e:
            logger.warning(f"âš ï¸ Reward calculation failed: {e}")
            return 0.0
    
    def get_action_space_size(self) -> int:
        return self.action_space_size
    
    def get_state_dimension(self) -> int:
        return self.qwen_model.model.config.hidden_size

class GRPOTrainer:
    """
    QWEN ëª¨ë¸ì„ ìœ„í•œ GRPO íŠ¸ë ˆì´ë„ˆ
    
    CartPole GRPO ì½”ë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ í…ìŠ¤íŠ¸ ìƒì„±ì— ì ìš©
    """
    
    def __init__(self, qwen_model, sd3_generator, clip_calculator, config: GRPOConfig):
        self.qwen_model = qwen_model  # í•™ìŠµí•  ì •ì±… ë„¤íŠ¸ì›Œí¬
        self.sd3_generator = sd3_generator
        self.clip_calculator = clip_calculator
        self.config = config
        
        # í™˜ê²½ ì´ˆê¸°í™”
        self.env = PromptEnvironment(qwen_model, sd3_generator, clip_calculator, config)
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        if config.device == "auto":
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = torch.device(config.device)
        
        # ì •ì±… ë„¤íŠ¸ì›Œí¬ (QWEN ëª¨ë¸ì„ ì•¡ì…˜ í™•ë¥  ë¶„í¬ ì¶œë ¥í•˜ë„ë¡ ì–´ëŒ‘í„° ì¶”ê°€)
        self.action_head = nn.Linear(
            self.env.get_state_dimension(),
            self.env.get_action_space_size()
        ).to(self.device)
        
        # ì˜µí‹°ë§ˆì´ì € (QWEN ëª¨ë¸ + ì•¡ì…˜ í—¤ë“œ í•¨ê»˜ í•™ìŠµ)
        self.optimizer = optim.Adam(
            list(self.qwen_model.model.parameters()) + list(self.action_head.parameters()),
            lr=config.learning_rate
        )
        
        # í•™ìŠµ í†µê³„
        self.iteration_rewards = []
        self.iteration_policy_losses = []
        self.iteration_entropies = []
        self.iteration_kl_divs = []
        
        logger.info(f"ğŸš€ GRPO Trainer initialized for QWEN model")
    
    def get_action_distribution(self, state: torch.Tensor) -> Categorical:
        """
        ìƒíƒœì—ì„œ ì•¡ì…˜ í™•ë¥  ë¶„í¬ ê³„ì‚°
        """
        # QWEN ëª¨ë¸ì˜ hidden stateì—ì„œ ì•¡ì…˜ ë¡œì§“ ê³„ì‚°
        action_logits = self.action_head(state)
        return Categorical(logits=action_logits)
    
    def collect_group_trajectories(self, user_prompts: List[str]) -> Dict[str, Any]:
        """
        ê·¸ë£¹ ê¶¤ì  ìˆ˜ì§‘ (CartPole ì½”ë“œ ê¸°ë°˜)
        """
        logger.info(f"ğŸ”„ Collecting trajectories for {len(user_prompts)} prompts...")
        
        group_states_list = []
        group_actions_list = []
        group_log_probs_old_list = []
        group_rewards_list = []
        
        episode_rewards_in_iter = []
        episode_lengths_in_iter = []
        
        # í‰ê°€ ëª¨ë“œë¡œ rollout
        self.qwen_model.model.eval()
        self.action_head.eval()
        
        for rollout_idx, user_prompt in enumerate(user_prompts):
            rollout_states = []
            rollout_actions = []
            rollout_log_probs = []
            rollout_rewards = []
            
            # í™˜ê²½ ë¦¬ì…‹
            state = self.env.reset(user_prompt)
            episode_reward = 0.0
            episode_steps = 0
            done = False
            
            while not done and episode_steps < self.config.max_new_tokens:
                state_tensor = state.to(self.device)
                
                # ì•¡ì…˜ ì„ íƒ
                with torch.no_grad():
                    action_dist = self.get_action_distribution(state_tensor)
                    action_tensor = action_dist.sample()
                    log_prob = action_dist.log_prob(action_tensor)
                
                # í™˜ê²½ì—ì„œ ìŠ¤í… ì‹¤í–‰
                next_state, reward, done = self.env.step(action_tensor.item())
                
                # ë°ì´í„° ì €ì¥
                rollout_states.append(state_tensor)
                rollout_actions.append(action_tensor)
                rollout_log_probs.append(log_prob)
                rollout_rewards.append(reward)
                
                state = next_state
                episode_reward += reward
                episode_steps += 1
            
            # ì™„ë£Œëœ rollout ì €ì¥
            if rollout_states:
                group_states_list.append(torch.stack(rollout_states))
                group_actions_list.append(torch.stack(rollout_actions).squeeze())
                group_log_probs_old_list.append(torch.stack(rollout_log_probs).squeeze())
                group_rewards_list.append(rollout_rewards)
            
            episode_rewards_in_iter.append(episode_reward)
            episode_lengths_in_iter.append(episode_steps)
        
        # í›ˆë ¨ ëª¨ë“œë¡œ ë³µì›
        self.qwen_model.model.train()
        self.action_head.train()
        
        return {
            'states': group_states_list,
            'actions': group_actions_list,
            'log_probs_old': group_log_probs_old_list,
            'rewards': group_rewards_list,
            'episode_rewards': episode_rewards_in_iter,
            'episode_lengths': episode_lengths_in_iter
        }
    
    def calculate_group_advantages(self, group_data: Dict[str, Any]) -> List[torch.Tensor]:
        """
        ê·¸ë£¹ ìƒëŒ€ì  ì–´ë“œë°´í‹°ì§€ ê³„ì‚° (CartPole ì½”ë“œ ê¸°ë°˜)
        """
        group_rewards_list = group_data['rewards']
        
        # 1ë‹¨ê³„: í• ì¸ëœ ë¦¬í„´ ê³„ì‚°
        all_raw_advantages = []
        temp_raw_advantages_tensors = []
        
        for rollout_rewards in group_rewards_list:
            rollout_len = len(rollout_rewards)
            rollout_advantages = torch.zeros(rollout_len, dtype=torch.float32, device=self.device)
            
            if rollout_len > 0:
                # í• ì¸ëœ ë¦¬í„´ ê³„ì‚°
                discounted_return = 0.0
                for t in reversed(range(rollout_len)):
                    discounted_return = rollout_rewards[t] + self.config.gamma * discounted_return
                    rollout_advantages[t] = discounted_return
                
                temp_raw_advantages_tensors.append(rollout_advantages)
                all_raw_advantages.extend(rollout_advantages.cpu().numpy())
            else:
                temp_raw_advantages_tensors.append(torch.empty((0,), device=self.device))
        
        # 2ë‹¨ê³„: ê·¸ë£¹ ì •ê·œí™”
        if len(all_raw_advantages) > 1:
            group_mean = np.mean(all_raw_advantages)
            group_std = np.std(all_raw_advantages)
        else:
            group_mean = 0.0
            group_std = 1.0
        
        # 3ë‹¨ê³„: ì •ê·œí™”ëœ ì–´ë“œë°´í‹°ì§€ ê³„ì‚°
        group_advantages_list = []
        for raw_advantages in temp_raw_advantages_tensors:
            if raw_advantages.nelement() > 0:
                normalized_advantages = (raw_advantages - group_mean) / (group_std + self.config.epsilon_std)
            else:
                normalized_advantages = raw_advantages
            group_advantages_list.append(normalized_advantages)
        
        return group_advantages_list
    
    def update_grpo(self, group_data: Dict[str, Any], group_advantages: List[torch.Tensor]) -> Tuple[float, float, float]:
        """
        GRPO ì—…ë°ì´íŠ¸ (CartPole ì½”ë“œ ê¸°ë°˜)
        """
        # ë°ì´í„° ì—°ê²°
        states = torch.cat(group_data['states'], dim=0).to(self.device)
        actions = torch.cat(group_data['actions'], dim=0).to(self.device)
        log_probs_old = torch.cat(group_data['log_probs_old'], dim=0).to(self.device)
        advantages = torch.cat(group_advantages, dim=0).to(self.device)
        
        # ìƒìˆ˜ë¡œ ê³ ì •
        advantages = advantages.detach()
        log_probs_old = log_probs_old.detach()
        
        # ì°¸ì¡° ëª¨ë¸ ìƒì„± (í˜„ì¬ ëª¨ë¸ ìƒíƒœ ë³µì‚¬)
        action_head_ref = copy.deepcopy(self.action_head)
        action_head_ref.eval()
        
        total_policy_objective = 0.0
        total_kl_div = 0.0
        total_entropy = 0.0
        
        for epoch in range(self.config.grpo_epochs):
            # í˜„ì¬ ì •ì±…ìœ¼ë¡œ í™•ë¥  ê³„ì‚°
            action_dist = self.get_action_distribution(states)
            log_probs_new = action_dist.log_prob(actions)
            entropy = action_dist.entropy().mean()
            
            # ë¹„ìœ¨ ê³„ì‚°
            ratio = torch.exp(log_probs_new - log_probs_old)
            
            # í´ë¦¬í•‘ëœ ì„œë¡œê²Œì´íŠ¸ ëª©ì  í•¨ìˆ˜
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 1.0 - self.config.grpo_clip_epsilon, 1.0 + self.config.grpo_clip_epsilon) * advantages
            clipped_surrogate_objective = torch.min(surr1, surr2).mean()
            
            # KL ë°œì‚° ê³„ì‚°
            with torch.no_grad():
                action_dist_ref = Categorical(logits=action_head_ref(states))
                log_probs_ref = action_dist_ref.log_prob(actions)
            
            log_ratio_ref_curr = log_probs_ref - log_probs_new.detach()
            kl_div_estimate = torch.exp(log_ratio_ref_curr) - log_ratio_ref_curr - 1
            kl_div_estimate_mean = torch.relu(kl_div_estimate.mean())
            
            # ì „ì²´ ì†ì‹¤
            policy_loss = -clipped_surrogate_objective + self.config.grpo_kl_beta * kl_div_estimate_mean - self.config.entropy_coeff * entropy
            
            # ìµœì í™”
            self.optimizer.zero_grad()
            policy_loss.backward()
            torch.nn.utils.clip_grad_norm_(
                list(self.qwen_model.model.parameters()) + list(self.action_head.parameters()),
                max_norm=1.0
            )
            self.optimizer.step()
            
            # í†µê³„ ìˆ˜ì§‘
            total_policy_objective += clipped_surrogate_objective.item()
            total_kl_div += kl_div_estimate_mean.item()
            total_entropy += entropy.item()
        
        # í‰ê·  ê³„ì‚°
        avg_policy_objective = total_policy_objective / self.config.grpo_epochs
        avg_kl_div = total_kl_div / self.config.grpo_epochs
        avg_entropy = total_entropy / self.config.grpo_epochs
        
        return avg_policy_objective, avg_kl_div, avg_entropy
    
    def train_iteration(self, user_prompts: List[str]) -> Dict[str, float]:
        """
        í•˜ë‚˜ì˜ GRPO í•™ìŠµ iteration ì‹¤í–‰
        """
        # 1. ê¶¤ì  ìˆ˜ì§‘
        group_data = self.collect_group_trajectories(user_prompts)
        
        # 2. ì–´ë“œë°´í‹°ì§€ ê³„ì‚°
        group_advantages = self.calculate_group_advantages(group_data)
        
        # 3. GRPO ì—…ë°ì´íŠ¸
        avg_policy_obj, avg_kl, avg_entropy = self.update_grpo(group_data, group_advantages)
        
        # 4. í†µê³„ ê¸°ë¡
        avg_reward = np.mean(group_data['episode_rewards']) if group_data['episode_rewards'] else 0.0
        avg_length = np.mean(group_data['episode_lengths']) if group_data['episode_lengths'] else 0.0
        
        self.iteration_rewards.append(avg_reward)
        self.iteration_policy_losses.append(avg_policy_obj)
        self.iteration_entropies.append(avg_entropy)
        self.iteration_kl_divs.append(avg_kl)
        
        return {
            'avg_reward': avg_reward,
            'avg_length': avg_length,
            'policy_objective': avg_policy_obj,
            'kl_divergence': avg_kl,
            'entropy': avg_entropy
        } 