"""
GRPO (Group Relative Policy Optimization) Trainer
================================================

GRPO ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í•œ í•µì‹¬ í•™ìŠµ ëª¨ë“ˆì…ë‹ˆë‹¤.
VLM í”„ë¡¬í”„íŠ¸ ê°œì„ ì„ ìœ„í•œ ê°•í™”í•™ìŠµ íŠ¸ë ˆì´ë„ˆì…ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
1. GRPO ì •ì±… ì—…ë°ì´íŠ¸
2. ê·¸ë£¹ ê¸°ë°˜ ì–´ë“œë°´í‹°ì§€ ê³„ì‚°
3. KL ë°œì‚° í˜ë„í‹°
4. ì°¸ì¡° ëª¨ë¸ ê´€ë¦¬

GRPO vs PPO ì°¨ì´ì :
- PPO: ê°œë³„ ìƒ˜í”Œ ê¸°ë°˜ ì–´ë“œë°´í‹°ì§€
- GRPO: ê·¸ë£¹ ë‚´ ìƒëŒ€ì  ì–´ë“œë°´í‹°ì§€ (ë” ì•ˆì •ì )

Author: AI Assistant
Date: 2025-01-22
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

from typing import List, Dict, Tuple, Optional, Any
import logging
import numpy as np
import copy
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class GRPOConfig:
    """GRPO í•™ìŠµ ì„¤ì • í´ë˜ìŠ¤"""
    learning_rate: float = 1e-5
    group_size: int = 4
    num_iterations: int = 20
    grpo_epochs: int = 2
    gamma: float = 0.99           # í• ì¸ íŒ©í„°
    kl_beta: float = 0.01         # KL ë°œì‚° í˜ë„í‹° ê³„ìˆ˜
    clip_epsilon: float = 0.2     # í´ë¦¬í•‘ ë²”ìœ„
    entropy_coeff: float = 0.01   # ì—”íŠ¸ë¡œí”¼ ë³´ë„ˆìŠ¤ ê³„ìˆ˜
    max_grad_norm: float = 1.0    # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
    
    # ìƒì„± íŒŒë¼ë¯¸í„°
    max_new_tokens: int = 50
    temperature: float = 0.8
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device: str = "auto"

class GRPOTrainer:
    """
    GRPO ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•œ VLM í•™ìŠµ í´ë˜ìŠ¤
    
    ì´ í´ë˜ìŠ¤ëŠ” GRPO ë…¼ë¬¸ì˜ í•µì‹¬ ì•„ì´ë””ì–´ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤:
    1. ê·¸ë£¹ ë‚´ì—ì„œ ìƒëŒ€ì  ì„±ëŠ¥ ë¹„êµ
    2. ì°¸ì¡° ëª¨ë¸ê³¼ì˜ KL ë°œì‚° ì œí•œ
    3. ì•ˆì •ì ì¸ ì •ì±… ì—…ë°ì´íŠ¸
    
    Attributes:
        config (GRPOConfig): í•™ìŠµ ì„¤ì •
        vlm: VLM ëª¨ë¸ (í•™ìŠµ ëŒ€ìƒ)
        vlm_ref: ì°¸ì¡° VLM ëª¨ë¸ (ê³ ì •)
        optimizer: ì˜µí‹°ë§ˆì´ì €
        device: ì—°ì‚° ë””ë°”ì´ìŠ¤
    """
    
    def __init__(self, 
                 vlm_model,
                 config: GRPOConfig):
        """
        GRPO Trainer ì´ˆê¸°í™”
        
        Args:
            vlm_model: í•™ìŠµí•  VLM ëª¨ë¸ (í”Œë ˆì´ìŠ¤í™€ë” ë°©ì‹)
            config (GRPOConfig): GRPO í•™ìŠµ ì„¤ì •
        """
        self.config = config
        self.vlm = vlm_model
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        if config.device == "auto":
            if torch.backends.mps.is_available():
                self.device = torch.device("mps")
                logger.info("ğŸ Using Apple Silicon MPS for GRPO")
            elif torch.cuda.is_available():
                self.device = torch.device("cuda")
                logger.info("ğŸš€ Using CUDA GPU for GRPO")
            else:
                self.device = torch.device("cpu")
                logger.info("ğŸ’» Using CPU for GRPO")
        else:
            self.device = torch.device(config.device)
        
        # í”Œë ˆì´ìŠ¤í™€ë” ë°©ì‹ì—ì„œëŠ” ì‹¤ì œ ëª¨ë¸ íŒŒë¼ë¯¸í„°ê°€ ì—†ìœ¼ë¯€ë¡œ ë”ë¯¸ íŒŒë¼ë¯¸í„° ìƒì„±
        self.dummy_param = nn.Parameter(torch.randn(1, requires_grad=True))
        
        # ì°¸ì¡° ëª¨ë¸ ìƒì„± (ë§¤ iterationë§ˆë‹¤ ì—…ë°ì´íŠ¸)
        self.vlm_ref = None
        
        # ì˜µí‹°ë§ˆì´ì € ì„¤ì • (ë”ë¯¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©)
        self.optimizer = optim.AdamW(
            [self.dummy_param],  # ë”ë¯¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©
            lr=config.learning_rate,
            weight_decay=0.01
        )
        
        # í•™ìŠµ í†µê³„
        self.training_stats = {
            'iteration': 0,
            'total_samples': 0,
            'avg_reward': 0.0,
            'policy_loss': 0.0,
            'kl_divergence': 0.0,
            'entropy': 0.0
        }
        
        logger.info(f"ğŸ”§ GRPO Trainer initialized with config: {config}")
        logger.info("ğŸ“ Using placeholder-based VLM, no actual parameter optimization")
    
    def collect_group_data(self, prompts: List[str]) -> Dict[str, Any]:
        """
        ê·¸ë£¹ ë°ì´í„° ìˆ˜ì§‘: í”„ë¡¬í”„íŠ¸ ê°œì„  ë° ë³´ìƒ ê³„ì‚°
        
        ì´ ë©”ì„œë“œëŠ” GRPOì˜ í•µì‹¬ ë°ì´í„° ìˆ˜ì§‘ ë‹¨ê³„ì…ë‹ˆë‹¤:
        1. ê° í”„ë¡¬í”„íŠ¸ë¥¼ VLMìœ¼ë¡œ ê°œì„ 
        2. ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ë¡œ ì´ë¯¸ì§€ ìƒì„±
        3. CLIPìœ¼ë¡œ ë³´ìƒ ê³„ì‚°
        4. ë¡œê·¸ í™•ë¥  ê³„ì‚°
        
        Args:
            prompts (List[str]): ì…ë ¥ í”„ë¡¬í”„íŠ¸ ê·¸ë£¹
            
        Returns:
            Dict[str, Any]: ìˆ˜ì§‘ëœ ê·¸ë£¹ ë°ì´í„°
        """
        logger.debug(f"ğŸ“Š Collecting group data for {len(prompts)} prompts")
        
        group_data = {
            'prompts': prompts,
            'enhanced_prompts': [],
            'images': [],
            'rewards': [],
            'log_probs': [],
            'ref_log_probs': [],
            'advantages': [],
            'returns': []
        }
        
        # ì°¸ì¡° ëª¨ë¸ ì—…ë°ì´íŠ¸ (í˜„ì¬ ì •ì±…ì˜ ë³µì‚¬ë³¸)
        self._update_reference_model()
        
        # ê° í”„ë¡¬í”„íŠ¸ì— ëŒ€í•´ ë°ì´í„° ìˆ˜ì§‘
        for prompt in prompts:
            try:
                # 1. VLMìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ê°œì„ 
                enhanced_prompt, log_prob = self._enhance_prompt_with_logprob(prompt)
                
                # 2. ì°¸ì¡° ëª¨ë¸ë¡œ ë¡œê·¸ í™•ë¥  ê³„ì‚°
                ref_log_prob = self._calculate_reference_logprob(prompt, enhanced_prompt)
                
                # 3. ì´ë¯¸ì§€ ìƒì„± (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” SD3 ì‚¬ìš©)
                image = self._generate_image(enhanced_prompt)
                
                # 4. ë³´ìƒ ê³„ì‚° (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” CLIP ì‚¬ìš©)
                reward = self._calculate_reward(image, enhanced_prompt, prompt)
                
                # ë°ì´í„° ì €ì¥
                group_data['enhanced_prompts'].append(enhanced_prompt)
                group_data['images'].append(image)
                group_data['rewards'].append(reward)
                group_data['log_probs'].append(log_prob)
                group_data['ref_log_probs'].append(ref_log_prob)
                
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to process prompt '{prompt}': {e}")
                # ì‹¤íŒ¨í•œ ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš© (ì¼ê´€ëœ í…ì„œ ì†ì„±ìœ¼ë¡œ)
                group_data['enhanced_prompts'].append(prompt)
                group_data['images'].append(None)
                group_data['rewards'].append(0.0)
                group_data['log_probs'].append(torch.tensor(-2.0, dtype=torch.float32, requires_grad=True))
                group_data['ref_log_probs'].append(torch.tensor(-2.0, dtype=torch.float32))
        
        # 5. ì–´ë“œë°´í‹°ì§€ ë° ë¦¬í„´ ê³„ì‚°
        self._calculate_advantages_and_returns(group_data)
        
        logger.debug(f"âœ… Group data collected: avg_reward={np.mean(group_data['rewards']):.4f}")
        return group_data
    
    def _enhance_prompt_with_logprob(self, prompt: str) -> Tuple[str, torch.Tensor]:
        """
        VLMìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ê°œì„  ë° ë¡œê·¸ í™•ë¥  ê³„ì‚° (í”Œë ˆì´ìŠ¤í™€ë” ë°©ì‹)
        
        Args:
            prompt (str): ì›ë³¸ í”„ë¡¬í”„íŠ¸
            
        Returns:
            Tuple[str, torch.Tensor]: (ê°œì„ ëœ í”„ë¡¬í”„íŠ¸, ë¡œê·¸ í™•ë¥ )
        """
        try:
            # í”Œë ˆì´ìŠ¤í™€ë” ë°©ì‹ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ê°œì„ 
            enhanced_prompt = self.vlm.enhance_prompt(prompt)
            
            # ë”ë¯¸ ë¡œê·¸ í™•ë¥  ìƒì„± (ì‹¤ì œ ê³„ì‚° ëŒ€ì‹ )
            log_prob = torch.tensor(-1.0, dtype=torch.float32, requires_grad=True)
            
            return enhanced_prompt, log_prob
            
        except Exception as e:
            logger.warning(f"âš ï¸ Prompt enhancement failed: {e}")
            # ì‹¤íŒ¨ ì‹œ ì›ë³¸ í”„ë¡¬í”„íŠ¸ì™€ ë”ë¯¸ ë¡œê·¸ í™•ë¥  ë°˜í™˜
            return prompt, torch.tensor(-2.0, dtype=torch.float32, requires_grad=True)
    
    def _calculate_reference_logprob(self, prompt: str, enhanced_prompt: str) -> torch.Tensor:
        """
        ì°¸ì¡° ëª¨ë¸ë¡œ ë¡œê·¸ í™•ë¥  ê³„ì‚° (í”Œë ˆì´ìŠ¤í™€ë” ë°©ì‹)
        
        Args:
            prompt (str): ì›ë³¸ í”„ë¡¬í”„íŠ¸
            enhanced_prompt (str): ê°œì„ ëœ í”„ë¡¬í”„íŠ¸
            
        Returns:
            torch.Tensor: ì°¸ì¡° ë¡œê·¸ í™•ë¥ 
        """
        try:
            # ë”ë¯¸ ì°¸ì¡° ë¡œê·¸ í™•ë¥  ìƒì„±
            ref_log_prob = torch.tensor(-1.2, dtype=torch.float32)
            return ref_log_prob
            
        except Exception as e:
            logger.warning(f"âš ï¸ Reference log prob calculation failed: {e}")
            return torch.tensor(-2.0, dtype=torch.float32)
    
    def _generate_image(self, prompt: str):
        """
        í”„ë¡¬í”„íŠ¸ë¡œë¶€í„° ì´ë¯¸ì§€ ìƒì„± (í”Œë ˆì´ìŠ¤í™€ë”)
        
        ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” SD3Generatorë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        
        Args:
            prompt (str): ì´ë¯¸ì§€ ìƒì„±ìš© í”„ë¡¬í”„íŠ¸
            
        Returns:
            ìƒì„±ëœ ì´ë¯¸ì§€ (í”Œë ˆì´ìŠ¤í™€ë”)
        """
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” SD3Generator.generate_image() í˜¸ì¶œ
        return f"image_for_{prompt[:20]}"  # í”Œë ˆì´ìŠ¤í™€ë”
    
    def _calculate_reward(self, image, enhanced_prompt: str, original_prompt: str) -> float:
        """
        ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê¸°ë°˜ ë³´ìƒ ê³„ì‚° (í”Œë ˆì´ìŠ¤í™€ë”)
        
        ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” CLIPRewardCalculatorë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        
        Args:
            image: ìƒì„±ëœ ì´ë¯¸ì§€
            enhanced_prompt (str): ê°œì„ ëœ í”„ë¡¬í”„íŠ¸
            original_prompt (str): ì›ë³¸ í”„ë¡¬í”„íŠ¸
            
        Returns:
            float: ê³„ì‚°ëœ ë³´ìƒ
        """
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” CLIPRewardCalculator.calculate_reward() í˜¸ì¶œ
        # ê°„ë‹¨í•œ ì‹œë®¬ë ˆì´ì…˜: í”„ë¡¬í”„íŠ¸ ê¸¸ì´ì— ë”°ë¥¸ ë³´ìƒ
        reward = min(len(enhanced_prompt) / 100.0, 1.0)
        return reward
    
    def _calculate_advantages_and_returns(self, group_data: Dict[str, Any]):
        """
        GRPOì˜ í•µì‹¬: ê·¸ë£¹ ê¸°ë°˜ ì–´ë“œë°´í‹°ì§€ ê³„ì‚°
        
        GRPOëŠ” ê·¸ë£¹ ë‚´ì—ì„œ ìƒëŒ€ì  ì„±ëŠ¥ì„ ë¹„êµí•˜ì—¬ ì–´ë“œë°´í‹°ì§€ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤:
        1. ê·¸ë£¹ í‰ê·  ë³´ìƒ ê³„ì‚°
        2. ê° ìƒ˜í”Œì˜ ìƒëŒ€ì  ì„±ëŠ¥ ì¸¡ì •
        3. í• ì¸ëœ ë¦¬í„´ ê³„ì‚°
        4. ì •ê·œí™”
        
        Args:
            group_data (Dict[str, Any]): ê·¸ë£¹ ë°ì´í„° (in-place ìˆ˜ì •)
        """
        # ë°ì´í„° ì™„ì„±ë„ ê²€ì¦
        expected_length = len(group_data['prompts'])
        
        # rewards ê¸¸ì´ ê²€ì¦ ë° ë³´ì •
        if len(group_data['rewards']) != expected_length:
            logger.warning(f"âš ï¸ Rewards length mismatch: {len(group_data['rewards'])} != {expected_length}")
            while len(group_data['rewards']) < expected_length:
                group_data['rewards'].append(0.0)
        
        # ref_log_probs ê¸¸ì´ ê²€ì¦ ë° ë³´ì •
        if len(group_data['ref_log_probs']) != expected_length:
            logger.warning(f"âš ï¸ ref_log_probs length mismatch: {len(group_data['ref_log_probs'])} != {expected_length}")
            while len(group_data['ref_log_probs']) < expected_length:
                group_data['ref_log_probs'].append(torch.tensor(-1.2, dtype=torch.float32))
        
        rewards = np.array(group_data['rewards'])
        
        # 1. í• ì¸ëœ ë¦¬í„´ ê³„ì‚° (ë‹¨ìˆœí™”: ë‹¨ì¼ ìŠ¤í…)
        returns = rewards.copy()
        
        # 2. ê·¸ë£¹ ê¸°ë°˜ ì–´ë“œë°´í‹°ì§€ ê³„ì‚°
        # GRPOì˜ í•µì‹¬: ê·¸ë£¹ í‰ê·  ëŒ€ë¹„ ìƒëŒ€ì  ì„±ëŠ¥
        group_mean_reward = np.mean(rewards)
        advantages = rewards - group_mean_reward
        
        # 3. ì–´ë“œë°´í‹°ì§€ ì •ê·œí™” (í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ)
        if len(advantages) > 1:
            advantages = (advantages - np.mean(advantages)) / (np.std(advantages) + 1e-8)
        
        # 4. í…ì„œë¡œ ë³€í™˜ (ë””ë°”ì´ìŠ¤ ë¬¸ì œ í•´ê²°)
        group_data['returns'] = [torch.tensor(float(r), dtype=torch.float32) for r in returns]
        group_data['advantages'] = [torch.tensor(float(a), dtype=torch.float32) for a in advantages]
        
        # 5. ìµœì¢… ê¸¸ì´ ê²€ì¦
        for key in ['returns', 'advantages']:
            if len(group_data[key]) != expected_length:
                logger.error(f"âŒ Final length mismatch for {key}: {len(group_data[key])} != {expected_length}")
        
        logger.debug(f"ğŸ“Š Advantages calculated: mean={np.mean(advantages):.4f}, std={np.std(advantages):.4f}")
    
    def _update_reference_model(self):
        """
        ì°¸ì¡° ëª¨ë¸ ì—…ë°ì´íŠ¸ (í”Œë ˆì´ìŠ¤í™€ë” ë°©ì‹ì—ì„œëŠ” ê°„ë‹¨íˆ ì²˜ë¦¬)
        """
        try:
            # í”Œë ˆì´ìŠ¤í™€ë” ë°©ì‹ì—ì„œëŠ” ì°¸ì¡° ëª¨ë¸ ì—…ë°ì´íŠ¸ê°€ ë¶ˆí•„ìš”
            self.vlm_ref = "placeholder_ref_model"
            logger.debug("ğŸ”„ Reference model updated (placeholder)")
            
        except Exception as e:
            logger.warning(f"âš ï¸ Reference model update failed: {e}")
    
    def grpo_update(self, group_data: Dict[str, Any]) -> Dict[str, float]:
        """
        GRPO ì •ì±… ì—…ë°ì´íŠ¸ ìˆ˜í–‰
        
        Args:
            group_data (Dict[str, Any]): ìˆ˜ì§‘ëœ ê·¸ë£¹ ë°ì´í„°
            
        Returns:
            Dict[str, float]: í•™ìŠµ ë©”íŠ¸ë¦­
        """
        logger.debug("ğŸ”„ Starting GRPO update")
        
        # ì…ë ¥ ê²€ì¦
        if not group_data or len(group_data.get('prompts', [])) == 0:
            logger.warning("âš ï¸ Empty group data provided")
            return {
                'policy_loss': 0.0,
                'kl_div': 0.0,
                'entropy': 0.0,
                'total_loss': 0.0,
                'avg_reward': 0.0
            }
        
        # ë©”íŠ¸ë¦­ ì´ˆê¸°í™”
        metrics = {
            'policy_loss': 0.0,
            'kl_div': 0.0,
            'entropy': 0.0,
            'total_loss': 0.0,
            'avg_reward': np.mean(group_data['rewards']) if group_data['rewards'] else 0.0
        }
        
        try:
            # GRPO ì—í¬í¬ë§Œí¼ ë°˜ë³µ í•™ìŠµ
            for epoch in range(self.config.grpo_epochs):
                epoch_metrics = self._grpo_epoch_update(group_data)
                
                # ë©”íŠ¸ë¦­ ëˆ„ì 
                for key in ['policy_loss', 'kl_div', 'entropy', 'total_loss']:
                    metrics[key] += epoch_metrics.get(key, 0.0)
            
            # í‰ê·  ê³„ì‚°
            for key in ['policy_loss', 'kl_div', 'entropy', 'total_loss']:
                metrics[key] /= self.config.grpo_epochs
            
            # í•™ìŠµ í†µê³„ ì—…ë°ì´íŠ¸
            self.training_stats.update(metrics)
            self.training_stats['iteration'] += 1
            self.training_stats['total_samples'] += len(group_data['prompts'])
            self.training_stats['avg_reward'] = metrics['avg_reward']
            
            logger.info(f"ğŸ”„ GRPO update completed: loss={metrics['total_loss']:.4f}, "
                       f"reward={metrics['avg_reward']:.4f}")
            
        except Exception as e:
            logger.error(f"âŒ GRPO update failed: {e}")
            # ì—ëŸ¬ ë°œìƒ ì‹œ ê¸°ë³¸ ë©”íŠ¸ë¦­ ë°˜í™˜
            
        return metrics
    
    def _grpo_epoch_update(self, group_data: Dict[str, Any]) -> Dict[str, float]:
        """
        ë‹¨ì¼ GRPO ì—í¬í¬ ì—…ë°ì´íŠ¸
        
        Args:
            group_data (Dict[str, Any]): ê·¸ë£¹ ë°ì´í„°
            
        Returns:
            Dict[str, float]: ì—í¬í¬ ë©”íŠ¸ë¦­
        """
        try:
            self.optimizer.zero_grad()
            
            # í˜„ì¬ ì •ì±…ìœ¼ë¡œ ë¡œê·¸ í™•ë¥  ì¬ê³„ì‚° (ì •ì±…ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìœ¼ë¯€ë¡œ)
            current_log_probs = []
            for i, prompt in enumerate(group_data['prompts']):
                try:
                    enhanced_prompt = group_data['enhanced_prompts'][i]
                    _, log_prob = self._enhance_prompt_with_logprob(prompt)
                    current_log_probs.append(log_prob)
                except Exception as e:
                    logger.warning(f"âš ï¸ Failed to get log prob for prompt {i}: {e}")
                    current_log_probs.append(torch.tensor(-2.0, dtype=torch.float32, requires_grad=True))
            
            # ì†ì‹¤ ê³„ì‚°
            policy_loss = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)
            kl_div_estimates = []  # KL divergence estimates for batch average
            entropy = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)
            
            for i in range(len(group_data['prompts'])):
                try:
                    # í˜„ì¬ ë¡œê·¸ í™•ë¥ ê³¼ ì°¸ì¡° ë¡œê·¸ í™•ë¥  ê°€ì ¸ì˜¤ê¸°
                    current_log_prob = current_log_probs[i]
                    
                    # ref_log_probs í‚¤ ì•ˆì „í•˜ê²Œ ì ‘ê·¼
                    if 'ref_log_probs' not in group_data or i >= len(group_data['ref_log_probs']):
                        logger.warning(f"âš ï¸ Missing ref_log_probs for sample {i}")
                        ref_log_prob = torch.tensor(-1.2, dtype=torch.float32)
                    else:
                        ref_log_prob = group_data['ref_log_probs'][i]
                    
                    # í…ì„œë¡œ ë³€í™˜ (í•„ìš”ì‹œ)
                    if not isinstance(ref_log_prob, torch.Tensor):
                        ref_log_prob = torch.tensor(float(ref_log_prob), dtype=torch.float32)
                    
                    # advantages í‚¤ ì•ˆì „í•˜ê²Œ ì ‘ê·¼
                    if 'advantages' not in group_data or i >= len(group_data['advantages']):
                        logger.warning(f"âš ï¸ Missing advantages for sample {i}")
                        advantage = torch.tensor(0.0, dtype=torch.float32)
                    else:
                        advantage = group_data['advantages'][i]
                        if not isinstance(advantage, torch.Tensor):
                            advantage = torch.tensor(float(advantage), dtype=torch.float32)
                    
                    # ì •ì±… ë¹„ìœ¨ ê³„ì‚°: Ï€_Î¸(a|s) / Ï€_ref(a|s)
                    log_ratio = current_log_prob - ref_log_prob
                    ratio = torch.exp(log_ratio)
                    
                    # í´ë¦¬í•‘ëœ ì„œë¡œê²Œì´íŠ¸ ì†ì‹¤ (PPO ìŠ¤íƒ€ì¼)
                    surr1 = ratio * advantage
                    surr2 = torch.clamp(ratio, 1 - self.config.clip_epsilon, 1 + self.config.clip_epsilon) * advantage
                    policy_loss_i = -torch.min(surr1, surr2)
                    
                    # KL divergence ê³„ì‚° (ìˆ˜ì •ëœ ê³µì‹)
                    log_ratio_ref_curr = ref_log_prob - current_log_prob.detach()
                    kl_div_i = torch.exp(log_ratio_ref_curr) - log_ratio_ref_curr - 1
                    kl_div_i = torch.relu(kl_div_i)  # ìŒìˆ˜ ë°©ì§€
                    
                    # ì—”íŠ¸ë¡œí”¼ ê³„ì‚°
                    entropy_i = -current_log_prob
                    
                    policy_loss = policy_loss + policy_loss_i
                    kl_div_estimates.append(kl_div_i)
                    entropy = entropy + entropy_i
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ Failed to calculate loss for sample {i}: {e}")
                    continue
            
            # ë°°ì¹˜ í¬ê¸°ë¡œ ì •ê·œí™”
            batch_size = len(group_data['prompts'])
            if batch_size > 0:
                policy_loss = policy_loss / batch_size
                entropy = entropy / batch_size
            
            # KL divergence í‰ê·  ê³„ì‚°
            if len(kl_div_estimates) > 0:
                kl_div_estimate_mean = torch.stack(kl_div_estimates).mean()
            else:
                kl_div_estimate_mean = torch.tensor(0.0, dtype=torch.float32)
            
            # ì´ ì†ì‹¤: ì •ì±… ì†ì‹¤ + KL í˜ë„í‹° - ì—”íŠ¸ë¡œí”¼ ë³´ë„ˆìŠ¤
            total_loss = policy_loss + self.config.kl_beta * kl_div_estimate_mean - self.config.entropy_coeff * entropy
            
            # ì—­ì „íŒŒ (ë”ë¯¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©)
            total_loss.backward()
            
            # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ (ë”ë¯¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©)
            torch.nn.utils.clip_grad_norm_([self.dummy_param], self.config.max_grad_norm)
            
            # ì˜µí‹°ë§ˆì´ì € ìŠ¤í…
            self.optimizer.step()
            
            # ì•ˆì „í•œ item() í˜¸ì¶œ
            return {
                'policy_loss': float(policy_loss.detach().numpy()) if hasattr(policy_loss, 'detach') else float(policy_loss),
                'kl_div': float(kl_div_estimate_mean.detach().numpy()) if hasattr(kl_div_estimate_mean, 'detach') else float(kl_div_estimate_mean),
                'entropy': float(entropy.detach().numpy()) if hasattr(entropy, 'detach') else float(entropy),
                'total_loss': float(total_loss.detach().numpy()) if hasattr(total_loss, 'detach') else float(total_loss)
            }
            
        except Exception as e:
            logger.error(f"âŒ Epoch update failed: {e}")
            return {
                'policy_loss': 0.0,
                'kl_div': 0.0,
                'entropy': 0.0,
                'total_loss': 0.0
            }
    
    def get_training_stats(self) -> Dict[str, Any]:
        """í˜„ì¬ í•™ìŠµ í†µê³„ ë°˜í™˜"""
        return self.training_stats.copy()
    
    def save_checkpoint(self, checkpoint_path: str):
        """í•™ìŠµ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (í”Œë ˆì´ìŠ¤í™€ë” ë°©ì‹)"""
        try:
            checkpoint = {
                'dummy_param': self.dummy_param.data,
                'optimizer_state_dict': self.optimizer.state_dict(),
                'config': self.config,
                'training_stats': self.training_stats
            }
            torch.save(checkpoint, checkpoint_path)
            logger.info(f"ğŸ’¾ Checkpoint saved: {checkpoint_path}")
        except Exception as e:
            logger.error(f"âŒ Failed to save checkpoint: {e}")
    
    def load_checkpoint(self, checkpoint_path: str):
        """í•™ìŠµ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (í”Œë ˆì´ìŠ¤í™€ë” ë°©ì‹)"""
        try:
            checkpoint = torch.load(checkpoint_path, map_location='cpu')
            if 'dummy_param' in checkpoint:
                self.dummy_param.data = checkpoint['dummy_param']
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            self.training_stats = checkpoint['training_stats']
            logger.info(f"ğŸ“¥ Checkpoint loaded: {checkpoint_path}")
        except Exception as e:
            logger.error(f"âŒ Failed to load checkpoint: {e}")


if __name__ == "__main__":
    # GRPO Trainer í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ğŸ§ª GRPO Trainer Test")
    print("=" * 30)
    
    try:
        # Mock VLM ëª¨ë¸
        class MockVLM(nn.Module):
            def __init__(self):
                super().__init__()
                self.linear = nn.Linear(10, 10)
            
            def enhance_prompt(self, prompt):
                return f"enhanced: {prompt}"
        
        # ì„¤ì • ë° íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
        config = GRPOConfig(
            learning_rate=1e-4,
            group_size=3,
            num_iterations=2
        )
        
        mock_vlm = MockVLM()
        trainer = GRPOTrainer(mock_vlm, config)
        
        print("âœ… GRPO Trainer initialized successfully")
        print(f"ğŸ“Š Training stats: {trainer.get_training_stats()}")
        
        # í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
        test_prompts = ["a cat", "sunset", "mountain"]
        
        print("\nğŸ”„ Testing group data collection:")
        group_data = trainer.collect_group_data(test_prompts)
        print(f"  Collected data for {len(group_data['prompts'])} prompts")
        print(f"  Average reward: {np.mean(group_data['rewards']):.4f}")
        
        print("\nğŸ”„ Testing GRPO update:")
        metrics = trainer.grpo_update(group_data)
        print(f"  Update metrics: {metrics}")
        
        print("\nâœ… All tests passed!")
        
    except Exception as e:
        print(f"âŒ Test failed: {e}")
    
    print("\nUsage:")
    print("from training.grpo_trainer import GRPOTrainer, GRPOConfig")
    print("config = GRPOConfig()")
    print("trainer = GRPOTrainer(vlm_model, config)")
    print("group_data = trainer.collect_group_data(prompts)")
    print("metrics = trainer.grpo_update(group_data)") 