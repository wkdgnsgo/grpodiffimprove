"""
GRPO (Group Relative Policy Optimization) Trainer
================================================

GRPO ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í•œ í•µì‹¬ í•™ìŠµ ëª¨ë“ˆì…ë‹ˆë‹¤.
VLM í”„ë¡¬í”„íŠ¸ ê°œì„ ì„ ìœ„í•œ ê°•í™”í•™ìŠµ íŠ¸ë ˆì´ë„ˆì…ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
1. GRPO ì •ì±… ì—…ë°ì´íŠ¸
2. ê·¸ë£¹ ê¸°ë°˜ ì–´ë“œë°´í‹°ì§€ ê³„ì‚°
3. KL ë°œì‚° í˜ë„í‹°
4. ì°¸ì¡° ëª¨ë¸ ê´€ë¦¬

GRPO vs PPO ì°¨ì´ì :
- PPO: ê°œë³„ ìƒ˜í”Œ ê¸°ë°˜ ì–´ë“œë°´í‹°ì§€
- GRPO: ê·¸ë£¹ ë‚´ ìƒëŒ€ì  ì–´ë“œë°´í‹°ì§€ (ë” ì•ˆì •ì )

Author: AI Assistant
Date: 2025-01-22
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.distributions import Categorical

from typing import List, Dict, Tuple, Optional, Any
import logging
import numpy as np
import copy
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class GRPOConfig:
    """GRPO í•™ìŠµ ì„¤ì • í´ë˜ìŠ¤"""
    learning_rate: float = 1e-5
    group_size: int = 4
    num_iterations: int = 20
    grpo_epochs: int = 2
    gamma: float = 0.99           # í• ì¸ íŒ©í„°
    kl_beta: float = 0.01         # KL ë°œì‚° í˜ë„í‹° ê³„ìˆ˜
    clip_epsilon: float = 0.2     # í´ë¦¬í•‘ ë²”ìœ„
    entropy_coeff: float = 0.01   # ì—”íŠ¸ë¡œí”¼ ë³´ë„ˆìŠ¤ ê³„ìˆ˜
    max_grad_norm: float = 1.0    # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
    epsilon_std: float = 1e-8     # í‘œì¤€í¸ì°¨ ì •ê·œí™”ìš© ì—¡ì‹¤ë¡ 
    
    # ìƒì„± íŒŒë¼ë¯¸í„°
    max_new_tokens: int = 50
    temperature: float = 0.8
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device: str = "auto"

class PolicyNetwork(nn.Module):
    """ê°„ë‹¨í•œ ì •ì±… ë„¤íŠ¸ì›Œí¬ (VLM í”„ë¡¬í”„íŠ¸ ê°œì„ ìš©)"""
    
    def __init__(self, input_dim: int = 128, hidden_dim: int = 256, vocab_size: int = 1000):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, vocab_size)
        
    def forward(self, x):
        """
        Args:
            x: ì…ë ¥ ìƒíƒœ (í”„ë¡¬í”„íŠ¸ ì„ë² ë”©)
        Returns:
            Categorical: ì •ì±… ë¶„í¬
        """
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        logits = self.fc3(x)
        return Categorical(logits=logits)

class GRPOTrainer:
    """
    Group Relative Policy Optimization (GRPO) íŠ¸ë ˆì´ë„ˆ
    
    ì°¸ì¡° ì½”ë“œ ê¸°ë°˜ìœ¼ë¡œ ì™„ì „íˆ ì¬êµ¬í˜„ëœ ë²„ì „
    """
    
    def __init__(self, vlm_model, config: GRPOConfig):
        self.vlm = vlm_model
        self.config = config
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        if config.device == "auto":
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = torch.device(config.device)
        
        logger.info(f"ğŸ”§ GRPO Trainer initialized on device: {self.device}")
        
        # ì •ì±… ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™” (ì‹¤ì œ í•™ìŠµ ê°€ëŠ¥í•œ ëª¨ë¸)
        self.policy_network = PolicyNetwork().to(self.device)
        
        # ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™”
        self.optimizer = optim.Adam(self.policy_network.parameters(), lr=config.learning_rate)
        
        # í•™ìŠµ í†µê³„
        self.training_stats = {
            'iteration': 0,
            'total_samples': 0,
            'avg_reward': 0.0,
            'policy_loss': 0.0,
            'entropy': 0.0,
            'kl_div': 0.0
        }
        
        logger.info("âœ… GRPO Trainer ready for training")
    
    def collect_group_data(self, prompts: List[str]) -> Dict[str, Any]:
        """
        ê·¸ë£¹ ë°ì´í„° ìˆ˜ì§‘ (ì°¸ì¡° ì½”ë“œì˜ rollout ë‹¨ê³„ì™€ ìœ ì‚¬)
        
        Args:
            prompts: ì…ë ¥ í”„ë¡¬í”„íŠ¸ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            Dict: ìˆ˜ì§‘ëœ ê·¸ë£¹ ë°ì´í„°
        """
        logger.info(f"ğŸ”„ Collecting group data for {len(prompts)} prompts...")
        
        group_data = {
            'prompts': prompts,
            'enhanced_prompts': [],
            'states': [],
            'actions': [],
            'log_probs_old': [],  # ìƒ˜í”Œë§ ì‹œì ì˜ ë¡œê·¸ í™•ë¥ 
            'rewards': [],
            'policy_distributions': []  # ì‹¤ì œ ì •ì±… ë¶„í¬ ì €ì¥
        }
        
        # ì •ì±… ë„¤íŠ¸ì›Œí¬ë¥¼ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì • (rollout ë‹¨ê³„)
        self.policy_network.eval()
        
        with torch.no_grad():
            for i, prompt in enumerate(prompts):
                try:
                    # 1. í”„ë¡¬í”„íŠ¸ë¥¼ ìƒíƒœ ë²¡í„°ë¡œ ë³€í™˜ (ê°„ë‹¨í•œ ì„ë² ë”©)
                    state = self._prompt_to_state(prompt)
                    
                    # 2. ì •ì±… ë¶„í¬ ê³„ì‚°
                    policy_dist = self.policy_network(state)
                    
                    # 3. ì•¡ì…˜ ìƒ˜í”Œë§ ë° ë¡œê·¸ í™•ë¥  ê³„ì‚°
                    action = policy_dist.sample()
                    log_prob_old = policy_dist.log_prob(action)
                    
                    # 4. í”„ë¡¬í”„íŠ¸ ê°œì„  (ì•¡ì…˜ ê¸°ë°˜)
                    enhanced_prompt = self._action_to_enhanced_prompt(prompt, action)
                    
                    # 5. ì´ë¯¸ì§€ ìƒì„± ë° ë³´ìƒ ê³„ì‚°
                    image = self._generate_image(enhanced_prompt)
                    reward = self._calculate_reward(image, enhanced_prompt, prompt)
                    
                    # 6. ë°ì´í„° ì €ì¥
                    group_data['enhanced_prompts'].append(enhanced_prompt)
                    group_data['states'].append(state)
                    group_data['actions'].append(action)
                    group_data['log_probs_old'].append(log_prob_old)
                    group_data['rewards'].append(reward)
                    group_data['policy_distributions'].append(policy_dist)
                    
                    logger.debug(f"  Prompt {i}: reward={reward:.4f}, log_prob={log_prob_old:.4f}")
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ Failed to process prompt {i}: {e}")
                    # ê¸°ë³¸ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
                    default_state = torch.zeros(128, device=self.device)
                    default_action = torch.tensor(0, device=self.device)
                    default_log_prob = torch.tensor(-2.0, device=self.device)
                    
                    group_data['enhanced_prompts'].append(f"enhanced: {prompt}")
                    group_data['states'].append(default_state)
                    group_data['actions'].append(default_action)
                    group_data['log_probs_old'].append(default_log_prob)
                    group_data['rewards'].append(0.0)
                    group_data['policy_distributions'].append(None)
        
        # ì •ì±… ë„¤íŠ¸ì›Œí¬ë¥¼ ë‹¤ì‹œ í›ˆë ¨ ëª¨ë“œë¡œ ì„¤ì •
        self.policy_network.train()
        
        logger.info(f"âœ… Group data collected: {len(group_data['prompts'])} samples")
        logger.info(f"ğŸ“Š Average reward: {np.mean(group_data['rewards']):.4f}")
        
        return group_data
    
    def _prompt_to_state(self, prompt: str) -> torch.Tensor:
        """í”„ë¡¬í”„íŠ¸ë¥¼ ìƒíƒœ ë²¡í„°ë¡œ ë³€í™˜"""
        # ê°„ë‹¨í•œ í•´ì‹œ ê¸°ë°˜ ì„ë² ë”© (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ë°©ë²• ì‚¬ìš©)
        hash_val = hash(prompt) % 1000000
        state = torch.zeros(128, device=self.device)
        state[:10] = torch.tensor([float(hash_val % (10**i)) / (10**i) for i in range(1, 11)], device=self.device)
        return state
    
    def _action_to_enhanced_prompt(self, prompt: str, action: torch.Tensor) -> str:
        """ì•¡ì…˜ì„ ê¸°ë°˜ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ê°œì„ """
        action_val = action.item()
        enhancements = [
            "detailed, high quality",
            "artistic, beautiful",
            "photorealistic, 4k",
            "cinematic, dramatic lighting",
            "vibrant colors, sharp focus"
        ]
        enhancement = enhancements[action_val % len(enhancements)]
        return f"{prompt}, {enhancement}"
    
    def _generate_image(self, prompt: str):
        """ì´ë¯¸ì§€ ìƒì„± (í”Œë ˆì´ìŠ¤í™€ë”)"""
        return f"image_for_{prompt[:20]}"
    
    def _calculate_reward(self, image, enhanced_prompt: str, original_prompt: str) -> float:
        """ë³´ìƒ ê³„ì‚° (í”Œë ˆì´ìŠ¤í™€ë”)"""
        return min(len(enhanced_prompt) / 100.0, 1.0) + np.random.normal(0, 0.1)
    
    def _calculate_advantages_and_returns(self, group_data: Dict[str, Any]):
        """
        ì°¸ì¡° ì½”ë“œ ë°©ì‹ì˜ ì–´ë“œë°´í‹°ì§€ ê³„ì‚°
        
        1. í• ì¸ëœ ë¦¬í„´ ê³„ì‚°
        2. ê·¸ë£¹ ì •ê·œí™”
        """
        logger.debug("ğŸ”„ Calculating advantages and returns (reference code style)...")
        
        rewards = group_data['rewards']
        group_size = len(rewards)
        
        # 1. í• ì¸ëœ ë¦¬í„´ ê³„ì‚° (ê° ìƒ˜í”Œì€ ë‹¨ì¼ ìŠ¤í…ì´ë¯€ë¡œ ë‹¨ìˆœí™”)
        returns = np.array(rewards, dtype=np.float32)
        
        # 2. ê·¸ë£¹ í‰ê·  ê¸°ë°˜ ì–´ë“œë°´í‹°ì§€ ê³„ì‚°
        if group_size > 1:
            group_mean = np.mean(returns)
            group_std = np.std(returns)
            
            # ì •ê·œí™”: (x - mean) / (std + epsilon)
            advantages = (returns - group_mean) / (group_std + self.config.epsilon_std)
        else:
            advantages = np.array([0.0])
        
        # 3. í…ì„œë¡œ ë³€í™˜
        group_data['returns'] = [torch.tensor(ret, dtype=torch.float32, device=self.device) for ret in returns]
        group_data['advantages'] = [torch.tensor(adv, dtype=torch.float32, device=self.device) for adv in advantages]
        
        logger.debug(f"ğŸ“Š Returns: mean={np.mean(returns):.4f}, std={np.std(returns):.4f}")
        logger.debug(f"ğŸ“Š Advantages: mean={np.mean(advantages):.4f}, std={np.std(advantages):.4f}")
    
    def grpo_update(self, group_data: Dict[str, Any]) -> Dict[str, float]:
        """
        GRPO ì—…ë°ì´íŠ¸ (ì°¸ì¡° ì½”ë“œ ë°©ì‹)
        
        Args:
            group_data: ìˆ˜ì§‘ëœ ê·¸ë£¹ ë°ì´í„°
            
        Returns:
            Dict: ì—…ë°ì´íŠ¸ ë©”íŠ¸ë¦­
        """
        logger.info("ğŸ”„ Starting GRPO update...")
        
        # 1. ì–´ë“œë°´í‹°ì§€ ê³„ì‚°
        self._calculate_advantages_and_returns(group_data)
        
        # 2. ì°¸ì¡° ëª¨ë¸ ìƒì„± (í˜„ì¬ ëª¨ë¸ì˜ ì™„ì „í•œ ë³µì‚¬ë³¸)
        policy_ref = PolicyNetwork().to(self.device)
        policy_ref.load_state_dict(self.policy_network.state_dict())
        policy_ref.eval()
        
        # 3. ì—¬ëŸ¬ ì—í¬í¬ ì—…ë°ì´íŠ¸
        total_policy_loss = 0.0
        total_entropy = 0.0
        total_kl_div = 0.0
        
        for epoch in range(self.config.grpo_epochs):
            metrics = self._grpo_epoch_update(group_data, policy_ref)
            total_policy_loss += metrics['policy_loss']
            total_entropy += metrics['entropy']
            total_kl_div += metrics['kl_div']
        
        # 4. í‰ê·  ë©”íŠ¸ë¦­ ê³„ì‚°
        avg_metrics = {
            'policy_loss': total_policy_loss / self.config.grpo_epochs,
            'entropy': total_entropy / self.config.grpo_epochs,
            'kl_div': total_kl_div / self.config.grpo_epochs,
            'avg_reward': np.mean(group_data['rewards'])
        }
        
        # 5. í†µê³„ ì—…ë°ì´íŠ¸
        self.training_stats.update(avg_metrics)
        self.training_stats['iteration'] += 1
        self.training_stats['total_samples'] += len(group_data['prompts'])
        
        logger.info(f"âœ… GRPO update complete:")
        logger.info(f"  Policy Loss: {avg_metrics['policy_loss']:.4f}")
        logger.info(f"  Entropy: {avg_metrics['entropy']:.4f}")
        logger.info(f"  KL Div: {avg_metrics['kl_div']:.4f}")
        logger.info(f"  Avg Reward: {avg_metrics['avg_reward']:.4f}")
        
        return avg_metrics
    
    def _grpo_epoch_update(self, group_data: Dict[str, Any], policy_ref: PolicyNetwork) -> Dict[str, float]:
        """
        ë‹¨ì¼ ì—í¬í¬ GRPO ì—…ë°ì´íŠ¸ (ì°¸ì¡° ì½”ë“œ ë°©ì‹)
        """
        self.optimizer.zero_grad()
        
        total_policy_loss = 0.0
        total_entropy = 0.0
        total_kl_div = 0.0
        batch_size = len(group_data['prompts'])
        
        for i in range(batch_size):
            # 1. í˜„ì¬ ì •ì±…ìœ¼ë¡œ ì¬ê³„ì‚°
            state = group_data['states'][i]
            action = group_data['actions'][i]
            old_log_prob = group_data['log_probs_old'][i]
            advantage = group_data['advantages'][i]
            
            # 2. í˜„ì¬ ì •ì±… ë¶„í¬
            current_policy_dist = self.policy_network(state)
            current_log_prob = current_policy_dist.log_prob(action)
            
            # 3. ì°¸ì¡° ì •ì±… ë¶„í¬
            with torch.no_grad():
                ref_policy_dist = policy_ref(state)
                ref_log_prob = ref_policy_dist.log_prob(action)
            
            # 4. ì •ì±… ë¹„ìœ¨ ê³„ì‚°
            log_ratio = current_log_prob - old_log_prob.detach()
            ratio = torch.exp(log_ratio)
            
            # 5. í´ë¦¬í•‘ëœ ì„œë¡œê²Œì´íŠ¸ ì†ì‹¤ (PPO/GRPO)
            surr1 = ratio * advantage
            surr2 = torch.clamp(ratio, 1 - self.config.clip_epsilon, 1 + self.config.clip_epsilon) * advantage
            policy_loss_i = -torch.min(surr1, surr2)  # ìŒìˆ˜: ìµœëŒ€í™” -> ìµœì†Œí™”
            
            # 6. ì—”íŠ¸ë¡œí”¼ ê³„ì‚° (ì‹¤ì œ ì •ì±… ë¶„í¬ì—ì„œ)
            entropy_i = current_policy_dist.entropy()
            
            # 7. KL divergence ê³„ì‚°
            kl_div_i = torch.distributions.kl_divergence(ref_policy_dist, current_policy_dist)
            
            total_policy_loss += policy_loss_i
            total_entropy += entropy_i
            total_kl_div += kl_div_i
        
        # 8. í‰ê·  ê³„ì‚°
        avg_policy_loss = total_policy_loss / batch_size
        avg_entropy = total_entropy / batch_size
        avg_kl_div = total_kl_div / batch_size
        
        # 9. ì´ ì†ì‹¤: ì •ì±… ì†ì‹¤ + KL í˜ë„í‹° - ì—”íŠ¸ë¡œí”¼ ë³´ë„ˆìŠ¤
        total_loss = avg_policy_loss + self.config.kl_beta * avg_kl_div - self.config.entropy_coeff * avg_entropy
        
        # 10. ì—­ì „íŒŒ
        total_loss.backward()
        
        # 11. ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
        torch.nn.utils.clip_grad_norm_(self.policy_network.parameters(), self.config.max_grad_norm)
        
        # 12. ì˜µí‹°ë§ˆì´ì € ìŠ¤í…
        self.optimizer.step()
        
        return {
            'policy_loss': float(avg_policy_loss.detach()),
            'entropy': float(avg_entropy.detach()),
            'kl_div': float(avg_kl_div.detach()),
            'total_loss': float(total_loss.detach())
        }
    
    def get_training_stats(self) -> Dict[str, Any]:
        """í˜„ì¬ í•™ìŠµ í†µê³„ ë°˜í™˜"""
        return self.training_stats.copy()
    
    def save_checkpoint(self, checkpoint_path: str):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        try:
            checkpoint = {
                'policy_network_state_dict': self.policy_network.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'config': self.config,
                'training_stats': self.training_stats
            }
            torch.save(checkpoint, checkpoint_path)
            logger.info(f"ğŸ’¾ Checkpoint saved: {checkpoint_path}")
        except Exception as e:
            logger.error(f"âŒ Failed to save checkpoint: {e}")
    
    def load_checkpoint(self, checkpoint_path: str):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        try:
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            self.policy_network.load_state_dict(checkpoint['policy_network_state_dict'])
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            self.training_stats = checkpoint['training_stats']
            logger.info(f"ğŸ“¥ Checkpoint loaded: {checkpoint_path}")
        except Exception as e:
            logger.error(f"âŒ Failed to load checkpoint: {e}")


if __name__ == "__main__":
    # GRPO Trainer í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ğŸ§ª GRPO Trainer Test (Reference Code Style)")
    print("=" * 50)
    
    try:
        # Mock VLM ëª¨ë¸
        class MockVLM:
            def enhance_prompt(self, prompt):
                return f"enhanced: {prompt}"
        
        # ì„¤ì • ë° íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
        config = GRPOConfig(
            learning_rate=1e-4,
            group_size=3,
            num_iterations=2,
            grpo_epochs=2
        )
        
        mock_vlm = MockVLM()
        trainer = GRPOTrainer(mock_vlm, config)
        
        print("âœ… GRPO Trainer initialized successfully")
        print(f"ğŸ“Š Training stats: {trainer.get_training_stats()}")
        
        # í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
        test_prompts = ["a cat", "sunset", "mountain"]
        
        print("\nğŸ”„ Testing group data collection:")
        group_data = trainer.collect_group_data(test_prompts)
        print(f"  Collected data for {len(group_data['prompts'])} prompts")
        print(f"  Average reward: {np.mean(group_data['rewards']):.4f}")
        
        print("\nğŸ”„ Testing GRPO update:")
        metrics = trainer.grpo_update(group_data)
        print(f"  Update metrics: {metrics}")
        
        print("\nâœ… All tests passed!")
        
    except Exception as e:
        print(f"âŒ Test failed: {e}")
        import traceback
        traceback.print_exc()
    
    print("\nUsage:")
    print("from training.grpo_trainer import GRPOTrainer, GRPOConfig")
    print("config = GRPOConfig()")
    print("trainer = GRPOTrainer(vlm_model, config)")
    print("group_data = trainer.collect_group_data(prompts)")
    print("metrics = trainer.grpo_update(group_data)") 