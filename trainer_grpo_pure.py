#!/usr/bin/env python3
"""
QWEN í†µí•© GRPO íŠ¸ë ˆì´ë„ˆ
QWEN ëª¨ë¸ì˜ enhance_prompt ê¸°ëŠ¥ê³¼ GRPOë¥¼ í†µí•©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ ê°œì„ 
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import logging
from typing import Dict, List, Tuple, Optional
from collections import defaultdict
from dataclasses import dataclass
import math
import os
from datetime import datetime
import matplotlib.pyplot as plt
import matplotlib.style as mplstyle
from qwen import QWENModel, QWENGRPOConfig

logger = logging.getLogger(__name__)

class QWENGRPOEnvironment:
    """QWEN GRPO í†µí•© í™˜ê²½ (Accelerate ì§€ì›)"""
    
    def __init__(self, qwen_model: QWENModel, reward_model, sd_pipeline, config: QWENGRPOConfig):
        self.qwen_model = qwen_model
        self.reward_model = reward_model
        self.sd_pipeline = sd_pipeline
        self.config = config
        
        # ë©€í‹° í”„ë¡œì„¸ìŠ¤ í™˜ê²½ì—ì„œ ë©”ì¸ í”„ë¡œì„¸ìŠ¤ ì—¬ë¶€ í™•ì¸
        self.is_main_process = reward_model is not None and sd_pipeline is not None
        
        # GPU ë””ë°”ì´ìŠ¤ ì„¤ì • (Accelerate ë©€í‹° GPU í™˜ê²½)
        self.qwen_device = "auto"         # Accelerateê°€ ê´€ë¦¬
        self.sd_device = "cuda:6"         # SD3 (GPU 6ë²ˆ)
        self.reward_device = "cuda:5"     # CLIP Reward (GPU 5ë²ˆ)
        self.ref_device = "cuda:5"        # Reference model (GPU 5ë²ˆ)
        
        self.current_user_prompt = ""
        self.current_enhanced_prompt = ""
        self.episode_count = 0
        
        # ë¡œê¹… ë””ë ‰í† ë¦¬ ì„¤ì •
        if config.save_images:
            self.base_log_dir = config.log_dir
            os.makedirs(self.base_log_dir, exist_ok=True)
            logger.info(f"ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬: {self.base_log_dir}")
    
    def reset(self, user_prompt: str) -> Dict:
        """í™˜ê²½ ë¦¬ì…‹"""
        self.current_user_prompt = user_prompt
        self.current_enhanced_prompt = ""
        self.episode_count += 1
        
        # ì—í”¼ì†Œë“œ ë””ë ‰í† ë¦¬ ìƒì„±
        if self.config.save_images:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            safe_prompt = "".join(c for c in user_prompt if c.isalnum() or c in (' ', '-', '_')).rstrip()
            safe_prompt = safe_prompt[:30] + "..." if len(safe_prompt) > 30 else safe_prompt
            
            self.episode_dir = os.path.join(
                self.base_log_dir,
                f"episode_{self.episode_count:03d}_{timestamp}_{safe_prompt.replace(' ', '_')}"
            )
            os.makedirs(self.episode_dir, exist_ok=True)
        
        logger.info(f"ğŸ”„ í™˜ê²½ ë¦¬ì…‹: '{user_prompt}'")
        
        # ì´ˆê¸° ìƒíƒœ ë°˜í™˜ (ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸)
        return {
            'user_prompt': self.current_user_prompt,
            'enhanced_prompt': '',
            'episode': self.episode_count
        }
    
    def step(self, enhanced_prompt: str) -> Tuple[Dict, float, bool, Dict]:
        """í™˜ê²½ ìŠ¤í… - ë‹¨ìˆœí™”ëœ ë²„ì „ (ë°°ì¹˜ ì²˜ë¦¬ëŠ” trainerì—ì„œ ë‹´ë‹¹)"""
        # ê¸°ë³¸ ë¦¬ì›Œë“œ ë°˜í™˜ (ì‹¤ì œ ì²˜ë¦¬ëŠ” trainerì˜ ë°°ì¹˜ ë©”ì„œë“œì—ì„œ ìˆ˜í–‰)
        reward = 0.3  # ê¸°ë³¸ ë¦¬ì›Œë“œ
        
        next_state = {
            'user_prompt': self.current_user_prompt,
            'enhanced_prompt': enhanced_prompt,
            'episode': self.episode_count
        }
        
        info = {
            'original_prompt': self.current_user_prompt,
            'enhanced_prompt': enhanced_prompt,
            'original_reward': reward,
            'enhanced_reward': reward
        }
        
        # ì—í”¼ì†Œë“œ ì¹´ìš´í„° ì¦ê°€
        self.episode_count += 1
        
        return next_state, reward, True, info
    
    def _save_episode_results(self, original_image, enhanced_image, original_reward, enhanced_reward, total_reward, enhanced_prompt):
        """ì—í”¼ì†Œë“œ ê²°ê³¼ ì €ì¥"""
        try:
            # ì´ë¯¸ì§€ ì €ì¥
            original_image.save(os.path.join(self.episode_dir, "original_image.png"))
            enhanced_image.save(os.path.join(self.episode_dir, "enhanced_image.png"))
            
            # ë¡œê·¸ íŒŒì¼ ì‘ì„±
            log_content = f"""=== QWEN GRPO Episode Results ===
Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Episode: {self.episode_count}

=== Prompts ===
Original Prompt: {self.current_user_prompt}
Enhanced Prompt: {self.current_enhanced_prompt}

=== GRPO Direct Generation ===
Generated Enhanced Prompt: {enhanced_prompt}

=== Reward Components ===
Original Reward (Originalâ†’Original): {original_reward:.4f}
Enhanced Reward (Originalâ†’Enhanced): {enhanced_reward:.4f}
Total Reward: {total_reward:.4f}

=== Improvement ===
Reward Improvement: {enhanced_reward - original_reward:.4f}
Relative Improvement: {((enhanced_reward - original_reward) / max(original_reward, 0.001) * 100):.2f}%

=== Files ===
Original Image: original_image.png
Enhanced Image: enhanced_image.png
"""
            
            # ë¡œê·¸ íŒŒì¼ ì €ì¥
            with open(os.path.join(self.episode_dir, "episode_log.txt"), "w", encoding="utf-8") as f:
                f.write(log_content)
            
            logger.info(f"ğŸ’¾ ì—í”¼ì†Œë“œ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {self.episode_dir}")
            
        except Exception as e:
            logger.warning(f"Failed to save episode results: {e}")
    
    def _save_error_log(self, enhanced_prompt: str, error_msg: str):
        """ì—ëŸ¬ ë°œìƒ ì‹œ ë¡œê·¸ë§Œ ì €ì¥"""
        try:
            # ì—ëŸ¬ ë¡œê·¸ íŒŒì¼ ì‘ì„±
            error_log_content = f"""=== QWEN GRPO Error Log ===
Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Episode: {self.episode_count}

=== Error Info ===
Error Message: {error_msg}
Original Prompt: {self.current_user_prompt}
Enhanced Prompt: {enhanced_prompt}

=== Note ===
Images could not be generated/saved due to the error above.
"""
            
            # ì—ëŸ¬ ë¡œê·¸ íŒŒì¼ ì €ì¥
            error_log_path = os.path.join(self.episode_dir, "error_log.txt")
            with open(error_log_path, "w", encoding="utf-8") as f:
                f.write(error_log_content)
            
            logger.info(f"ğŸ“ ì—ëŸ¬ ë¡œê·¸ ì €ì¥ ì™„ë£Œ: {error_log_path}")
            
        except Exception as log_error:
            logger.error(f"âŒ ì—ëŸ¬ ë¡œê·¸ ì €ì¥ë„ ì‹¤íŒ¨: {log_error}")

class QWENGRPOTrainer:
    """QWEN í†µí•© GRPO íŠ¸ë ˆì´ë„ˆ (Accelerate ì§€ì›)"""
    
    def __init__(self, qwen_model: QWENModel, reward_model, sd_pipeline, config: QWENGRPOConfig):
        self.config = config
        self.qwen_model = qwen_model
        self.reward_model = reward_model
        self.sd_pipeline = sd_pipeline
        self.accelerator = None  # Accelerate ê°ì²´ (ë‚˜ì¤‘ì— ì„¤ì •)
        
        # ë©€í‹° í”„ë¡œì„¸ìŠ¤ í™˜ê²½ì—ì„œ ë©”ì¸ í”„ë¡œì„¸ìŠ¤ ì—¬ë¶€ í™•ì¸
        self.is_main_process = reward_model is not None and sd_pipeline is not None
        
        # QWEN ëª¨ë¸ì— GRPO ì»´í¬ë„ŒíŠ¸ê°€ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        if not hasattr(qwen_model, 'ref_model'):
            raise ValueError("QWEN ëª¨ë¸ì— GRPO ì»´í¬ë„ŒíŠ¸ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. grpo_configë¥¼ ì „ë‹¬í•˜ì—¬ ì´ˆê¸°í™”í•˜ì„¸ìš”.")
        
        self.env = QWENGRPOEnvironment(qwen_model, reward_model, sd_pipeline, config)
        
        # ë¦¬ì›Œë“œ ì¶”ì ì„ ìœ„í•œ ë³€ìˆ˜ë“¤
        self.episode_rewards = []
        self.episode_numbers = []
        self.running_avg_rewards = []
        
        # í”Œë¡¯ ì €ì¥ì„ ìœ„í•œ ë””ë ‰í† ë¦¬ ì„¤ì • (Environmentì™€ ë™ì¼í•œ ë””ë ‰í† ë¦¬ ì‚¬ìš©)
        self.plot_save_dir = self.env.base_log_dir if config.save_images else config.log_dir
        os.makedirs(self.plot_save_dir, exist_ok=True)
        
        # í”Œë¡¯ ì„¤ì •
        mplstyle.use('fast')
        plt.ion()  # Interactive mode on
        
        logger.info("ğŸ¯ QWEN GRPO íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"âœ… QWEN ì§ì ‘ í•™ìŠµ ë°©ì‹ìœ¼ë¡œ ì„¤ì • (Accelerate ì§€ì›)")
        logger.info(f"ğŸ“Š í”Œë¡¯ ì €ì¥ ë””ë ‰í† ë¦¬: {self.plot_save_dir}")
    
    def collect_rollouts(self, prompts: List[str], is_baseline: bool = False) -> List[Dict]:
        """ë¡¤ì•„ì›ƒ ìˆ˜ì§‘ - ë°°ì¹˜ ì´ë¯¸ì§€ ìƒì„± ìµœì í™”"""
        all_experiences = []
        rollout_type = "ë² ì´ìŠ¤ë¼ì¸" if is_baseline else "í•™ìŠµìš©"
        
        for prompt_idx, user_prompt in enumerate(prompts):
            logger.info(f"\nğŸ“ {rollout_type} í”„ë¡¬í”„íŠ¸ {prompt_idx + 1}/{len(prompts)}: '{user_prompt}'")
            
            # ë°°ì¹˜ ë¡¤ì•„ì›ƒ ìˆ˜ì§‘ (ëª¨ë“  í”„ë¡œì„¸ìŠ¤ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ í•œë²ˆì— ì²˜ë¦¬)
            batch_experiences = self.collect_batch_rollouts(user_prompt, is_baseline)
            all_experiences.extend(batch_experiences)
        
        logger.info(f"\nğŸ“Š ìˆ˜ì§‘ëœ {rollout_type} ê²½í—˜: {len(all_experiences)}ê°œ")
        return all_experiences
    
    def collect_batch_rollouts(self, user_prompt: str, is_baseline: bool = False) -> List[Dict]:
        """ë°°ì¹˜ ë¡¤ì•„ì›ƒ ìˆ˜ì§‘ - ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œ ë°°ì¹˜ ì´ë¯¸ì§€ ìƒì„±"""
        batch_experiences = []
        
        # ëª¨ë“  í”„ë¡œì„¸ìŠ¤ì—ì„œ í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ ìƒì„±
        enhanced_prompts = []
        log_probs = []
        
        for rollout_idx in range(self.config.num_rollouts):
            logger.info(f"  ğŸ² ë¡¤ì•„ì›ƒ {rollout_idx + 1}/{self.config.num_rollouts}")
            
            try:
                # QWEN GRPOë¡œ í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ ìƒì„±
                if self.accelerator:
                    with self.accelerator.device:
                        enhanced_prompt, log_prob = self.qwen_model.generate_grpo_enhanced_prompt(user_prompt)
                else:
                    enhanced_prompt, log_prob = self.qwen_model.generate_grpo_enhanced_prompt(user_prompt)
                
                enhanced_prompts.append(enhanced_prompt)
                log_probs.append(log_prob)
                logger.info(f"    ğŸ¯ ìƒì„±ëœ í”„ë¡¬í”„íŠ¸: '{enhanced_prompt[:50]}...' (ë¡œê·¸ í™•ë¥ : {log_prob:.4f})")
                
            except Exception as e:
                logger.error(f"    âŒ í”„ë¡¬í”„íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}")
                continue
        
        # ë°°ì¹˜ ì´ë¯¸ì§€ ìƒì„± ë° ë¦¬ì›Œë“œ ê³„ì‚° (ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œë§Œ)
        if self.accelerator and self.accelerator.is_main_process and enhanced_prompts:
            batch_rewards = self.generate_batch_images_and_rewards(user_prompt, enhanced_prompts)
        else:
            # ì„œë¸Œ í”„ë¡œì„¸ìŠ¤ëŠ” ê¸°ë³¸ ë¦¬ì›Œë“œ ì‚¬ìš©
            batch_rewards = [0.3] * len(enhanced_prompts)
        
        # ëª¨ë“  í”„ë¡œì„¸ìŠ¤ì— ë¦¬ì›Œë“œ ë¶„ë°°
        if self.accelerator and self.accelerator.num_processes > 1:
            # ë¦¬ì›Œë“œë¥¼ ëª¨ë“  í”„ë¡œì„¸ìŠ¤ì— ë¸Œë¡œë“œìºìŠ¤íŠ¸
            if self.accelerator.is_main_process:
                rewards_tensor = torch.tensor(batch_rewards, device=self.accelerator.device)
            else:
                rewards_tensor = torch.zeros(len(enhanced_prompts), device=self.accelerator.device)
            
            # ë¸Œë¡œë“œìºìŠ¤íŠ¸
            rewards_tensor = self.accelerator.broadcast(rewards_tensor, from_process=0)
            batch_rewards = rewards_tensor.cpu().tolist()
        
        # ê²½í—˜ ìƒì„±
        for enhanced_prompt, log_prob, reward in zip(enhanced_prompts, log_probs, batch_rewards):
            experience = {
                'user_prompt': user_prompt,
                'enhanced_prompt': enhanced_prompt,
                'log_prob': log_prob,
                'reward': reward,
                'info': {
                    'original_prompt': user_prompt,
                    'enhanced_prompt': enhanced_prompt,
                    'original_reward': reward,
                    'enhanced_reward': reward
                },
                'is_baseline': is_baseline
            }
            batch_experiences.append(experience)
            logger.info(f"    âœ… ë¦¬ì›Œë“œ: {reward:.4f}")
        
        return batch_experiences
    
    def generate_batch_images_and_rewards(self, user_prompt: str, enhanced_prompts: List[str]) -> List[float]:
        """ë°°ì¹˜ ì´ë¯¸ì§€ ìƒì„± ë° ë¦¬ì›Œë“œ ê³„ì‚° (ë©”ì¸ í”„ë¡œì„¸ìŠ¤ ì „ìš©)"""
        logger.info(f"ğŸ–¼ï¸ ë°°ì¹˜ ì´ë¯¸ì§€ ìƒì„± ì‹œì‘ ({len(enhanced_prompts)}ê°œ)")
        
        batch_rewards = []
        available_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 0
        
        try:
            # ë°°ì¹˜ ì´ë¯¸ì§€ ìƒì„± ë° ë¦¬ì›Œë“œ ê³„ì‚°
            for i, enhanced_prompt in enumerate(enhanced_prompts):
                # ì´ë¯¸ì§€ ìƒì„± (GPU 6ë²ˆì—ì„œ SD3 ì‚¬ìš©)
                if available_gpus > 6 and hasattr(self, 'sd_pipeline') and self.sd_pipeline is not None:
                    with torch.cuda.device(6):
                        enhanced_result = self.sd_pipeline(
                            prompt=enhanced_prompt,
                            num_inference_steps=28,
                            guidance_scale=7.0,
                            height=1024,
                            width=1024
                        )
                        enhanced_image = enhanced_result.images[0]
                else:
                    # SD3ê°€ ì—†ëŠ” ê²½ìš° ë”ë¯¸ ì´ë¯¸ì§€
                    from PIL import Image
                    enhanced_image = Image.new('RGB', (1024, 1024), color='black')
                
                # ë¦¬ì›Œë“œ ê³„ì‚° (GPU 5ë²ˆì—ì„œ CLIP ì‚¬ìš©)
                if available_gpus > 5 and hasattr(self, 'reward_model') and self.reward_model is not None:
                    with torch.cuda.device(5):
                        reward = self.reward_model.calculate_reward(
                            user_prompt,
                            enhanced_prompt,
                            enhanced_image
                        )
                else:
                    reward = 0.3  # ê¸°ë³¸ ë¦¬ì›Œë“œ
                
                batch_rewards.append(reward)
                logger.info(f"  ì´ë¯¸ì§€ {i+1}/{len(enhanced_prompts)}: ë¦¬ì›Œë“œ {reward:.4f}")
                
        except Exception as e:
            logger.error(f"âŒ ë°°ì¹˜ ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")
            batch_rewards = [0.1] * len(enhanced_prompts)  # ì—ëŸ¬ ì‹œ ë‚®ì€ ë¦¬ì›Œë“œ
        
        logger.info(f"âœ… ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: í‰ê·  ë¦¬ì›Œë“œ {sum(batch_rewards)/len(batch_rewards):.4f}")
        return batch_rewards
    
    def compute_grpo_advantages(self, experiences: List[Dict]) -> List[Dict]:
        """GRPO ì–´ë“œë°´í‹°ì§€ ê³„ì‚° (ê·¸ë£¹ í‰ê·  baseline)"""
        if not experiences:
            return experiences
        
        # í”„ë¡¬í”„íŠ¸ë³„ë¡œ ê·¸ë£¹í™”
        prompt_groups = defaultdict(list)
        for exp in experiences:
            prompt_groups[exp['user_prompt']].append(exp)
        
        # ê·¸ë£¹ë³„ ì–´ë“œë°´í‹°ì§€ ê³„ì‚°
        for prompt, group_exps in prompt_groups.items():
            rewards = [exp['reward'] for exp in group_exps]
            group_baseline = np.mean(rewards)
            
            # ê° ê²½í—˜ì— ì–´ë“œë°´í‹°ì§€ ì¶”ê°€
            for exp in group_exps:
                exp['advantage'] = exp['reward'] - group_baseline
                exp['baseline'] = group_baseline
        
        return experiences
    
    def train_step(self, experiences: List[Dict]) -> Dict:
        """GRPO í•™ìŠµ ìŠ¤í…"""
        if not experiences:
            return {}
        
        logger.info(f"ğŸ¯ GRPO í•™ìŠµ ìŠ¤í… ì‹œì‘ (ê²½í—˜: {len(experiences)}ê°œ)")
        
        # ì–´ë“œë°´í‹°ì§€ ê³„ì‚°
        experiences = self.compute_grpo_advantages(experiences)
        
        # QWEN ëª¨ë¸ì˜ GRPO ì—…ë°ì´íŠ¸ í˜¸ì¶œ (Accelerate ì§€ì›)
        if self.accelerator:
            # Accelerate í™˜ê²½ì—ì„œëŠ” device context ë¶ˆí•„ìš”
            metrics = self.qwen_model.update_grpo_policy(experiences)
        else:
            with torch.cuda.device(0):
                metrics = self.qwen_model.update_grpo_policy(experiences)
        
        logger.info(f"âœ… GRPO ì—…ë°ì´íŠ¸ ì™„ë£Œ")
        logger.info(f"  Policy Loss: {metrics.get('policy_loss', 0):.4f}")
        logger.info(f"  KL Div: {metrics.get('kl_div', 0):.4f}")
        logger.info(f"  Mean Reward: {metrics.get('mean_reward', 0):.4f}")
        
        return metrics
    
    def collect_baseline_data(self, train_prompts: List[str], num_baseline_episodes: int = 3):
        """ë² ì´ìŠ¤ë¼ì¸ ë°ì´í„° ìˆ˜ì§‘ (í•™ìŠµì— ì‚¬ìš©ë˜ì§€ ì•ŠìŒ)"""
        logger.info(f"ğŸ“Š ë² ì´ìŠ¤ë¼ì¸ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ ({num_baseline_episodes} ì—í”¼ì†Œë“œ)")
        logger.info("=" * 80)
        
        baseline_experiences = []
        
        for episode in range(num_baseline_episodes):
            logger.info(f"\nğŸ“‹ ë² ì´ìŠ¤ë¼ì¸ ì—í”¼ì†Œë“œ {episode + 1}/{num_baseline_episodes}")
            logger.info("-" * 60)
            
            # ë² ì´ìŠ¤ë¼ì¸ìš© ë¡¤ì•„ì›ƒ ìˆ˜ì§‘ (ì €ì¥í•˜ì§€ ì•ŠìŒ)
            episode_experiences = self.collect_rollouts(train_prompts, is_baseline=True)
            baseline_experiences.extend(episode_experiences)
            
            if episode_experiences:
                episode_rewards = [exp['reward'] for exp in episode_experiences]
                avg_reward = np.mean(episode_rewards)
                logger.info(f"ğŸ“ˆ ë² ì´ìŠ¤ë¼ì¸ ì—í”¼ì†Œë“œ {episode + 1} í‰ê·  ë¦¬ì›Œë“œ: {avg_reward:.4f}")
        
        # ë² ì´ìŠ¤ë¼ì¸ í†µê³„ ê³„ì‚°
        if baseline_experiences:
            baseline_rewards = [exp['reward'] for exp in baseline_experiences]
            baseline_mean = np.mean(baseline_rewards)
            baseline_std = np.std(baseline_rewards)
            
            logger.info("\nğŸ“Š ë² ì´ìŠ¤ë¼ì¸ í†µê³„:")
            logger.info(f"  í‰ê·  ë¦¬ì›Œë“œ: {baseline_mean:.4f}")
            logger.info(f"  í‘œì¤€í¸ì°¨: {baseline_std:.4f}")
            logger.info(f"  ìµœëŒ€ ë¦¬ì›Œë“œ: {np.max(baseline_rewards):.4f}")
            logger.info(f"  ìµœì†Œ ë¦¬ì›Œë“œ: {np.min(baseline_rewards):.4f}")
            logger.info(f"  ì´ ê²½í—˜ ìˆ˜: {len(baseline_experiences)}ê°œ")
        
        # í™˜ê²½ ì—í”¼ì†Œë“œ ì¹´ìš´í„° ë¦¬ì…‹ (ì‹¤ì œ í•™ìŠµì€ 1ë¶€í„° ì‹œì‘)
        self.env.episode_count = 0
        logger.info("\nğŸ”„ í™˜ê²½ ì—í”¼ì†Œë“œ ì¹´ìš´í„° ë¦¬ì…‹ ì™„ë£Œ")
        logger.info("âœ… ë² ì´ìŠ¤ë¼ì¸ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
        
        return baseline_experiences

    def train(self, train_prompts: List[str], num_epochs: int = 10, num_baseline_episodes: int = 3):
        """GRPO í•™ìŠµ ì‹¤í–‰"""
        logger.info(f"ğŸš€ QWEN GRPO í•™ìŠµ ì‹œì‘")
        logger.info("=" * 80)
        
        # 1ë‹¨ê³„: ë² ì´ìŠ¤ë¼ì¸ ë°ì´í„° ìˆ˜ì§‘ (í•™ìŠµì— ì‚¬ìš©ë˜ì§€ ì•ŠìŒ)
        baseline_data = self.collect_baseline_data(train_prompts, num_baseline_episodes)
        
        logger.info(f"\nğŸ¯ ì‹¤ì œ GRPO í•™ìŠµ ì‹œì‘ (ì—í¬í¬: {num_epochs})")
        logger.info("=" * 80)
        
        all_metrics = []
        
        for epoch in range(num_epochs):
            logger.info(f"\nğŸ”„ í•™ìŠµ ì—í¬í¬ {epoch + 1}/{num_epochs}")
            logger.info("-" * 60)
            
            # í•™ìŠµìš© ë¡¤ì•„ì›ƒ ìˆ˜ì§‘
            experiences = self.collect_rollouts(train_prompts, is_baseline=False)
            
            if not experiences:
                logger.warning("âš ï¸ ìˆ˜ì§‘ëœ ê²½í—˜ì´ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìŒ ì—í¬í¬ë¡œ ê±´ë„ˆëœë‹ˆë‹¤.")
                continue
            
            # í•™ìŠµ ìŠ¤í…
            metrics = self.train_step(experiences)
            metrics['epoch'] = epoch + 1
            all_metrics.append(metrics)
            
            # ì—í”¼ì†Œë“œ í‰ê·  ë¦¬ì›Œë“œ ê³„ì‚° ë° í”Œë¡¯ ì—…ë°ì´íŠ¸
            epoch_rewards = [exp['reward'] for exp in experiences]
            avg_reward = np.mean(epoch_rewards)
            self._update_reward_plot(epoch + 1, avg_reward)
            
            # ì£¼ê¸°ì ìœ¼ë¡œ ìƒ˜í”Œ ì¶œë ¥ í™•ì¸ (ë‹¤ì–‘í•œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©)
            if (epoch + 1) % 3 == 0:
                logger.info(f"\nğŸ“‹ ì—í¬í¬ {epoch + 1} ìƒ˜í”Œ ì¶œë ¥:")
                # ë§¤ë²ˆ ë‹¤ë¥¸ í”„ë¡¬í”„íŠ¸ ì„ íƒ (ìˆœí™˜)
                sample_indices = [(epoch * 2) % len(train_prompts), ((epoch * 2) + 1) % len(train_prompts)]
                sample_prompts = [train_prompts[i] for i in sample_indices]
                self._log_sample_outputs(sample_prompts)
        
        # ìµœì¢… í”Œë¡¯ ì €ì¥
        self._save_reward_plot()
        logger.info("\nâœ… QWEN GRPO í•™ìŠµ ì™„ë£Œ!")
        return all_metrics, baseline_data
    
    def _log_sample_outputs(self, sample_prompts: List[str]):
        """ìƒ˜í”Œ ì¶œë ¥ ë¡œê¹…"""
        for prompt in sample_prompts:
            try:
                if self.accelerator:
                    # Accelerate í™˜ê²½ì—ì„œ ìƒ˜í”Œ ì¶œë ¥
                    basic_result = self.qwen_model.enhance_prompt(prompt)
                    grpo_enhanced, _ = self.qwen_model.generate_grpo_enhanced_prompt(prompt)
                else:
                    with torch.cuda.device(0):
                        # ê¸°ë³¸ í–¥ìƒ
                        basic_result = self.qwen_model.enhance_prompt(prompt)
                        
                        # GRPO ê¸°ë°˜ ìƒì„±
                        grpo_enhanced, _ = self.qwen_model.generate_grpo_enhanced_prompt(prompt)
                
                logger.info(f"  ì›ë³¸: '{prompt}'")
                logger.info(f"  ê¸°ë³¸: '{basic_result['enhanced_prompt'][:60]}...'")
                logger.info(f"  GRPO: '{grpo_enhanced[:60]}...'")
                
            except Exception as e:
                logger.warning(f"  ìƒ˜í”Œ ì¶œë ¥ ì‹¤íŒ¨: {e}")
    
    def _update_reward_plot(self, epoch: int, avg_reward: float):
        """ì‹¤ì‹œê°„ ë¦¬ì›Œë“œ í”Œë¡¯ ì—…ë°ì´íŠ¸"""
        self.episode_numbers.append(epoch)
        self.episode_rewards.append(avg_reward)
        
        # ì´ë™ í‰ê·  ê³„ì‚° (ìœˆë„ìš° í¬ê¸°: 5)
        window_size = min(5, len(self.episode_rewards))
        if len(self.episode_rewards) >= window_size:
            running_avg = np.mean(self.episode_rewards[-window_size:])
            self.running_avg_rewards.append(running_avg)
        else:
            self.running_avg_rewards.append(avg_reward)
        
        # í”Œë¡¯ ì—…ë°ì´íŠ¸
        try:
            plt.clf()  # Clear figure
            
            # ë©”ì¸ ë¦¬ì›Œë“œ í”Œë¡¯
            plt.plot(self.episode_numbers, self.episode_rewards, 'b-', alpha=0.6, label='Episode Reward')
            plt.plot(self.episode_numbers, self.running_avg_rewards, 'r-', linewidth=2, label='Moving Average (5)')
            
            plt.title('QWEN GRPO Training Progress', fontsize=14, fontweight='bold')
            plt.xlabel('Epoch', fontsize=12)
            plt.ylabel('Average Reward', fontsize=12)
            plt.legend()
            plt.grid(True, alpha=0.3)
            
            # Yì¶• ë²”ìœ„ ìë™ ì¡°ì •
            if len(self.episode_rewards) > 1:
                y_min = min(self.episode_rewards) * 0.95
                y_max = max(self.episode_rewards) * 1.05
                plt.ylim(y_min, y_max)
            
            # í˜„ì¬ ì—í¬í¬ ì •ë³´ í‘œì‹œ
            plt.text(0.02, 0.98, f'Current Epoch: {epoch}\nCurrent Reward: {avg_reward:.4f}\nMoving Avg: {self.running_avg_rewards[-1]:.4f}', 
                    transform=plt.gca().transAxes, fontsize=10, verticalalignment='top',
                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
            
            plt.tight_layout()
            plt.pause(0.01)  # ì§§ì€ pauseë¡œ í”Œë¡¯ ì—…ë°ì´íŠ¸
            
        except Exception as e:
            logger.warning(f"âš ï¸ í”Œë¡¯ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def _save_reward_plot(self):
        """ìµœì¢… ë¦¬ì›Œë“œ í”Œë¡¯ ì €ì¥"""
        try:
            if not self.episode_numbers:
                logger.warning("âš ï¸ ì €ì¥í•  ë¦¬ì›Œë“œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            # ìµœì¢… í”Œë¡¯ ìƒì„±
            plt.figure(figsize=(12, 8))
            
            # ì„œë¸Œí”Œë¡¯ 1: ë¦¬ì›Œë“œ ì¶”ì´
            plt.subplot(2, 1, 1)
            plt.plot(self.episode_numbers, self.episode_rewards, 'b-', alpha=0.6, marker='o', markersize=4, label='Episode Reward')
            plt.plot(self.episode_numbers, self.running_avg_rewards, 'r-', linewidth=3, label='Moving Average (5)')
            plt.title('QWEN GRPO Training Progress - Reward Trend', fontsize=14, fontweight='bold')
            plt.xlabel('Epoch')
            plt.ylabel('Average Reward')
            plt.legend()
            plt.grid(True, alpha=0.3)
            
            # ì„œë¸Œí”Œë¡¯ 2: ë¦¬ì›Œë“œ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
            plt.subplot(2, 1, 2)
            plt.hist(self.episode_rewards, bins=min(20, len(self.episode_rewards)), alpha=0.7, color='skyblue', edgecolor='black')
            plt.title('Reward Distribution', fontsize=14, fontweight='bold')
            plt.xlabel('Reward Value')
            plt.ylabel('Frequency')
            plt.grid(True, alpha=0.3)
            
            # í†µê³„ ì •ë³´ ì¶”ê°€
            mean_reward = np.mean(self.episode_rewards)
            std_reward = np.std(self.episode_rewards)
            max_reward = np.max(self.episode_rewards)
            min_reward = np.min(self.episode_rewards)
            
            stats_text = f'Statistics:\nMean: {mean_reward:.4f}\nStd: {std_reward:.4f}\nMax: {max_reward:.4f}\nMin: {min_reward:.4f}'
            plt.text(0.02, 0.98, stats_text, transform=plt.gca().transAxes, fontsize=10, 
                    verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))
            
            plt.tight_layout()
            
            # ì €ì¥
            if self.config.save_images:
                plot_path = os.path.join(self.plot_save_dir, 'training_progress.png')
                plt.savefig(plot_path, dpi=300, bbox_inches='tight')
                logger.info(f"ğŸ“Š í•™ìŠµ ì§„í–‰ í”Œë¡¯ ì €ì¥: {plot_path}")
                
                # ë°ì´í„°ë„ CSVë¡œ ì €ì¥
                import csv
                csv_path = os.path.join(self.plot_save_dir, 'training_rewards.csv')
                with open(csv_path, 'w', newline='', encoding='utf-8') as f:
                    writer = csv.writer(f)
                    writer.writerow(['Epoch', 'Average_Reward', 'Moving_Average'])
                    for i in range(len(self.episode_numbers)):
                        writer.writerow([self.episode_numbers[i], self.episode_rewards[i], self.running_avg_rewards[i]])
                logger.info(f"ğŸ“Š í•™ìŠµ ë°ì´í„° ì €ì¥: {csv_path}")
            
            plt.show()
            
        except Exception as e:
            logger.error(f"âŒ ìµœì¢… í”Œë¡¯ ì €ì¥ ì‹¤íŒ¨: {e}")

# ... existing code ...

 