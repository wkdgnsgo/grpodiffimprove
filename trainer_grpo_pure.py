#!/usr/bin/env python3
"""
ìˆœìˆ˜ GRPO íŠ¸ë ˆì´ë„ˆ (easyr1 ìŠ¤íƒ€ì¼)
Value Network ì—†ì´ ì˜¤ì§ Policy Networkë§Œ ì‚¬ìš©
ê·¸ë£¹ í‰ê· ì„ implicit baselineìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ì˜¬ë°”ë¥¸ GRPO êµ¬í˜„
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import logging
from typing import Dict, List, Tuple, Optional
from collections import defaultdict
from dataclasses import dataclass
import math
import re
import json
import os
from datetime import datetime

logger = logging.getLogger(__name__)

@dataclass
class PureGRPOConfig:
    """ìˆœìˆ˜ GRPO ì„¤ì • (Value Network ì—†ìŒ)"""
    learning_rate: float = 1e-6
    batch_size: int = 4
    num_rollouts: int = 5  # ê·¸ë£¹ë³„ ë¡¤ì•„ì›ƒ ìˆ˜
    max_prompt_length: int = 77
    max_new_tokens: int = 30
    temperature: float = 1.0
    top_p: float = 0.9
    top_k: int = 100
    kl_coef: float = 0.02
    clip_ratio: float = 0.1
    entropy_coef: float = 0.02
    vocab_size: int = 32000
    enable_step_logging: bool = True  # ìƒì„¸ ìŠ¤í… ë¡œê¹… í™œì„±í™”
    log_dir: str = "training_logs"    # ë¡œê·¸ ì €ì¥ ë””ë ‰í† ë¦¬

class StepLogger:
    """ê° ìŠ¤í…ì˜ ìƒì„¸ ì •ë³´ë¥¼ ê¸°ë¡í•˜ëŠ” ë¡œê±°"""
    
    def __init__(self, log_dir: str):
        self.log_dir = log_dir
        os.makedirs(log_dir, exist_ok=True)
        self.step_data = []
        self.episode_counter = 0
        
        # ì´ë¯¸ì§€ ì €ì¥ ë””ë ‰í† ë¦¬
        self.image_dir = os.path.join(log_dir, "images")
        os.makedirs(self.image_dir, exist_ok=True)
        
        # ì—í”¼ì†Œë“œë³„ ë””ë ‰í† ë¦¬
        self.episodes_dir = os.path.join(log_dir, "episodes")
        os.makedirs(self.episodes_dir, exist_ok=True)
        
        # ìš”ì•½ í†µê³„ ì €ì¥
        self.summary_stats = {
            'total_episodes': 0,
            'total_steps': 0,
            'average_reward': 0.0,
            'best_reward': 0.0,
            'worst_reward': 0.0,
            'reward_history': []
        }
    
    def log_step(self, step_info: Dict):
        """ìŠ¤í… ì •ë³´ ë¡œê¹…"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        step_info['timestamp'] = timestamp
        self.step_data.append(step_info)
        
        # ì½˜ì†” ì¶œë ¥
        self._print_step_summary(step_info)
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        self._save_to_json()
    
    def _print_step_summary(self, step_info: Dict):
        """ìŠ¤í… ìš”ì•½ ì •ë³´ ì¶œë ¥"""
        print("\n" + "="*80)
        print(f"ğŸ“Š STEP {step_info.get('step', 'N/A')} - {step_info.get('timestamp', '')}")
        print("="*80)
        
        print(f"ğŸ”¤ Original Prompt: '{step_info.get('original_prompt', 'N/A')}'")
        print(f"âœ¨ Enhanced Prompt: '{step_info.get('enhanced_prompt', 'N/A')}'")
        
        if 'reward_components' in step_info:
            rewards = step_info['reward_components']
            print(f"ğŸ¯ Rewards:")
            print(f"   - Originalâ†’Image: {rewards.get('original_reward', 0):.3f}")
            print(f"   - Enhancedâ†’Image: {rewards.get('enhanced_reward', 0):.3f}")
            print(f"   - Final Reward: {rewards.get('final_reward', 0):.3f}")
        
        if 'action_info' in step_info:
            action = step_info['action_info']
            print(f"ğŸ¬ Action: Token {action.get('token_id', 'N/A')} â†’ '{action.get('token_text', 'N/A')}'")
            print(f"   - Log Prob: {action.get('log_prob', 0):.4f}")
        
        if 'images_saved' in step_info:
            print(f"ğŸ–¼ï¸  Images saved: {step_info['images_saved']}")
        
        print("="*80)
    
    def _save_to_json(self):
        """JSON íŒŒì¼ë¡œ ì €ì¥"""
        json_path = os.path.join(self.log_dir, "step_logs.json")
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(self.step_data, f, indent=2, ensure_ascii=False)
    
    def save_image(self, image, filename: str) -> str:
        """ì´ë¯¸ì§€ ì €ì¥"""
        image_path = os.path.join(self.image_dir, filename)
        image.save(image_path)
        return image_path
    
    def start_new_episode(self, episode_id: str, original_prompt: str):
        """ìƒˆ ì—í”¼ì†Œë“œ ì‹œì‘"""
        self.episode_counter += 1
        episode_dir = os.path.join(self.episodes_dir, f"episode_{self.episode_counter:03d}_{episode_id}")
        os.makedirs(episode_dir, exist_ok=True)
        
        # ì—í”¼ì†Œë“œ ë©”íƒ€ë°ì´í„° ì €ì¥
        episode_meta = {
            'episode_id': episode_id,
            'episode_number': self.episode_counter,
            'original_prompt': original_prompt,
            'start_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'steps': []
        }
        
        meta_path = os.path.join(episode_dir, "episode_meta.json")
        with open(meta_path, 'w', encoding='utf-8') as f:
            json.dump(episode_meta, f, indent=2, ensure_ascii=False)
        
        return episode_dir
    
    def log_episode_step(self, episode_dir: str, step_data: Dict):
        """ì—í”¼ì†Œë“œ ë‚´ ìŠ¤í… ë¡œê¹…"""
        # ì—í”¼ì†Œë“œ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
        meta_path = os.path.join(episode_dir, "episode_meta.json")
        with open(meta_path, 'r', encoding='utf-8') as f:
            episode_meta = json.load(f)
        
        episode_meta['steps'].append(step_data)
        
        with open(meta_path, 'w', encoding='utf-8') as f:
            json.dump(episode_meta, f, indent=2, ensure_ascii=False)
    
    def finish_episode(self, episode_dir: str, final_reward: float, total_steps: int):
        """ì—í”¼ì†Œë“œ ì™„ë£Œ ì²˜ë¦¬"""
        # ì—í”¼ì†Œë“œ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
        meta_path = os.path.join(episode_dir, "episode_meta.json")
        with open(meta_path, 'r', encoding='utf-8') as f:
            episode_meta = json.load(f)
        
        episode_meta.update({
            'end_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'final_reward': final_reward,
            'total_steps': total_steps,
            'completed': True
        })
        
        with open(meta_path, 'w', encoding='utf-8') as f:
            json.dump(episode_meta, f, indent=2, ensure_ascii=False)
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self.summary_stats['total_episodes'] += 1
        self.summary_stats['total_steps'] += total_steps
        self.summary_stats['reward_history'].append(final_reward)
        
        if len(self.summary_stats['reward_history']) == 1:
            self.summary_stats['best_reward'] = final_reward
            self.summary_stats['worst_reward'] = final_reward
        else:
            self.summary_stats['best_reward'] = max(self.summary_stats['best_reward'], final_reward)
            self.summary_stats['worst_reward'] = min(self.summary_stats['worst_reward'], final_reward)
        
        self.summary_stats['average_reward'] = np.mean(self.summary_stats['reward_history'])
        
        # ìš”ì•½ í†µê³„ ì €ì¥
        stats_path = os.path.join(self.log_dir, "summary_stats.json")
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(self.summary_stats, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ… ì—í”¼ì†Œë“œ ì™„ë£Œ: ë¦¬ì›Œë“œ={final_reward:.3f}, ìŠ¤í…={total_steps}")
    
    def create_comparison_html(self):
        """ì´ë¯¸ì§€ ë¹„êµë¥¼ ìœ„í•œ HTML ë³´ê³ ì„œ ìƒì„±"""
        html_content = """
<!DOCTYPE html>
<html>
<head>
    <title>GRPO í›ˆë ¨ ê²°ê³¼ ë¹„êµ</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        .episode { border: 1px solid #ddd; margin: 20px 0; padding: 20px; }
        .step { border-left: 3px solid #007bff; margin: 10px 0; padding: 10px; }
        .image-comparison { display: flex; gap: 20px; margin: 10px 0; }
        .image-container { text-align: center; }
        .image-container img { max-width: 300px; height: auto; border: 1px solid #ddd; }
        .reward-info { background: #f8f9fa; padding: 10px; margin: 10px 0; }
        .prompt-info { background: #e9ecef; padding: 10px; margin: 10px 0; }
    </style>
</head>
<body>
    <h1>ğŸ¯ GRPO í›ˆë ¨ ê²°ê³¼ ë¹„êµ</h1>
    <div class="summary">
        <h2>ğŸ“Š ìš”ì•½ í†µê³„</h2>
        <p>ì´ ì—í”¼ì†Œë“œ: {total_episodes}</p>
        <p>ì´ ìŠ¤í…: {total_steps}</p>
        <p>í‰ê·  ë¦¬ì›Œë“œ: {average_reward:.3f}</p>
        <p>ìµœê³  ë¦¬ì›Œë“œ: {best_reward:.3f}</p>
        <p>ìµœì € ë¦¬ì›Œë“œ: {worst_reward:.3f}</p>
    </div>
""".format(**self.summary_stats)
        
        # ê° ì—í”¼ì†Œë“œ ì •ë³´ ì¶”ê°€
        for episode_dir in sorted(os.listdir(self.episodes_dir)):
            episode_path = os.path.join(self.episodes_dir, episode_dir)
            meta_path = os.path.join(episode_path, "episode_meta.json")
            
            if os.path.exists(meta_path):
                with open(meta_path, 'r', encoding='utf-8') as f:
                    episode_meta = json.load(f)
                
                html_content += f"""
    <div class="episode">
        <h3>ì—í”¼ì†Œë“œ {episode_meta['episode_number']}: {episode_meta['original_prompt']}</h3>
        <p>ìµœì¢… ë¦¬ì›Œë“œ: {episode_meta.get('final_reward', 0):.3f}</p>
        <p>ì´ ìŠ¤í…: {episode_meta.get('total_steps', 0)}</p>
"""
                
                # ê° ìŠ¤í…ì˜ ì´ë¯¸ì§€ ë¹„êµ
                for step in episode_meta.get('steps', []):
                    if 'images_saved' in step:
                        html_content += f"""
        <div class="step">
            <h4>ìŠ¤í… {step['step']}</h4>
            <div class="prompt-info">
                <p><strong>ì›ë³¸ í”„ë¡¬í”„íŠ¸:</strong> {step['original_prompt']}</p>
                <p><strong>í–¥ìƒëœ í”„ë¡¬í”„íŠ¸:</strong> {step['enhanced_prompt']}</p>
            </div>
            <div class="reward-info">
                <p><strong>ë¦¬ì›Œë“œ:</strong> ì›ë³¸â†’ì´ë¯¸ì§€ {step['reward_components']['original_reward']:.3f}, 
                   í–¥ìƒâ†’ì´ë¯¸ì§€ {step['reward_components']['enhanced_reward']:.3f}</p>
            </div>
            <div class="image-comparison">
                <div class="image-container">
                    <img src="{step['images_saved']['original']}" alt="ì›ë³¸ ì´ë¯¸ì§€">
                    <p>ì›ë³¸ í”„ë¡¬í”„íŠ¸ ì´ë¯¸ì§€</p>
                </div>
                <div class="image-container">
                    <img src="{step['images_saved']['enhanced']}" alt="í–¥ìƒëœ ì´ë¯¸ì§€">
                    <p>í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ ì´ë¯¸ì§€</p>
                </div>
            </div>
        </div>
"""
                
                html_content += "    </div>\n"
        
        html_content += """
</body>
</html>
"""
        
        # HTML íŒŒì¼ ì €ì¥
        html_path = os.path.join(self.log_dir, "comparison_report.html")
        with open(html_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        logger.info(f"ğŸ“„ HTML ë¹„êµ ë³´ê³ ì„œ ìƒì„±ë¨: {html_path}")
        return html_path

class EnglishTokenFilter:
    """ì˜ì–´ í† í°ë§Œ í—ˆìš©í•˜ëŠ” í•„í„°"""
    
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer
        self.english_token_ids = self._build_english_vocab()
        logger.info(f"ì˜ì–´ í† í° í•„í„° ì´ˆê¸°í™”: {len(self.english_token_ids)}/{len(tokenizer.get_vocab())} í† í°")
    
    def _build_english_vocab(self) -> set:
        """ì˜ì–´ í† í° ID ì§‘í•© êµ¬ì„±"""
        vocab = self.tokenizer.get_vocab()
        english_tokens = set()
        
        # ì˜ì–´ íŒ¨í„´ ì •ì˜
        english_pattern = re.compile(r'^[a-zA-Z0-9\s\.,!?;:\-_\'\"()\[\]{}@#$%^&*+=<>/\\|`~]*$')
        
        for token, token_id in vocab.items():
            # í† í° ë””ì½”ë”©
            try:
                decoded = self.tokenizer.decode([token_id], skip_special_tokens=False)
                # ì˜ì–´ íŒ¨í„´ ë§¤ì¹­
                if english_pattern.match(decoded.strip()):
                    english_tokens.add(token_id)
            except:
                continue
        
        # íŠ¹ìˆ˜ í† í°ë“¤ ì¶”ê°€ (EOS, BOS, PAD ë“±)
        special_tokens = [
            self.tokenizer.eos_token_id,
            self.tokenizer.bos_token_id if hasattr(self.tokenizer, 'bos_token_id') else None,
            self.tokenizer.pad_token_id if hasattr(self.tokenizer, 'pad_token_id') else None,
            self.tokenizer.unk_token_id if hasattr(self.tokenizer, 'unk_token_id') else None,
        ]
        
        for token_id in special_tokens:
            if token_id is not None:
                english_tokens.add(token_id)
        
        return english_tokens
    
    def filter_logits(self, logits: torch.Tensor) -> torch.Tensor:
        """ì˜ì–´ê°€ ì•„ë‹Œ í† í°ì˜ ë¡œì§“ì„ -infë¡œ ì„¤ì •"""
        filtered_logits = logits.clone()
        
        # ëª¨ë“  í† í°ì„ -infë¡œ ì„¤ì •
        filtered_logits.fill_(float('-inf'))
        
        # ì˜ì–´ í† í°ë§Œ ì›ë˜ ê°’ìœ¼ë¡œ ë³µì›
        for token_id in self.english_token_ids:
            if token_id < logits.size(-1):
                filtered_logits[..., token_id] = logits[..., token_id]
        
        return filtered_logits

class PureGRPOPolicy(nn.Module):
    """ìˆœìˆ˜ GRPO ì •ì±… ë„¤íŠ¸ì›Œí¬ (Value Head ì—†ìŒ)"""
    
    def __init__(self, qwen_model, config: PureGRPOConfig):
        super().__init__()
        self.qwen_model = qwen_model
        self.config = config
        
        # ì˜ì–´ í† í° í•„í„° ì´ˆê¸°í™”
        self.english_filter = EnglishTokenFilter(qwen_model.tokenizer)
        
        # GPU ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.qwen_device = "cuda:0"  # QWENì€ GPU 0
        self.policy_device = "cuda:0"  # Policy headë„ GPU 0ì—ì„œ í•™ìŠµ
        
        self.hidden_size = qwen_model.model.config.hidden_size
        self.vocab_size = len(qwen_model.tokenizer.get_vocab())
        
        logger.info(f"ìˆœìˆ˜ GRPO ì •ì±… - Hidden: {self.hidden_size}, Vocab: {self.vocab_size}")
        logger.info(f"GPU ë°°ì¹˜: QWEN={self.qwen_device}, Policy={self.policy_device}")
        logger.info(f"ì˜ì–´ í† í° í•„í„°ë§ í™œì„±í™”: {len(self.english_filter.english_token_ids)} í† í°")
        
        # ì˜¤ì§ ì •ì±… í—¤ë“œë§Œ! (Value Head ì—†ìŒ) - GPU 0ì— ë°°ì¹˜ (float16ìœ¼ë¡œ í†µì¼)
        self.policy_head = nn.Sequential(
            nn.Linear(self.hidden_size, 2048),
            nn.LayerNorm(2048),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(2048, 1024),
            nn.LayerNorm(1024),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(1024, self.vocab_size)
        ).to(self.policy_device).half()  # float16ìœ¼ë¡œ ë³€í™˜
        
        self._init_weights()
        
        logger.info(f"ìˆœìˆ˜ GRPO ì •ì±… ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™” ì™„ë£Œ - Action Space: {self.vocab_size}")
        logger.info("âœ… Value Network ì—†ìŒ - ê·¸ë£¹ í‰ê· ì„ implicit baselineìœ¼ë¡œ ì‚¬ìš©")
    
    def _init_weights(self):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        for layer in self.policy_head:
            if isinstance(layer, nn.Linear):
                gain = 0.02 if layer.out_features == self.vocab_size else 0.1
                nn.init.xavier_normal_(layer.weight, gain=gain)
                nn.init.constant_(layer.bias, 0.0)
    
    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor = None):
        """ì˜¤ì§ ì •ì±… ë¡œì§“ë§Œ ë°˜í™˜ (Values ì—†ìŒ)"""
        batch_size = input_ids.size(0)
        
        # ì…ë ¥ í…ì„œë¥¼ QWEN GPU(0ë²ˆ)ë¡œ ì´ë™
        input_ids = input_ids.to(self.qwen_device)
        if attention_mask is not None:
            attention_mask = attention_mask.to(self.qwen_device)
        
        with torch.no_grad():
            outputs = self.qwen_model.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                output_hidden_states=True
            )
            # QWEN2VL ëª¨ë¸ì€ last_hidden_state ëŒ€ì‹  hidden_statesë¥¼ ì‚¬ìš©
            if hasattr(outputs, 'last_hidden_state') and outputs.last_hidden_state is not None:
                hidden_states = outputs.last_hidden_state
            elif hasattr(outputs, 'hidden_states') and outputs.hidden_states is not None:
                # hidden_statesëŠ” íŠœí”Œì´ë¯€ë¡œ ë§ˆì§€ë§‰ ë ˆì´ì–´ ì„ íƒ
                hidden_states = outputs.hidden_states[-1]
            else:
                # ëŒ€ì•ˆ: logitsì—ì„œ íˆë“  ìŠ¤í…Œì´íŠ¸ ì¶”ì¶œ ì‹œë„
                raise AttributeError("Cannot find hidden states in model output")
        
        if attention_mask is not None:
            last_valid_indices = attention_mask.sum(dim=1) - 1
            last_valid_indices = torch.clamp(last_valid_indices, min=0)
            last_hidden = hidden_states[torch.arange(batch_size, device=self.qwen_device), last_valid_indices]
        else:
            last_hidden = hidden_states[:, -1, :]
        
        # Hidden statesë¥¼ Policy GPUë¡œ ì´ë™í•˜ê³  float16ìœ¼ë¡œ ë³€í™˜
        last_hidden = last_hidden.to(self.policy_device).half()
        
        # ì˜¤ì§ ì •ì±… ë¡œì§“ë§Œ ë°˜í™˜!
        policy_logits = self.policy_head(last_hidden)
        
        return policy_logits  # Values ì—†ìŒ!
    
    def get_action_and_log_prob(self, state: Dict):
        """ì•¡ì…˜ ì„ íƒê³¼ ë¡œê·¸ í™•ë¥  (Value ì—†ìŒ) - ì˜ì–´ í† í° í•„í„°ë§ ì ìš©"""
        input_ids = state['input_ids'].unsqueeze(0)
        attention_mask = state['attention_mask'].unsqueeze(0)
        
        policy_logits = self(input_ids, attention_mask)
        
        # ì˜ì–´ í† í° í•„í„°ë§ ì ìš©
        filtered_logits = self.english_filter.filter_logits(policy_logits)
        
        scaled_logits = filtered_logits / self.config.temperature
        scaled_logits = torch.clamp(scaled_logits, min=-10, max=10)
        
        # Top-k í•„í„°ë§
        if self.config.top_k > 0:
            top_k_logits, top_k_indices = torch.topk(scaled_logits, self.config.top_k, dim=-1)
            scaled_logits = torch.full_like(scaled_logits, float('-inf'))
            scaled_logits.scatter_(-1, top_k_indices, top_k_logits)
        
        token_probs = F.softmax(scaled_logits, dim=-1)
        
        # Top-p í•„í„°ë§
        if self.config.top_p < 1.0:
            sorted_probs, sorted_indices = torch.sort(token_probs, descending=True, dim=-1)
            cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
            
            sorted_indices_to_remove = cumulative_probs > self.config.top_p
            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
            sorted_indices_to_remove[..., 0] = 0
            
            indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
            token_probs = token_probs.masked_fill(indices_to_remove, 0.0)
            
            prob_sum = token_probs.sum(dim=-1, keepdim=True)
            token_probs = token_probs / (prob_sum + 1e-8)
        
    
        token_dist = torch.distributions.Categorical(token_probs)
        action = token_dist.sample()
        action_log_prob = token_dist.log_prob(action).half()  # float16ìœ¼ë¡œ ë³€í™˜
    
        
        # Value ì—†ìŒ! ì˜¤ì§ action, log_prob, logitsë§Œ ë°˜í™˜ (ëª¨ë‘ float16)
        return action.item(), action_log_prob, scaled_logits.squeeze(0).half()

class PureGRPOPromptEnvironment:
    """ìˆœìˆ˜ GRPOìš© í”„ë¡¬í”„íŠ¸ í™˜ê²½"""
    
    def __init__(self, qwen_model, reward_model, sd_pipeline, config: PureGRPOConfig):
        self.qwen_model = qwen_model
        self.reward_model = reward_model
        self.sd_pipeline = sd_pipeline
        self.config = config
        self.tokenizer = qwen_model.tokenizer
        self.vocab_size = len(self.tokenizer.get_vocab())
        
        # ìŠ¤í… ë¡œê±° ì´ˆê¸°í™”
        if config.enable_step_logging:
            self.step_logger = StepLogger(config.log_dir)
        else:
            self.step_logger = None
        
        # GPU ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.qwen_device = "cuda:0"  # QWEN (í† í°í™”)
        self.sd_device = "cuda:1"    # Stable Diffusion (ì´ë¯¸ì§€ ìƒì„±)
        self.reward_device = "cuda:2"  # CLIP Reward (ë¦¬ì›Œë“œ ê³„ì‚°)
        
        self.current_prompt = ""
        self.original_prompt = ""
        self.step_count = 0
        self.current_episode_dir = None
        
        logger.info(f"ìˆœìˆ˜ GRPO í™˜ê²½ ì´ˆê¸°í™” - Vocab: {self.vocab_size}")
        logger.info(f"GPU ë°°ì¹˜: QWEN={self.qwen_device}, SD={self.sd_device}, Reward={self.reward_device}")
        if self.step_logger:
            logger.info(f"ìƒì„¸ ìŠ¤í… ë¡œê¹… í™œì„±í™”: {config.log_dir}")
    
    def reset(self, user_prompt: str):
        """í™˜ê²½ ë¦¬ì…‹ - GPU 0ìœ¼ë¡œ í† í° ì´ë™ + ìƒˆ ì—í”¼ì†Œë“œ ì‹œì‘"""
        self.original_prompt = user_prompt
        self.current_prompt = user_prompt
        self.step_count = 0
        
        # ìƒˆ ì—í”¼ì†Œë“œ ì‹œì‘
        if self.step_logger:
            episode_id = user_prompt.replace(' ', '_')[:20]  # ê°„ë‹¨í•œ ID ìƒì„±
            self.current_episode_dir = self.step_logger.start_new_episode(episode_id, user_prompt)
            logger.info(f"ğŸ¬ ìƒˆ ì—í”¼ì†Œë“œ ì‹œì‘: {episode_id}")
        
        # í˜„ì¬ í”„ë¡¬í”„íŠ¸ë¥¼ í† í°í™”í•˜ê³  QWEN GPU(0ë²ˆ)ë¡œ ì´ë™
        tokens = self.tokenizer.encode(
            self.current_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=self.config.max_prompt_length,
            padding='max_length'
        ).to(self.qwen_device)
        
        attention_mask = (tokens != self.tokenizer.pad_token_id).long().to(self.qwen_device)
        
        return {
            'input_ids': tokens.squeeze(0),
            'attention_mask': attention_mask.squeeze(0),
            'current_prompt': self.current_prompt,
            'original_prompt': self.original_prompt
        }
    
    def step(self, action: int):
        """í™˜ê²½ ìŠ¤í… - GPU ê°„ ë°ì´í„° ì´ë™ ì²˜ë¦¬ + ìƒì„¸ ë¡œê¹…"""
        # ì•¡ì…˜(í† í°)ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        try:
            token_text = self.tokenizer.decode([action], skip_special_tokens=True)
            
            # í”„ë¡¬í”„íŠ¸ì— í† í° ì¶”ê°€
            if token_text.strip():
                if self.current_prompt.endswith(' ') or token_text.startswith(' '):
                    self.current_prompt += token_text
                else:
                    self.current_prompt += ' ' + token_text
            
            self.step_count += 1
            
            # ì¢…ë£Œ ì¡°ê±´
            done = (self.step_count >= self.config.max_new_tokens or 
                   action == self.tokenizer.eos_token_id or
                   len(self.current_prompt) >= self.config.max_prompt_length * 4)
            
            # ë¦¬ì›Œë“œ ê³„ì‚° (ì—í”¼ì†Œë“œ ëì—ë§Œ) - GPU ê°„ ì´ë™ ì²˜ë¦¬
            if done:
                try:
                    logger.info(f"ğŸ–¼ï¸  ì´ë¯¸ì§€ ìƒì„± ì‹œì‘ (GPU {self.sd_device})")
                    
                    # SD3 íŒŒì´í”„ë¼ì¸ì„ GPU 1ë¡œ ì´ë™í•˜ì—¬ ì´ë¯¸ì§€ ìƒì„±
                    with torch.cuda.device(1):
                        # ì›ë³¸ í”„ë¡¬í”„íŠ¸ë¡œ ì´ë¯¸ì§€ ìƒì„± (ë¹„êµìš©)
                        original_result = self.sd_pipeline(
                            prompt=self.original_prompt,
                            num_inference_steps=20,
                            guidance_scale=7.0,
                            height=1024,
                            width=1024
                        )
                        original_image = original_result.images[0]
                        
                        # í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ë¡œ ì´ë¯¸ì§€ ìƒì„±
                        enhanced_result = self.sd_pipeline(
                            prompt=self.current_prompt,
                            num_inference_steps=20,
                            guidance_scale=7.0,
                            height=1024,
                            width=1024
                        )
                        enhanced_image = enhanced_result.images[0]
                    
                    logger.info(f"ğŸ¯ ë¦¬ì›Œë“œ ê³„ì‚° ì‹œì‘ (GPU {self.reward_device})")
                    
                    # CLIP ë¦¬ì›Œë“œë¥¼ GPU 2ì—ì„œ ê³„ì‚°
                    with torch.cuda.device(2):
                        # ì›ë³¸ í”„ë¡¬í”„íŠ¸ vs ì›ë³¸ ì´ë¯¸ì§€
                        original_reward = self.reward_model.calculate_reward(
                            self.original_prompt,
                            self.original_prompt,
                            original_image
                        )
                        
                        # ì›ë³¸ í”„ë¡¬í”„íŠ¸ vs í–¥ìƒëœ ì´ë¯¸ì§€ (ì‹¤ì œ ë¦¬ì›Œë“œ)
                        enhanced_reward = self.reward_model.calculate_reward(
                            self.original_prompt,
                            self.current_prompt,
                            enhanced_image
                        )
                    
                    # ê¸¸ì´ ë³´ë„ˆìŠ¤
                    length_bonus = min(self.step_count / self.config.max_new_tokens, 1.0) * 0.1
                    total_reward = enhanced_reward + length_bonus
                    
                    logger.info(f"âœ… ë¦¬ì›Œë“œ ê³„ì‚° ì™„ë£Œ: {total_reward:.4f}")
                    
                    # ìƒì„¸ ë¡œê¹…
                    if self.step_logger:
                        # ì´ë¯¸ì§€ ì €ì¥
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        original_img_path = self.step_logger.save_image(
                            original_image, 
                            f"step_{self.step_count:03d}_{timestamp}_original.png"
                        )
                        enhanced_img_path = self.step_logger.save_image(
                            enhanced_image, 
                            f"step_{self.step_count:03d}_{timestamp}_enhanced.png"
                        )
                        
                        # ìŠ¤í… ì •ë³´ ë¡œê¹…
                        step_info = {
                            'step': self.step_count,
                            'original_prompt': self.original_prompt,
                            'enhanced_prompt': self.current_prompt,
                            'action_info': {
                                'token_id': action,
                                'token_text': token_text,
                                'log_prob': 0.0  # ë‚˜ì¤‘ì— ì—…ë°ì´íŠ¸ë¨
                            },
                            'reward_components': {
                                'original_reward': float(original_reward),
                                'enhanced_reward': float(enhanced_reward),
                                'length_bonus': float(length_bonus),
                                'final_reward': float(total_reward)
                            },
                            'images_saved': {
                                'original': original_img_path,
                                'enhanced': enhanced_img_path
                            }
                        }
                        
                        # ì „ì—­ ìŠ¤í… ë¡œê¹…
                        self.step_logger.log_step(step_info)
                        
                        # ì—í”¼ì†Œë“œë³„ ìŠ¤í… ë¡œê¹…
                        if self.current_episode_dir:
                            self.step_logger.log_episode_step(self.current_episode_dir, step_info)
                        
                        # ì—í”¼ì†Œë“œ ì™„ë£Œ ì²˜ë¦¬
                        if done and self.current_episode_dir:
                            self.step_logger.finish_episode(
                                self.current_episode_dir, 
                                float(total_reward), 
                                self.step_count
                            )
                            # HTML ë³´ê³ ì„œ ìƒì„±
                            self.step_logger.create_comparison_html()
                    
                except Exception as e:
                    logger.warning(f"Reward calculation failed: {e}")
                    total_reward = 0.0
                    original_image = None
                    enhanced_image = None
            else:
                total_reward = 0.0
                original_image = None
                enhanced_image = None
            
            # ë‹¤ìŒ ìƒíƒœ (GPU 0ìœ¼ë¡œ ì´ë™)
            if not done:
                next_tokens = self.tokenizer.encode(
                    self.current_prompt,
                    return_tensors="pt",
                    truncation=True,
                    max_length=self.config.max_prompt_length,
                    padding='max_length'
                ).to(self.qwen_device)
                
                next_attention_mask = (next_tokens != self.tokenizer.pad_token_id).long().to(self.qwen_device)
                
                # ë‹¤ìŒ ìƒíƒœë¥¼ QWEN GPU(0ë²ˆ)ë¡œ ì´ë™
                next_state = {
                    'input_ids': next_tokens.squeeze(0),
                    'attention_mask': next_attention_mask.squeeze(0),
                    'current_prompt': self.current_prompt,
                    'original_prompt': self.original_prompt
                }
            else:
                next_state = None
            
            info = {
                'current_prompt': self.current_prompt,
                'step_count': self.step_count,
                'token_added': token_text,
                'original_image': original_image,
                'enhanced_image': enhanced_image
            }
            
            return next_state, total_reward, done, info
            
        except Exception as e:
            logger.warning(f"Step failed: {e}")
            return None, 0.0, True, {'error': str(e)}

class PureGRPOTrainer:
    """ìˆœìˆ˜ GRPO íŠ¸ë ˆì´ë„ˆ (Value Network ì—†ìŒ)"""
    
    def __init__(self, qwen_model, reward_model, sd_pipeline, config: PureGRPOConfig):
        self.config = config
        self.qwen_model = qwen_model
        self.reward_model = reward_model
        self.sd_pipeline = sd_pipeline
        
        self.env = PureGRPOPromptEnvironment(qwen_model, reward_model, sd_pipeline, config)
        
        # ì˜¤ì§ ì •ì±… ë„¤íŠ¸ì›Œí¬ë§Œ! (Value Network ì—†ìŒ)
        self.policy = PureGRPOPolicy(qwen_model, config)
        
        # ì°¸ì¡° ì •ì±… (float16ìœ¼ë¡œ í†µì¼)
        self.ref_policy = PureGRPOPolicy(qwen_model, config)
        self.ref_policy.load_state_dict(self.policy.state_dict())
        self.ref_policy.eval()
        self.ref_policy.half()  # float16ìœ¼ë¡œ ë³€í™˜
        
        # ì˜¤ì§ ì •ì±… íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµ
        trainable_params = list(self.policy.policy_head.parameters())
        self.optimizer = optim.AdamW(trainable_params, lr=config.learning_rate, weight_decay=0.01)
        
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=100, eta_min=1e-7)
        
        logger.info("ğŸ¯ ìˆœìˆ˜ GRPO íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"âœ… Value Network ì—†ìŒ - ì˜¤ì§ Policy Networkë§Œ ì‚¬ìš©")
        logger.info(f"ğŸ“Š Trainable parameters: {sum(p.numel() for p in trainable_params):,}")
    
    def collect_rollouts(self, prompts: List[str]) -> List[Dict]:
        """ë¡¤ì•„ì›ƒ ìˆ˜ì§‘ (Value ìˆ˜ì§‘ ì—†ìŒ)"""
        all_experiences = []
        
        for prompt_idx, user_prompt in enumerate(prompts):
            logger.info(f"Processing prompt {prompt_idx+1}/{len(prompts)}: '{user_prompt}'")
            
            for rollout_idx in range(self.config.num_rollouts):
                episode_experiences = []
                state = self.env.reset(user_prompt)
                done = False
                
                logger.info(f"  Rollout {rollout_idx+1}/{self.config.num_rollouts}")
                
                step_count = 0
                while not done and step_count < self.config.max_new_tokens:
                    # ì •ì±…ì—ì„œ ì•¡ì…˜ ì„ íƒ (Value ì—†ìŒ!)
                    action, log_prob, logits = self.policy.get_action_and_log_prob(state)
                    
                    # ì°¸ì¡° ì •ì±…ì˜ ë¡œê·¸ í™•ë¥ 
                    with torch.no_grad():
                        ref_logits = self.ref_policy(
                            state['input_ids'].unsqueeze(0),
                            state['attention_mask'].unsqueeze(0)
                        )
                        ref_log_prob = F.log_softmax(ref_logits, dim=-1)[0, action]
                    
                    next_state, reward, done, info = self.env.step(action)
                    
                    # ìŠ¤í… ë¡œê±°ì— log_prob ì—…ë°ì´íŠ¸ (ì—í”¼ì†Œë“œ ëì—ì„œ)
                    if done and self.env.step_logger and len(self.env.step_logger.step_data) > 0:
                        last_step_info = self.env.step_logger.step_data[-1]
                        if 'action_info' in last_step_info:
                            last_step_info['action_info']['log_prob'] = float(log_prob)
                            # JSON íŒŒì¼ ë‹¤ì‹œ ì €ì¥
                            self.env.step_logger._save_to_json()
                    
                    # Value ì—†ëŠ” ê²½í—˜ ì €ì¥!
                    experience = {
                        'state': {k: v.clone() if torch.is_tensor(v) else v for k, v in state.items()},
                        'action': action,
                        'log_prob': log_prob,
                        'ref_log_prob': ref_log_prob,
                        'reward': reward,
                        'done': done,
                        'prompt_idx': prompt_idx,
                        'rollout_idx': rollout_idx,
                        'info': info
                    }
                    
                    episode_experiences.append(experience)
                    state = next_state
                    step_count += 1
                
                all_experiences.extend(episode_experiences)
                
                if episode_experiences:
                    final_prompt = episode_experiences[-1]['info']['current_prompt']
                    final_reward = episode_experiences[-1]['reward']
                    logger.info(f"    Generated: '{final_prompt}' (reward: {final_reward:.3f})")
        
        return all_experiences
    
    def compute_grpo_advantages(self, experiences: List[Dict]) -> List[Dict]:
        """ìˆœìˆ˜ GRPO Advantage ê³„ì‚° (easyr1ê³¼ ë™ì¼)"""
        # í”„ë¡¬í”„íŠ¸ë³„ ë¦¬ì›Œë“œ ê·¸ë£¹í™”
        prompt_rewards = defaultdict(list)
        for exp in experiences:
            if exp['done']:
                prompt_rewards[exp['prompt_idx']].append(exp['reward'])
        
        # ê·¸ë£¹ë³„ ì •ê·œí™” (easyr1ê³¼ ë™ì¼í•œ ë°©ì‹)
        advantages = {}
        for prompt_idx, rewards in prompt_rewards.items():
            if len(rewards) > 1:
                mean_reward = np.mean(rewards)
                std_reward = np.std(rewards) + 1e-8
                normalized_rewards = [(r - mean_reward) / std_reward for r in rewards]
            else:
                normalized_rewards = [0.0]
            
            advantages[prompt_idx] = normalized_rewards
            logger.info(f"Prompt {prompt_idx} GRPO: rewards={rewards} -> advantages={normalized_rewards}")
        
        # ê²½í—˜ì— advantage í• ë‹¹
        rollout_counters = defaultdict(int)
        for exp in experiences:
            if exp['done']:
                prompt_idx = exp['prompt_idx']
                rollout_idx = rollout_counters[prompt_idx]
                if prompt_idx in advantages and rollout_idx < len(advantages[prompt_idx]):
                    exp['advantage'] = advantages[prompt_idx][rollout_idx]
                else:
                    exp['advantage'] = 0.0
                rollout_counters[prompt_idx] += 1
            else:
                exp['advantage'] = 0.0
        
        return experiences
    
    def train_step(self, experiences: List[Dict]) -> Dict:
        """ìˆœìˆ˜ GRPO í•™ìŠµ ìŠ¤í… (Value Loss ì—†ìŒ)"""
        if not experiences:
            return {}
        
        valid_experiences = [exp for exp in experiences if exp.get('advantage', 0) != 0]
        if not valid_experiences:
            logger.warning("No valid experiences for training")
            return {}
        
        batch_states = []
        actions = []
        old_log_probs = []
        ref_log_probs = []
        advantages = []
        
        for exp in valid_experiences:
            batch_states.append(exp['state'])
            actions.append(exp['action'])
            old_log_probs.append(exp['log_prob'])
            ref_log_probs.append(exp['ref_log_prob'])
            advantages.append(exp['advantage'])
            # âŒ values.append(exp['value'])  # Value ì—†ìŒ!
        
        if len(batch_states) == 0:
            return {}
        
        # íŒ¨ë”©ì„ ìœ„í•œ ìµœëŒ€ ê¸¸ì´ ì°¾ê¸°
        max_length = max(state['input_ids'].size(0) for state in batch_states)
        
        # íŒ¨ë”©ëœ í…ì„œ ìƒì„± ë° GPU 0ìœ¼ë¡œ ì´ë™
        padded_input_ids = []
        padded_attention_masks = []
        
        for state in batch_states:
            input_ids_tensor = state['input_ids']
            attention_mask_tensor = state['attention_mask']
            
            # íŒ¨ë”© í•„ìš”í•œ ê¸¸ì´ ê³„ì‚°
            pad_length = max_length - input_ids_tensor.size(0)
            
            if pad_length > 0:
                # íŒ¨ë”© ì¶”ê°€ (ì˜¤ë¥¸ìª½ì— íŒ¨ë”©) - GPU 0ì—ì„œ, dtype ë³´ì¡´
                padded_input = torch.cat([
                    input_ids_tensor.to("cuda:0"),
                    torch.zeros(pad_length, dtype=input_ids_tensor.dtype, device="cuda:0")
                ])
                padded_mask = torch.cat([
                    attention_mask_tensor.to("cuda:0"),
                    torch.zeros(pad_length, dtype=attention_mask_tensor.dtype, device="cuda:0")
                ])
            else:
                padded_input = input_ids_tensor.to("cuda:0")
                padded_mask = attention_mask_tensor.to("cuda:0")
            
            padded_input_ids.append(padded_input)
            padded_attention_masks.append(padded_mask)
        
        # ëª¨ë“  í…ì„œë¥¼ GPU 0ìœ¼ë¡œ ì´ë™í•˜ê³  ì ì ˆí•œ dtype ì„¤ì •
        input_ids = torch.stack(padded_input_ids).to("cuda:0")  # int íƒ€ì… ìœ ì§€
        attention_masks = torch.stack(padded_attention_masks).to("cuda:0")  # int íƒ€ì… ìœ ì§€
        actions = torch.tensor(actions).to("cuda:0")  # int íƒ€ì… ìœ ì§€
        old_log_probs = torch.stack(old_log_probs).to("cuda:0").half()  # float16
        ref_log_probs = torch.stack(ref_log_probs).to("cuda:0").half()  # float16
        advantages = torch.tensor(advantages, dtype=torch.float16).to("cuda:0")  # float16
        
        # ì˜¤ì§ ì •ì±… ë¡œì§“ë§Œ ê³„ì‚°! (Values ì—†ìŒ)
        policy_logits = self.policy(input_ids, attention_masks)
        
        new_log_probs = F.log_softmax(policy_logits, dim=-1)
        new_log_probs = new_log_probs.gather(1, actions.unsqueeze(1)).squeeze(1)
        
        # PPO ì •ì±… ì†ì‹¤
        ratio = torch.exp(new_log_probs - old_log_probs)
        surr1 = ratio * advantages
        surr2 = torch.clamp(ratio, 1 - self.config.clip_ratio, 1 + self.config.clip_ratio) * advantages
        policy_loss = -torch.min(surr1, surr2).mean()
        
        # KL í˜ë„í‹°
        kl_penalty = (new_log_probs - ref_log_probs).mean()
        
        # ì—”íŠ¸ë¡œí”¼
        entropy = -(F.softmax(policy_logits, dim=-1) * F.log_softmax(policy_logits, dim=-1)).sum(-1).mean()
        
        # ìˆœìˆ˜ GRPO ì´ ì†ì‹¤ (Value Loss ì—†ìŒ!)
        total_loss = (policy_loss + 
                     self.config.kl_coef * kl_penalty - 
                     self.config.entropy_coef * entropy)
        
        # ì—­ì „íŒŒ
        total_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 0.5)
        self.optimizer.step()
        self.scheduler.step()
        self.optimizer.zero_grad()
        
        return {
            'total_loss': total_loss.item(),
            'policy_loss': policy_loss.item(),
            'kl_penalty': kl_penalty.item(),
            'entropy': entropy.item(),
            'avg_advantage': advantages.mean().item(),
            'learning_rate': self.scheduler.get_last_lr()[0],
            'num_valid_experiences': len(valid_experiences)
        }
    
    def train(self, train_prompts: List[str], num_epochs: int = 10):
        """ìˆœìˆ˜ GRPO í•™ìŠµ"""
        logger.info(f"ğŸš€ ìˆœìˆ˜ GRPO í•™ìŠµ ì‹œì‘ (Value Network ì—†ìŒ)")
        logger.info(f"í”„ë¡¬í”„íŠ¸: {len(train_prompts)}ê°œ, ì—í¬í¬: {num_epochs}ê°œ")
        logger.info(f"Action Space: {self.env.vocab_size}ê°œ í† í° (ì „ì²´ ì–´íœ˜)")
        logger.info(f"âœ… easyr1ê³¼ ë™ì¼í•œ êµ¬ì¡°: ê·¸ë£¹ í‰ê· ì„ implicit baselineìœ¼ë¡œ ì‚¬ìš©")
        
        for epoch in range(num_epochs):
            logger.info(f"\nEpoch {epoch + 1}/{num_epochs}")
            logger.info("-" * 50)
            
            experiences = self.collect_rollouts(train_prompts)
            experiences = self.compute_grpo_advantages(experiences)
            metrics = self.train_step(experiences)
            
            logger.info(f"Epoch {epoch + 1} metrics:")
            for key, value in metrics.items():
                logger.info(f"  {key}: {value:.6f}")
            
            if epoch % 2 == 0:
                self._log_sample_outputs(train_prompts[:2])
    
    def _log_sample_outputs(self, sample_prompts: List[str]):
        """ìƒ˜í”Œ ì¶œë ¥ ë¡œê¹…"""
        logger.info("ğŸ“ Sample outputs:")
        for prompt in sample_prompts:
            state = self.env.reset(prompt)
            original_prompt = self.env.current_prompt
            
            # ëª‡ ìŠ¤í… ì‹¤í–‰
            for _ in range(5):
                action, _, _ = self.policy.get_action_and_log_prob(state)
                state, _, done, info = self.env.step(action)
                if done:
                    break
            
            enhanced_prompt = self.env.current_prompt
            logger.info(f"  Original: {original_prompt}")
            logger.info(f"  Enhanced: {enhanced_prompt}")

def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    logging.basicConfig(level=logging.INFO)
    
    class MockQwenModel:
        def __init__(self):
            from transformers import AutoTokenizer
            self.tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2-VL-7B-Instruct")
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            class MockModel:
                def __init__(self):
                    class Config:
                        hidden_size = 4096
                    self.config = Config()
                
                def __call__(self, **kwargs):
                    batch_size, seq_len = kwargs['input_ids'].shape
                    class Output:
                        last_hidden_state = torch.randn(batch_size, seq_len, 4096)
                    return Output()
            
            self.model = MockModel()
    
    class MockReward:
        def calculate_reward(self, original, enhanced, image):
            return np.random.uniform(5.0, 9.0)
    
    class MockSD:
        def __call__(self, **kwargs):
            from PIL import Image
            class Result:
                images = [Image.new('RGB', (1024, 1024), color='red')]
            return Result()
    
    config = PureGRPOConfig(
        learning_rate=1e-6,
        batch_size=2,
        num_rollouts=3,
        max_new_tokens=10,
        top_k=50
    )
    
    qwen = MockQwenModel()
    reward = MockReward()
    sd = MockSD()
    
    trainer = PureGRPOTrainer(qwen, reward, sd, config)
    
    test_prompts = ["a cat sitting", "beautiful sunset"]
    trainer.train(test_prompts, num_epochs=2)

if __name__ == "__main__":
    main() 