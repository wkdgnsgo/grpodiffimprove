#!/usr/bin/env python3
"""
QWEN í†µí•© GRPO íŠ¸ë ˆì´ë„ˆ
QWEN ëª¨ë¸ì˜ enhance_prompt ê¸°ëŠ¥ê³¼ GRPOë¥¼ í†µí•©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ ê°œì„ 
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import logging
from typing import Dict, List, Tuple, Optional
from collections import defaultdict
from dataclasses import dataclass
import math
import os
from datetime import datetime
import matplotlib.pyplot as plt
import matplotlib.style as mplstyle
from qwen import QWENModel, QWENGRPOConfig

logger = logging.getLogger(__name__)

class QWENGRPOEnvironment:
    """QWEN GRPO í†µí•© í™˜ê²½"""
    
    def __init__(self, qwen_model: QWENModel, reward_model, sd_pipeline, config: QWENGRPOConfig):
        self.qwen_model = qwen_model
        self.reward_model = reward_model
        self.sd_pipeline = sd_pipeline
        self.config = config
        
        # GPU ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.qwen_device = "cuda:0"    # QWEN (í”„ë¡¬í”„íŠ¸ í–¥ìƒ)
        self.sd_device = "cuda:1"      # Stable Diffusion (ì´ë¯¸ì§€ ìƒì„±)
        self.reward_device = "cuda:2"  # CLIP Reward (ë¦¬ì›Œë“œ ê³„ì‚°)
        
        self.current_user_prompt = ""
        self.current_enhanced_prompt = ""
        self.current_candidates = []
        self.episode_count = 0
        
        # ë¡œê¹… ë””ë ‰í† ë¦¬ ì„¤ì •
        if config.save_images:
            self.base_log_dir = config.log_dir
            os.makedirs(self.base_log_dir, exist_ok=True)
            logger.info(f"ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬: {self.base_log_dir}")
    
    def reset(self, user_prompt: str) -> Dict:
        """í™˜ê²½ ë¦¬ì…‹"""
        self.current_user_prompt = user_prompt
        self.current_enhanced_prompt = ""
        self.current_candidates = []
        self.episode_count += 1
        
        # ì—í”¼ì†Œë“œ ë””ë ‰í† ë¦¬ ìƒì„±
        if self.config.save_images:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            safe_prompt = "".join(c for c in user_prompt if c.isalnum() or c in (' ', '-', '_')).rstrip()
            safe_prompt = safe_prompt[:30] + "..." if len(safe_prompt) > 30 else safe_prompt
            
            self.episode_dir = os.path.join(
                self.base_log_dir,
                f"episode_{self.episode_count:03d}_{timestamp}_{safe_prompt.replace(' ', '_')}"
            )
            os.makedirs(self.episode_dir, exist_ok=True)
        
        logger.info(f"ğŸ”„ í™˜ê²½ ë¦¬ì…‹: '{user_prompt}'")
        
        # ì´ˆê¸° ìƒíƒœ ë°˜í™˜ (ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸)
        return {
            'user_prompt': self.current_user_prompt,
            'enhanced_prompt': '',
            'candidates': [],
            'episode': self.episode_count
        }
    
    def step(self, action: int) -> Tuple[Dict, float, bool, Dict]:
        """í™˜ê²½ ìŠ¤í… - QWEN í›„ë³´ ì¤‘ì—ì„œ ì„ íƒ"""
        original_image = None
        enhanced_image = None
        original_reward = 0.0
        enhanced_reward = 0.0
        total_reward = 0.0
        candidates = []
        
        try:
            # QWENì—ì„œ í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ í›„ë³´ë“¤ ìƒì„±
            logger.info(f"ğŸ§  QWEN í›„ë³´ ìƒì„± ì¤‘... (GPU {self.qwen_device})")
            with torch.cuda.device(0):
                candidates = self.qwen_model.generate_enhancement_candidates(self.current_user_prompt)
            
            self.current_candidates = candidates
            
            # ì•¡ì…˜ì— í•´ë‹¹í•˜ëŠ” í›„ë³´ ì„ íƒ
            if 0 <= action < len(candidates):
                selected_prompt = candidates[action]
            else:
                logger.warning(f"Invalid action {action}, using first candidate")
                selected_prompt = candidates[0] if candidates else self.current_user_prompt
            
            self.current_enhanced_prompt = selected_prompt
            
            logger.info(f"âœ… ì„ íƒëœ í”„ë¡¬í”„íŠ¸: '{selected_prompt[:50]}...'")
            
            # ì´ë¯¸ì§€ ìƒì„± ì‹œë„
            try:
                logger.info(f"ğŸ–¼ï¸  ì´ë¯¸ì§€ ìƒì„± ì‹œì‘ (GPU {self.sd_device})")
                
                with torch.cuda.device(1):
                    # ì›ë³¸ í”„ë¡¬í”„íŠ¸ë¡œ ì´ë¯¸ì§€ ìƒì„±
                    original_result = self.sd_pipeline(
                        prompt=self.current_user_prompt,
                        num_inference_steps=28,
                        guidance_scale=7.0,
                        height=1024,
                        width=1024
                    )
                    original_image = original_result.images[0]
                    
                    # í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ë¡œ ì´ë¯¸ì§€ ìƒì„±
                    enhanced_result = self.sd_pipeline(
                        prompt=self.current_enhanced_prompt,
                        num_inference_steps=28,
                        guidance_scale=7.0,
                        height=1024,
                        width=1024
                    )
                    enhanced_image = enhanced_result.images[0]
                
                logger.info(f"âœ… ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ")
                
            except Exception as img_error:
                logger.error(f"âŒ ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {img_error}")
                # ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„± (ê²€ì€ ì´ë¯¸ì§€)
                from PIL import Image
                original_image = Image.new('RGB', (1024, 1024), color='black')
                enhanced_image = Image.new('RGB', (1024, 1024), color='black')
            
            # ë¦¬ì›Œë“œ ê³„ì‚° ì‹œë„
            try:
                logger.info(f"ğŸ¯ ë¦¬ì›Œë“œ ê³„ì‚° ì‹œì‘ (GPU {self.reward_device})")
                
                # CLIP ë¦¬ì›Œë“œë¥¼ GPU 2ì—ì„œ ê³„ì‚°
                with torch.cuda.device(2):
                    enhanced_reward = self.reward_model.calculate_reward(
                        self.current_user_prompt,
                        self.current_enhanced_prompt,
                        enhanced_image
                    )
                    
                    # ì›ë³¸ í”„ë¡¬í”„íŠ¸ vs ì›ë³¸ ì´ë¯¸ì§€ (ì°¸ê³ ìš©)
                    original_reward = self.reward_model.calculate_reward(
                        self.current_user_prompt,
                        self.current_user_prompt,
                        original_image
                    )
                
                # ë¦¬ì›Œë“œ ê³„ì‚°
                total_reward = enhanced_reward
                
                logger.info(f"âœ… ë¦¬ì›Œë“œ ê³„ì‚° ì™„ë£Œ: {total_reward:.4f}")
                
            except Exception as reward_error:
                logger.error(f"âŒ ë¦¬ì›Œë“œ ê³„ì‚° ì‹¤íŒ¨: {reward_error}")
                # ê¸°ë³¸ ë¦¬ì›Œë“œ ê°’ ì‚¬ìš©
                original_reward = 0.1
                enhanced_reward = 0.1
                total_reward = 0.1
            
            # ë‹¤ìŒ ìƒíƒœ (ì—í”¼ì†Œë“œ ì™„ë£Œ)
            next_state = {
                'user_prompt': self.current_user_prompt,
                'enhanced_prompt': self.current_enhanced_prompt,
                'candidates': self.current_candidates,
                'episode': self.episode_count
            }
            
            info = {
                'original_prompt': self.current_user_prompt,
                'enhanced_prompt': self.current_enhanced_prompt,
                'candidates': self.current_candidates,
                'selected_action': action,
                'original_reward': original_reward,
                'enhanced_reward': enhanced_reward
            }
            
            # í•œ ìŠ¤í…ìœ¼ë¡œ ì™„ë£Œ (done=True)
            return next_state, total_reward, True, info
            
        except Exception as e:
            logger.error(f"âŒ ì „ì²´ ìŠ¤í… ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            
            # ì—ëŸ¬ê°€ ë°œìƒí•´ë„ ê¸°ë³¸ê°’ìœ¼ë¡œ ìƒíƒœ ë°˜í™˜
            if not candidates:
                candidates = [self.current_user_prompt]
            
            next_state = {
                'user_prompt': self.current_user_prompt,
                'enhanced_prompt': self.current_user_prompt,
                'candidates': candidates,
                'episode': self.episode_count
            }
            
            info = {
                'original_prompt': self.current_user_prompt,
                'enhanced_prompt': self.current_user_prompt,
                'candidates': candidates,
                'selected_action': 0,
                'original_reward': 0.0,
                'enhanced_reward': 0.0,
                'error': str(e)
            }
            
            return next_state, 0.0, True, info
        
        finally:
            # ì—ëŸ¬ ë°œìƒ ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´ í•­ìƒ ì´ë¯¸ì§€ ì €ì¥ ì‹œë„
            if self.config.save_images and original_image is not None and enhanced_image is not None:
                try:
                    self._save_episode_results(
                        original_image, enhanced_image, 
                        original_reward, enhanced_reward, total_reward,
                        action, candidates
                    )
                except Exception as save_error:
                    logger.warning(f"âš ï¸ ì´ë¯¸ì§€ ì €ì¥ ì‹¤íŒ¨: {save_error}")
                    # ì—ëŸ¬ ì •ë³´ë¼ë„ ì €ì¥
                    self._save_error_log(action, candidates, str(save_error))
    
    def _save_episode_results(self, original_image, enhanced_image, original_reward, enhanced_reward, total_reward, action, candidates):
        """ì—í”¼ì†Œë“œ ê²°ê³¼ ì €ì¥"""
        try:
            # ì´ë¯¸ì§€ ì €ì¥
            original_image.save(os.path.join(self.episode_dir, "original_image.png"))
            enhanced_image.save(os.path.join(self.episode_dir, "enhanced_image.png"))
            
            # í›„ë³´ë“¤ ì •ë³´
            candidates_info = "\n".join([f"  {i}: {cand}" for i, cand in enumerate(candidates)])
            
            # ë¡œê·¸ íŒŒì¼ ì‘ì„±
            log_content = f"""=== QWEN GRPO Episode Results ===
Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Episode: {self.episode_count}

=== Prompts ===
Original Prompt: {self.current_user_prompt}
Enhanced Prompt: {self.current_enhanced_prompt}

=== GRPO Action ===
Selected Action: {action}
Available Candidates ({len(candidates)}):
{candidates_info}

=== Reward Components ===
Original Reward (Originalâ†’Original): {original_reward:.4f}
Enhanced Reward (Originalâ†’Enhanced): {enhanced_reward:.4f}
Total Reward: {total_reward:.4f}

=== Improvement ===
Reward Improvement: {enhanced_reward - original_reward:.4f}
Relative Improvement: {((enhanced_reward - original_reward) / max(original_reward, 0.001) * 100):.2f}%

=== Files ===
Original Image: original_image.png
Enhanced Image: enhanced_image.png
"""
            
            # ë¡œê·¸ íŒŒì¼ ì €ì¥
            with open(os.path.join(self.episode_dir, "episode_log.txt"), "w", encoding="utf-8") as f:
                f.write(log_content)
            
            logger.info(f"ğŸ’¾ ì—í”¼ì†Œë“œ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {self.episode_dir}")
            
        except Exception as e:
            logger.warning(f"Failed to save episode results: {e}")
    
    def _save_error_log(self, action: int, candidates: List[str], error_msg: str):
        """ì—ëŸ¬ ë°œìƒ ì‹œ ë¡œê·¸ë§Œ ì €ì¥"""
        try:
            # ì—ëŸ¬ ë¡œê·¸ íŒŒì¼ ì‘ì„±
            error_log_content = f"""=== QWEN GRPO Error Log ===
Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Episode: {self.episode_count}

=== Error Info ===
Error Message: {error_msg}
Original Prompt: {self.current_user_prompt}
Selected Action: {action}

=== Available Candidates ===
{chr(10).join([f"  {i}: {cand}" for i, cand in enumerate(candidates)])}

=== Note ===
Images could not be generated/saved due to the error above.
"""
            
            # ì—ëŸ¬ ë¡œê·¸ íŒŒì¼ ì €ì¥
            error_log_path = os.path.join(self.episode_dir, "error_log.txt")
            with open(error_log_path, "w", encoding="utf-8") as f:
                f.write(error_log_content)
            
            logger.info(f"ğŸ“ ì—ëŸ¬ ë¡œê·¸ ì €ì¥ ì™„ë£Œ: {error_log_path}")
            
        except Exception as log_error:
            logger.error(f"âŒ ì—ëŸ¬ ë¡œê·¸ ì €ì¥ë„ ì‹¤íŒ¨: {log_error}")

class QWENGRPOTrainer:
    """QWEN í†µí•© GRPO íŠ¸ë ˆì´ë„ˆ"""
    
    def __init__(self, qwen_model: QWENModel, reward_model, sd_pipeline, config: QWENGRPOConfig):
        self.config = config
        self.qwen_model = qwen_model
        self.reward_model = reward_model
        self.sd_pipeline = sd_pipeline
        
        # QWEN ëª¨ë¸ì— GRPO ì»´í¬ë„ŒíŠ¸ê°€ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        if not hasattr(qwen_model, 'grpo_policy_head'):
            raise ValueError("QWEN ëª¨ë¸ì— GRPO ì»´í¬ë„ŒíŠ¸ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. grpo_configë¥¼ ì „ë‹¬í•˜ì—¬ ì´ˆê¸°í™”í•˜ì„¸ìš”.")
        
        self.env = QWENGRPOEnvironment(qwen_model, reward_model, sd_pipeline, config)
        
        # ë¦¬ì›Œë“œ ì¶”ì ì„ ìœ„í•œ ë³€ìˆ˜ë“¤
        self.episode_rewards = []
        self.episode_numbers = []
        self.running_avg_rewards = []
        
        # í”Œë¡¯ ì €ì¥ì„ ìœ„í•œ ë””ë ‰í† ë¦¬ ì„¤ì • (Environmentì™€ ë™ì¼í•œ ë””ë ‰í† ë¦¬ ì‚¬ìš©)
        self.plot_save_dir = self.env.base_log_dir if config.save_images else config.log_dir
        os.makedirs(self.plot_save_dir, exist_ok=True)
        
        # í”Œë¡¯ ì„¤ì •
        mplstyle.use('fast')
        plt.ion()  # Interactive mode on
        
        logger.info("ğŸ¯ QWEN GRPO íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"âœ… Action Space: {config.num_enhancement_candidates} enhancement candidates")
        logger.info(f"ğŸ“Š í”Œë¡¯ ì €ì¥ ë””ë ‰í† ë¦¬: {self.plot_save_dir}")
    
    def collect_rollouts(self, prompts: List[str], is_baseline: bool = False) -> List[Dict]:
        """ë¡¤ì•„ì›ƒ ìˆ˜ì§‘"""
        all_experiences = []
        rollout_type = "ë² ì´ìŠ¤ë¼ì¸" if is_baseline else "í•™ìŠµìš©"
        
        for prompt_idx, user_prompt in enumerate(prompts):
            logger.info(f"\nğŸ“ {rollout_type} í”„ë¡¬í”„íŠ¸ {prompt_idx + 1}/{len(prompts)}: '{user_prompt}'")
            
            # í”„ë¡¬í”„íŠ¸ë³„ ë¡¤ì•„ì›ƒ ìˆ˜ì§‘
            for rollout_idx in range(self.config.num_rollouts):
                logger.info(f"  ğŸ² {rollout_type} ë¡¤ì•„ì›ƒ {rollout_idx + 1}/{self.config.num_rollouts}")
                
                try:
                    # í™˜ê²½ ë¦¬ì…‹ (ë² ì´ìŠ¤ë¼ì¸ì¼ ë•ŒëŠ” ì´ë¯¸ì§€ ì €ì¥ ì•ˆí•¨)
                    if is_baseline:
                        # ë² ì´ìŠ¤ë¼ì¸ ìˆ˜ì§‘ ì‹œì—ëŠ” ì´ë¯¸ì§€ ì €ì¥ ë¹„í™œì„±í™”
                        original_save_setting = self.config.save_images
                        self.config.save_images = False
                    
                    state = self.env.reset(user_prompt)
                    
                    # QWEN GRPOë¡œ ì•¡ì…˜ ì„ íƒ
                    with torch.cuda.device(0):
                        action, log_prob, candidates = self.qwen_model.get_grpo_action_and_log_prob(user_prompt)
                    
                    logger.info(f"    ğŸ¯ ì„ íƒëœ ì•¡ì…˜: {action} (ë¡œê·¸ í™•ë¥ : {log_prob:.4f})")
                    
                    # í™˜ê²½ ìŠ¤í… ì‹¤í–‰
                    next_state, reward, done, info = self.env.step(action)
                    
                    # ë² ì´ìŠ¤ë¼ì¸ì¼ ë•ŒëŠ” ì„¤ì • ë³µì›
                    if is_baseline:
                        self.config.save_images = original_save_setting
                    
                    if next_state is not None:
                        # ê²½í—˜ ì €ì¥
                        experience = {
                            'user_prompt': user_prompt,
                            'action': action,
                            'log_prob': log_prob,
                            'reward': reward,
                            'candidates': candidates,
                            'info': info,
                            'is_baseline': is_baseline
                        }
                        
                        all_experiences.append(experience)
                        logger.info(f"    âœ… {rollout_type} ë¦¬ì›Œë“œ: {reward:.4f}")
                    else:
                        logger.warning(f"    âŒ {rollout_type} ë¡¤ì•„ì›ƒ ì‹¤íŒ¨")
                
                except Exception as e:
                    logger.error(f"    âŒ {rollout_type} ë¡¤ì•„ì›ƒ ì˜¤ë¥˜: {e}")
                    # ë² ì´ìŠ¤ë¼ì¸ì¼ ë•Œ ì„¤ì • ë³µì› (ì—ëŸ¬ ìƒí™©ì—ì„œë„)
                    if is_baseline and 'original_save_setting' in locals():
                        self.config.save_images = original_save_setting
                    continue
        
        logger.info(f"\nğŸ“Š ìˆ˜ì§‘ëœ {rollout_type} ê²½í—˜: {len(all_experiences)}ê°œ")
        return all_experiences
    
    def compute_grpo_advantages(self, experiences: List[Dict]) -> List[Dict]:
        """GRPO ì–´ë“œë°´í‹°ì§€ ê³„ì‚° (ê·¸ë£¹ í‰ê·  baseline)"""
        if not experiences:
            return experiences
        
        # í”„ë¡¬í”„íŠ¸ë³„ë¡œ ê·¸ë£¹í™”
        prompt_groups = defaultdict(list)
        for exp in experiences:
            prompt_groups[exp['user_prompt']].append(exp)
        
        # ê·¸ë£¹ë³„ ì–´ë“œë°´í‹°ì§€ ê³„ì‚°
        for prompt, group_exps in prompt_groups.items():
            rewards = [exp['reward'] for exp in group_exps]
            group_baseline = np.mean(rewards)
            
            # ê° ê²½í—˜ì— ì–´ë“œë°´í‹°ì§€ ì¶”ê°€
            for exp in group_exps:
                exp['advantage'] = exp['reward'] - group_baseline
                exp['baseline'] = group_baseline
        
        return experiences
    
    def train_step(self, experiences: List[Dict]) -> Dict:
        """GRPO í•™ìŠµ ìŠ¤í…"""
        if not experiences:
            return {}
        
        logger.info(f"ğŸ¯ GRPO í•™ìŠµ ìŠ¤í… ì‹œì‘ (ê²½í—˜: {len(experiences)}ê°œ)")
        
        # ì–´ë“œë°´í‹°ì§€ ê³„ì‚°
        experiences = self.compute_grpo_advantages(experiences)
        
        # QWEN ëª¨ë¸ì˜ GRPO ì—…ë°ì´íŠ¸ í˜¸ì¶œ
        with torch.cuda.device(0):
            metrics = self.qwen_model.update_grpo_policy(experiences)
        
        logger.info(f"âœ… GRPO ì—…ë°ì´íŠ¸ ì™„ë£Œ")
        logger.info(f"  Policy Loss: {metrics.get('policy_loss', 0):.4f}")
        logger.info(f"  KL Div: {metrics.get('kl_div', 0):.4f}")
        logger.info(f"  Entropy: {metrics.get('entropy', 0):.4f}")
        logger.info(f"  Mean Reward: {metrics.get('mean_reward', 0):.4f}")
        
        return metrics
    
    def collect_baseline_data(self, train_prompts: List[str], num_baseline_episodes: int = 3):
        """ë² ì´ìŠ¤ë¼ì¸ ë°ì´í„° ìˆ˜ì§‘ (í•™ìŠµì— ì‚¬ìš©ë˜ì§€ ì•ŠìŒ)"""
        logger.info(f"ğŸ“Š ë² ì´ìŠ¤ë¼ì¸ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ ({num_baseline_episodes} ì—í”¼ì†Œë“œ)")
        logger.info("=" * 80)
        
        baseline_experiences = []
        
        for episode in range(num_baseline_episodes):
            logger.info(f"\nğŸ“‹ ë² ì´ìŠ¤ë¼ì¸ ì—í”¼ì†Œë“œ {episode + 1}/{num_baseline_episodes}")
            logger.info("-" * 60)
            
            # ë² ì´ìŠ¤ë¼ì¸ìš© ë¡¤ì•„ì›ƒ ìˆ˜ì§‘ (ì €ì¥í•˜ì§€ ì•ŠìŒ)
            episode_experiences = self.collect_rollouts(train_prompts, is_baseline=True)
            baseline_experiences.extend(episode_experiences)
            
            if episode_experiences:
                episode_rewards = [exp['reward'] for exp in episode_experiences]
                avg_reward = np.mean(episode_rewards)
                logger.info(f"ğŸ“ˆ ë² ì´ìŠ¤ë¼ì¸ ì—í”¼ì†Œë“œ {episode + 1} í‰ê·  ë¦¬ì›Œë“œ: {avg_reward:.4f}")
        
        # ë² ì´ìŠ¤ë¼ì¸ í†µê³„ ê³„ì‚°
        if baseline_experiences:
            baseline_rewards = [exp['reward'] for exp in baseline_experiences]
            baseline_mean = np.mean(baseline_rewards)
            baseline_std = np.std(baseline_rewards)
            
            logger.info("\nğŸ“Š ë² ì´ìŠ¤ë¼ì¸ í†µê³„:")
            logger.info(f"  í‰ê·  ë¦¬ì›Œë“œ: {baseline_mean:.4f}")
            logger.info(f"  í‘œì¤€í¸ì°¨: {baseline_std:.4f}")
            logger.info(f"  ìµœëŒ€ ë¦¬ì›Œë“œ: {np.max(baseline_rewards):.4f}")
            logger.info(f"  ìµœì†Œ ë¦¬ì›Œë“œ: {np.min(baseline_rewards):.4f}")
            logger.info(f"  ì´ ê²½í—˜ ìˆ˜: {len(baseline_experiences)}ê°œ")
        
        # í™˜ê²½ ì—í”¼ì†Œë“œ ì¹´ìš´í„° ë¦¬ì…‹ (ì‹¤ì œ í•™ìŠµì€ 1ë¶€í„° ì‹œì‘)
        self.env.episode_count = 0
        logger.info("\nğŸ”„ í™˜ê²½ ì—í”¼ì†Œë“œ ì¹´ìš´í„° ë¦¬ì…‹ ì™„ë£Œ")
        logger.info("âœ… ë² ì´ìŠ¤ë¼ì¸ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
        
        return baseline_experiences

    def train(self, train_prompts: List[str], num_epochs: int = 10, num_baseline_episodes: int = 3):
        """GRPO í•™ìŠµ ì‹¤í–‰"""
        logger.info(f"ğŸš€ QWEN GRPO í•™ìŠµ ì‹œì‘")
        logger.info("=" * 80)
        
        # 1ë‹¨ê³„: ë² ì´ìŠ¤ë¼ì¸ ë°ì´í„° ìˆ˜ì§‘ (í•™ìŠµì— ì‚¬ìš©ë˜ì§€ ì•ŠìŒ)
        baseline_data = self.collect_baseline_data(train_prompts, num_baseline_episodes)
        
        logger.info(f"\nğŸ¯ ì‹¤ì œ GRPO í•™ìŠµ ì‹œì‘ (ì—í¬í¬: {num_epochs})")
        logger.info("=" * 80)
        
        all_metrics = []
        
        for epoch in range(num_epochs):
            logger.info(f"\nğŸ”„ í•™ìŠµ ì—í¬í¬ {epoch + 1}/{num_epochs}")
            logger.info("-" * 60)
            
            # í•™ìŠµìš© ë¡¤ì•„ì›ƒ ìˆ˜ì§‘
            experiences = self.collect_rollouts(train_prompts, is_baseline=False)
            
            if not experiences:
                logger.warning("âš ï¸ ìˆ˜ì§‘ëœ ê²½í—˜ì´ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìŒ ì—í¬í¬ë¡œ ê±´ë„ˆëœë‹ˆë‹¤.")
                continue
            
            # í•™ìŠµ ìŠ¤í…
            metrics = self.train_step(experiences)
            metrics['epoch'] = epoch + 1
            all_metrics.append(metrics)
            
            # ì—í”¼ì†Œë“œ í‰ê·  ë¦¬ì›Œë“œ ê³„ì‚° ë° í”Œë¡¯ ì—…ë°ì´íŠ¸
            epoch_rewards = [exp['reward'] for exp in experiences]
            avg_reward = np.mean(epoch_rewards)
            self._update_reward_plot(epoch + 1, avg_reward)
            
            # ì£¼ê¸°ì ìœ¼ë¡œ ìƒ˜í”Œ ì¶œë ¥ í™•ì¸ (ë‹¤ì–‘í•œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©)
            if (epoch + 1) % 3 == 0:
                logger.info(f"\nğŸ“‹ ì—í¬í¬ {epoch + 1} ìƒ˜í”Œ ì¶œë ¥:")
                # ë§¤ë²ˆ ë‹¤ë¥¸ í”„ë¡¬í”„íŠ¸ ì„ íƒ (ìˆœí™˜)
                sample_indices = [(epoch * 2) % len(train_prompts), ((epoch * 2) + 1) % len(train_prompts)]
                sample_prompts = [train_prompts[i] for i in sample_indices]
                self._log_sample_outputs(sample_prompts)
        
        # ìµœì¢… í”Œë¡¯ ì €ì¥
        self._save_reward_plot()
        logger.info("\nâœ… QWEN GRPO í•™ìŠµ ì™„ë£Œ!")
        return all_metrics, baseline_data
    
    def _log_sample_outputs(self, sample_prompts: List[str]):
        """ìƒ˜í”Œ ì¶œë ¥ ë¡œê¹…"""
        for prompt in sample_prompts:
            try:
                with torch.cuda.device(0):
                    # ê¸°ë³¸ í–¥ìƒ
                    basic_result = self.qwen_model.enhance_prompt(prompt)
                    
                    # GRPO ê¸°ë°˜ ì„ íƒ
                    action, log_prob, candidates = self.qwen_model.get_grpo_action_and_log_prob(prompt)
                    grpo_enhanced = candidates[action] if 0 <= action < len(candidates) else candidates[0]
                
                logger.info(f"  ì›ë³¸: '{prompt}'")
                logger.info(f"  ê¸°ë³¸: '{basic_result['enhanced_prompt'][:60]}...'")
                logger.info(f"  GRPO: '{grpo_enhanced[:60]}...' (ì•¡ì…˜: {action})")
                
            except Exception as e:
                logger.warning(f"  ìƒ˜í”Œ ì¶œë ¥ ì‹¤íŒ¨: {e}")
    
    def _update_reward_plot(self, epoch: int, avg_reward: float):
        """ì‹¤ì‹œê°„ ë¦¬ì›Œë“œ í”Œë¡¯ ì—…ë°ì´íŠ¸"""
        self.episode_numbers.append(epoch)
        self.episode_rewards.append(avg_reward)
        
        # ì´ë™ í‰ê·  ê³„ì‚° (ìœˆë„ìš° í¬ê¸°: 5)
        window_size = min(5, len(self.episode_rewards))
        if len(self.episode_rewards) >= window_size:
            running_avg = np.mean(self.episode_rewards[-window_size:])
            self.running_avg_rewards.append(running_avg)
        else:
            self.running_avg_rewards.append(avg_reward)
        
        # í”Œë¡¯ ì—…ë°ì´íŠ¸
        try:
            plt.clf()  # Clear figure
            
            # ë©”ì¸ ë¦¬ì›Œë“œ í”Œë¡¯
            plt.plot(self.episode_numbers, self.episode_rewards, 'b-', alpha=0.6, label='Episode Reward')
            plt.plot(self.episode_numbers, self.running_avg_rewards, 'r-', linewidth=2, label='Moving Average (5)')
            
            plt.title('QWEN GRPO Training Progress', fontsize=14, fontweight='bold')
            plt.xlabel('Epoch', fontsize=12)
            plt.ylabel('Average Reward', fontsize=12)
            plt.legend()
            plt.grid(True, alpha=0.3)
            
            # Yì¶• ë²”ìœ„ ìë™ ì¡°ì •
            if len(self.episode_rewards) > 1:
                y_min = min(self.episode_rewards) * 0.95
                y_max = max(self.episode_rewards) * 1.05
                plt.ylim(y_min, y_max)
            
            # í˜„ì¬ ì—í¬í¬ ì •ë³´ í‘œì‹œ
            plt.text(0.02, 0.98, f'Current Epoch: {epoch}\nCurrent Reward: {avg_reward:.4f}\nMoving Avg: {self.running_avg_rewards[-1]:.4f}', 
                    transform=plt.gca().transAxes, fontsize=10, verticalalignment='top',
                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
            
            plt.tight_layout()
            plt.pause(0.01)  # ì§§ì€ pauseë¡œ í”Œë¡¯ ì—…ë°ì´íŠ¸
            
        except Exception as e:
            logger.warning(f"âš ï¸ í”Œë¡¯ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def _save_reward_plot(self):
        """ìµœì¢… ë¦¬ì›Œë“œ í”Œë¡¯ ì €ì¥"""
        try:
            if not self.episode_numbers:
                logger.warning("âš ï¸ ì €ì¥í•  ë¦¬ì›Œë“œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            # ìµœì¢… í”Œë¡¯ ìƒì„±
            plt.figure(figsize=(12, 8))
            
            # ì„œë¸Œí”Œë¡¯ 1: ë¦¬ì›Œë“œ ì¶”ì´
            plt.subplot(2, 1, 1)
            plt.plot(self.episode_numbers, self.episode_rewards, 'b-', alpha=0.6, marker='o', markersize=4, label='Episode Reward')
            plt.plot(self.episode_numbers, self.running_avg_rewards, 'r-', linewidth=3, label='Moving Average (5)')
            plt.title('QWEN GRPO Training Progress - Reward Trend', fontsize=14, fontweight='bold')
            plt.xlabel('Epoch')
            plt.ylabel('Average Reward')
            plt.legend()
            plt.grid(True, alpha=0.3)
            
            # ì„œë¸Œí”Œë¡¯ 2: ë¦¬ì›Œë“œ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
            plt.subplot(2, 1, 2)
            plt.hist(self.episode_rewards, bins=min(20, len(self.episode_rewards)), alpha=0.7, color='skyblue', edgecolor='black')
            plt.title('Reward Distribution', fontsize=14, fontweight='bold')
            plt.xlabel('Reward Value')
            plt.ylabel('Frequency')
            plt.grid(True, alpha=0.3)
            
            # í†µê³„ ì •ë³´ ì¶”ê°€
            mean_reward = np.mean(self.episode_rewards)
            std_reward = np.std(self.episode_rewards)
            max_reward = np.max(self.episode_rewards)
            min_reward = np.min(self.episode_rewards)
            
            stats_text = f'Statistics:\nMean: {mean_reward:.4f}\nStd: {std_reward:.4f}\nMax: {max_reward:.4f}\nMin: {min_reward:.4f}'
            plt.text(0.02, 0.98, stats_text, transform=plt.gca().transAxes, fontsize=10, 
                    verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))
            
            plt.tight_layout()
            
            # ì €ì¥
            if self.config.save_images:
                plot_path = os.path.join(self.plot_save_dir, 'training_progress.png')
                plt.savefig(plot_path, dpi=300, bbox_inches='tight')
                logger.info(f"ğŸ“Š í•™ìŠµ ì§„í–‰ í”Œë¡¯ ì €ì¥: {plot_path}")
                
                # ë°ì´í„°ë„ CSVë¡œ ì €ì¥
                import csv
                csv_path = os.path.join(self.plot_save_dir, 'training_rewards.csv')
                with open(csv_path, 'w', newline='', encoding='utf-8') as f:
                    writer = csv.writer(f)
                    writer.writerow(['Epoch', 'Average_Reward', 'Moving_Average'])
                    for i in range(len(self.episode_numbers)):
                        writer.writerow([self.episode_numbers[i], self.episode_rewards[i], self.running_avg_rewards[i]])
                logger.info(f"ğŸ“Š í•™ìŠµ ë°ì´í„° ì €ì¥: {csv_path}")
            
            plt.show()
            
        except Exception as e:
            logger.error(f"âŒ ìµœì¢… í”Œë¡¯ ì €ì¥ ì‹¤íŒ¨: {e}")

# ... existing code ...

 