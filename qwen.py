import torch
import torch.nn as nn
import torch.nn.functional as F
import logging
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor, GenerationConfig
import re
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class QWENGRPOConfig:
    """QWEN GRPO í†µí•© ì„¤ì •"""
    learning_rate: float = 1e-6
    batch_size: int = 4
    num_rollouts: int = 5
    max_prompt_length: int = 77
    max_new_tokens: int = 30
    temperature: float = 1.0
    top_p: float = 0.9
    top_k: int = 100
    kl_coef: float = 0.02
    clip_ratio: float = 0.1
    entropy_coef: float = 0.02
    num_enhancement_candidates: int = 20  # ìƒì„±í•  í›„ë³´ ê°œìˆ˜
    save_images: bool = True
    log_dir: str = "grpo_results"
    # Semantic filtering ì„¤ì • (ì œê±°ë¨ - GRPOê°€ ì§ì ‘ í•™ìŠµí•˜ë„ë¡)
    use_semantic_filtering: bool = False  # í›„ë³´ ìƒì„±ì‹œ semantic filtering ì‚¬ìš©
    semantic_threshold: float = 0.7  # Semantic similarity threshold

class QWENModel:

    def __init__(self, model_name = "Qwen/Qwen2-VL-7B-Instruct", device = "cuda", temperature = 0.7, grpo_config: QWENGRPOConfig = None):
        self.model_name = model_name
        self.device = device
        self.temperature = temperature
        self.grpo_config = grpo_config or QWENGRPOConfig()

        self._load_model()
        self._setup_prompt_template()
        
        # GRPO ê´€ë ¨ ì´ˆê¸°í™”
        if grpo_config:
            self._setup_grpo_components()
        
        logger.info(f"Qwen init : {self.model_name}")

    def _load_model(self):
        
        self.processor = AutoProcessor.from_pretrained(
                self.model_name,
                trust_remote_code=True
            )
        self.tokenizer = self.processor.tokenizer
  
        model_kwargs = {
            'torch_dtype': torch.float16,
            'trust_remote_code': True,
            'low_cpu_mem_usage': True
        }

        self.model = Qwen2VLForConditionalGeneration.from_pretrained(
                self.model_name,
                **model_kwargs
        ).to(self.device)

        if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
        # ìƒì„± ì„¤ì •
        self.generation_config = GenerationConfig(
            max_new_tokens=77,
            temperature=self.temperature,
            top_p=0.9,
            do_sample=True,
            pad_token_id=self.tokenizer.pad_token_id,
            eos_token_id=self.tokenizer.eos_token_id
        )
        
        logger.info("Model loaded")
        
    def _setup_prompt_template(self):
         
        # ìƒì„± ì§€ì‹œë¥¼ ìœ„í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        self.system_prompt = """You are an expert at enhancing image generation prompts. 
        Given a simple user prompt, expand it into a detailed, and high-quality prompt for image generation.

        Guidelines:
        - Keep the original concept unchanged
        - Only using English words and sentences
        - Do not use any other languages
        - Do not use any special characters
        - Add artistic style, mood, and atmosphere
        - Include technical specifications (lighting, composition, resolution)
        - Add creative details that make the image more realistic
        - Make each enhancement unique and varied
        - Be descriptive but concise (aim for 20-40 additional words) """

        # ì‚¬ìš©ì ì…ë ¥ í…œí”Œë¦¿
        self.user_template = """Original prompt: {user_prompt}

        Enhanced version:"""

    def _setup_grpo_components(self):
        """GRPO ê´€ë ¨ ì»´í¬ë„ŒíŠ¸ ì„¤ì • - QWEN ì§ì ‘ í•™ìŠµ"""
        self.hidden_size = self.model.config.hidden_size
        self.vocab_size = len(self.tokenizer.get_vocab())
        
        # ì°¸ì¡° ëª¨ë¸ (frozen copy of QWEN for KL penalty)
        from copy import deepcopy
        self.ref_model = deepcopy(self.model)
        self.ref_model.eval()
        
        # ì°¸ì¡° ëª¨ë¸ì˜ ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ freeze
        for param in self.ref_model.parameters():
            param.requires_grad = False
        
        # ì˜µí‹°ë§ˆì´ì € (QWEN ëª¨ë¸ë§Œ í•™ìŠµ)
        self.grpo_optimizer = torch.optim.AdamW(
            self.model.parameters(), 
            lr=self.grpo_config.learning_rate,
            weight_decay=0.01
        )
        
        logger.info("âœ… GRPO ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"ğŸ“Š QWEN ëª¨ë¸ ì§ì ‘ í•™ìŠµ ë°©ì‹ìœ¼ë¡œ ì„¤ì •")

    def _init_grpo_weights(self):
        """GRPO ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” (QWEN ì§ì ‘ í•™ìŠµ ë°©ì‹ì—ì„œëŠ” ë¶ˆí•„ìš”)"""
        pass

    def enhance_prompt(self, user_prompt):
        """ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ í–¥ìƒ (GRPO ì—†ì´)"""
        # VLMì— ì…ë ¥í•  ë©”ì‹œì§€ êµ¬ì„±
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": self.user_template.format(user_prompt=user_prompt)}
        ]
        
        # í…œí”Œë¦¿ ì ìš©
        prompt = self.tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        # í† í¬ë‚˜ì´ì§•
        inputs = self.tokenizer(
            prompt, 
            return_tensors="pt", 
            padding=True, 
            truncation=True
        ).to(self.device)
        
        # ìƒì„±
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                generation_config=self.generation_config,
                pad_token_id=self.tokenizer.pad_token_id
            )
        
        # ë””ì½”ë”©
        generated_text = self.tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[1]:], 
            skip_special_tokens=True
        ).strip()
        
        # í›„ì²˜ë¦¬
        enhanced_prompt = self._post_process_output(generated_text)
        
        result = {
            'original_prompt': user_prompt,
            'enhanced_prompt': enhanced_prompt,
            'raw_output': generated_text
        }
        
        logger.info(f"Enhanced prompt: '{user_prompt}' -> '{enhanced_prompt[:50]}...'")
        return result

    def generate_enhancement_candidates(self, user_prompt: str, num_candidates: int = None, 
                                        use_semantic_filtering: bool = True, 
                                        semantic_threshold: float = 0.7) -> List[str]:
        """ì—¬ëŸ¬ ê°œì˜ í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ í›„ë³´ ìƒì„± (semantic filtering í¬í•¨)"""
        if num_candidates is None:
            num_candidates = self.grpo_config.num_enhancement_candidates
        
        candidates = []
        
        # VLMì— ì…ë ¥í•  ë©”ì‹œì§€ êµ¬ì„±
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": self.user_template.format(user_prompt=user_prompt)}
        ]
        
        # í…œí”Œë¦¿ ì ìš©
        prompt = self.tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        # í† í¬ë‚˜ì´ì§•
        inputs = self.tokenizer(
            prompt, 
            return_tensors="pt", 
            padding=True, 
            truncation=True
        ).to(self.device)
        
        # Semantic filtering ì œê±° - GRPOê°€ ì§ì ‘ explorationê³¼ exploitation ê· í˜•ì„ í•™ìŠµ
        # ë‹¤ì–‘í•œ í›„ë³´ ìƒì„±ì„ ìœ„í•´ ë” ë§ì€ ì‹œë„
        total_generations = num_candidates * 2
        
        raw_candidates = []
        
        # ì—¬ëŸ¬ í›„ë³´ ìƒì„±
        for i in range(total_generations):
            # ë‹¤ì–‘ì„±ì„ ìœ„í•´ temperatureì™€ top_p ì¡°ì •
            temp_config = GenerationConfig(
                max_new_tokens=77,
                temperature=self.temperature + (i * 0.05),  # ë” ì„¸ë°€í•œ ì¡°ì •
                top_p=0.85 + (i % 3) * 0.05,  # 0.85, 0.9, 0.95 ìˆœí™˜
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    generation_config=temp_config,
                    pad_token_id=self.tokenizer.pad_token_id
                )
            
            # ë””ì½”ë”©
            generated_text = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:], 
                skip_special_tokens=True
            ).strip()
            
            # í›„ì²˜ë¦¬
            enhanced_prompt = self._post_process_output(generated_text)
            
            # ì¤‘ë³µ ì œê±°
            if enhanced_prompt not in raw_candidates and enhanced_prompt != user_prompt:
                raw_candidates.append(enhanced_prompt)
        
        # Semantic filtering ì œê±° - ë‹¤ì–‘í•œ í›„ë³´ë“¤ì„ GRPOê°€ ì§ì ‘ í‰ê°€í•˜ë„ë¡
        # ì¤‘ë³µ ì œê±°í•˜ê³  ë‹¤ì–‘ì„± í™•ë³´ë¥¼ ìœ„í•´ ì…”í”Œ
        import random
        random.shuffle(raw_candidates)
        candidates = raw_candidates[:num_candidates]
        
        logger.info(f"ğŸ¯ ë‹¤ì–‘í•œ í›„ë³´ ìƒì„±: {len(raw_candidates)}ê°œ â†’ {num_candidates}ê°œ ì„ íƒ (semantic filtering ì—†ìŒ)")
        
        # í›„ë³´ê°€ ë¶€ì¡±í•˜ë©´ ì›ë³¸ í”„ë¡¬í”„íŠ¸ë¡œ ì±„ì›€
        while len(candidates) < num_candidates:
            candidates.append(user_prompt)
            logger.warning(f"âš ï¸ í›„ë³´ ë¶€ì¡±ìœ¼ë¡œ ì›ë³¸ í”„ë¡¬í”„íŠ¸ ì¶”ê°€: {len(candidates)}/{num_candidates}")
        
        return candidates[:num_candidates]

    def get_grpo_state_representation(self, user_prompt: str) -> torch.Tensor:
        """GRPOë¥¼ ìœ„í•œ ìƒíƒœ í‘œí˜„ ìƒì„± (QWEN ëª¨ë¸ë„ í•™ìŠµ ê°€ëŠ¥)"""
        # VLMì— ì…ë ¥í•  ë©”ì‹œì§€ êµ¬ì„±
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": self.user_template.format(user_prompt=user_prompt)}
        ]
        
        # í…œí”Œë¦¿ ì ìš©
        prompt = self.tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        # í† í¬ë‚˜ì´ì§•
        inputs = self.tokenizer(
            prompt, 
            return_tensors="pt", 
            padding=True, 
            truncation=True
        ).to(self.device)
        
        # íˆë“  ìŠ¤í…Œì´íŠ¸ ì¶”ì¶œ (QWEN ëª¨ë¸ë„ í•™ìŠµë˜ë„ë¡ gradient ê³„ì‚°)
        outputs = self.model(
            input_ids=inputs['input_ids'],
            attention_mask=inputs['attention_mask'],
            output_hidden_states=True
        )
        
        if hasattr(outputs, 'last_hidden_state') and outputs.last_hidden_state is not None:
            hidden_states = outputs.last_hidden_state
        elif hasattr(outputs, 'hidden_states') and outputs.hidden_states is not None:
            hidden_states = outputs.hidden_states[-1]
        else:
            raise AttributeError("Cannot find hidden states in model output")
        
        # ë§ˆì§€ë§‰ í† í°ì˜ íˆë“  ìŠ¤í…Œì´íŠ¸ ì‚¬ìš©
        if inputs['attention_mask'] is not None:
            last_valid_indices = inputs['attention_mask'].sum(dim=1) - 1
            last_valid_indices = torch.clamp(last_valid_indices, min=0)
            state_repr = hidden_states[torch.arange(hidden_states.size(0)), last_valid_indices]
        else:
            state_repr = hidden_states[:, -1, :]
        
        return state_repr.squeeze(0)  # [hidden_size]

    def generate_grpo_enhanced_prompt(self, user_prompt: str) -> Tuple[str, torch.Tensor]:
        """GRPOë¥¼ í†µí•´ í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ ìƒì„± (QWEN ì§ì ‘ í•™ìŠµ)"""
        # VLMì— ì…ë ¥í•  ë©”ì‹œì§€ êµ¬ì„±
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": self.user_template.format(user_prompt=user_prompt)}
        ]
        
        # í…œí”Œë¦¿ ì ìš©
        prompt = self.tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        # í† í¬ë‚˜ì´ì§•
        inputs = self.tokenizer(
            prompt, 
            return_tensors="pt", 
            padding=True, 
            truncation=True
        ).to(self.device)
        
        # QWEN ëª¨ë¸ë¡œ ì§ì ‘ ìƒì„± (gradient ê³„ì‚°)
        outputs = self.model.generate(
            **inputs,
            generation_config=self.generation_config,
            pad_token_id=self.tokenizer.pad_token_id,
            return_dict_in_generate=True,
            output_scores=True
        )
        
        # ìƒì„±ëœ í…ìŠ¤íŠ¸ ë””ì½”ë”©
        generated_text = self.tokenizer.decode(
            outputs.sequences[0][inputs['input_ids'].shape[1]:], 
            skip_special_tokens=True
        ).strip()
        
        # í›„ì²˜ë¦¬
        enhanced_prompt = self._post_process_output(generated_text)
        
        # ë¡œê·¸ í™•ë¥  ê³„ì‚° (ìƒì„±ëœ í† í°ë“¤ì˜ í‰ê·  ë¡œê·¸ í™•ë¥ )
        if hasattr(outputs, 'scores') and outputs.scores:
            log_probs = []
            for i, score in enumerate(outputs.scores):
                token_id = outputs.sequences[0][inputs['input_ids'].shape[1] + i]
                log_prob = F.log_softmax(score, dim=-1)[0, token_id]
                log_probs.append(log_prob)
            
            avg_log_prob = torch.stack(log_probs).mean()
        else:
            # fallback: ë”ë¯¸ ë¡œê·¸ í™•ë¥ 
            avg_log_prob = torch.tensor(0.0, device=self.device)
        
        return enhanced_prompt, avg_log_prob

    def get_ref_model_log_prob(self, user_prompt: str, enhanced_prompt: str) -> torch.Tensor:
        """ì°¸ì¡° ëª¨ë¸ì˜ ë¡œê·¸ í™•ë¥  ê³„ì‚°"""
        # VLMì— ì…ë ¥í•  ë©”ì‹œì§€ êµ¬ì„±
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": self.user_template.format(user_prompt=user_prompt)}
        ]
        
        # í…œí”Œë¦¿ ì ìš©
        prompt = self.tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        # ì „ì²´ ì‹œí€€ìŠ¤ (í”„ë¡¬í”„íŠ¸ + ìƒì„±ëœ í…ìŠ¤íŠ¸)
        full_text = prompt + enhanced_prompt
        
        # í† í¬ë‚˜ì´ì§•
        inputs = self.tokenizer(
            full_text, 
            return_tensors="pt", 
            padding=True, 
            truncation=True
        ).to(self.device)
        
        # ì°¸ì¡° ëª¨ë¸ë¡œ ë¡œê·¸ í™•ë¥  ê³„ì‚°
        with torch.no_grad():
            outputs = self.ref_model(
                input_ids=inputs['input_ids'],
                attention_mask=inputs['attention_mask']
            )
            
            # ìƒì„±ëœ ë¶€ë¶„ì˜ ë¡œê·¸ í™•ë¥ ë§Œ ê³„ì‚°
            prompt_length = len(self.tokenizer.encode(prompt))
            generated_logits = outputs.logits[0, prompt_length-1:-1]  # ìƒì„±ëœ ë¶€ë¶„ë§Œ
            generated_tokens = inputs['input_ids'][0, prompt_length:]
            
            log_probs = F.log_softmax(generated_logits, dim=-1)
            token_log_probs = log_probs.gather(1, generated_tokens.unsqueeze(1)).squeeze(1)
            
            return token_log_probs.mean()

    def update_grpo_policy(self, experiences: List[Dict]) -> Dict:
        """GRPO ì •ì±… ì—…ë°ì´íŠ¸ (QWEN ì§ì ‘ í•™ìŠµ)"""
        if not experiences:
            return {}
        
        # ê²½í—˜ ë°ì´í„° ì¤€ë¹„
        user_prompts = []
        enhanced_prompts = []
        old_log_probs = []
        rewards = []
        
        for exp in experiences:
            user_prompts.append(exp['user_prompt'])
            enhanced_prompts.append(exp['enhanced_prompt'])
            old_log_probs.append(exp['log_prob'])
            rewards.append(exp['reward'])
        
        # í…ì„œë¡œ ë³€í™˜
        old_log_probs = torch.stack(old_log_probs).half()
        rewards = torch.tensor(rewards, device=self.device, dtype=torch.float16)
        
        # ê·¸ë£¹ í‰ê· ì„ baselineìœ¼ë¡œ ì‚¬ìš© (GRPO ë°©ì‹)
        baseline = rewards.mean()
        advantages = rewards - baseline
        
        # í˜„ì¬ ëª¨ë¸ê³¼ ì°¸ì¡° ëª¨ë¸ì˜ ë¡œê·¸ í™•ë¥  ê³„ì‚°
        current_log_probs = []
        ref_log_probs = []
        
        for user_prompt, enhanced_prompt in zip(user_prompts, enhanced_prompts):
            # í˜„ì¬ ëª¨ë¸ì˜ ë¡œê·¸ í™•ë¥  (gradient ê³„ì‚°)
            _, current_log_prob = self.generate_grpo_enhanced_prompt(user_prompt)
            current_log_probs.append(current_log_prob)
            
            # ì°¸ì¡° ëª¨ë¸ì˜ ë¡œê·¸ í™•ë¥ 
            ref_log_prob = self.get_ref_model_log_prob(user_prompt, enhanced_prompt)
            ref_log_probs.append(ref_log_prob)
        
        current_log_probs = torch.stack(current_log_probs).half()
        ref_log_probs = torch.stack(ref_log_probs).half()
        
        # ì¤‘ìš”ë„ ë¹„ìœ¨ ê³„ì‚°
        ratio = torch.exp(current_log_probs - old_log_probs)
        
        # PPO í´ë¦½ëœ ëª©ì  í•¨ìˆ˜
        clipped_ratio = torch.clamp(ratio, 1 - self.grpo_config.clip_ratio, 1 + self.grpo_config.clip_ratio)
        policy_loss = -torch.min(ratio * advantages, clipped_ratio * advantages).mean()
        
        # KL ë°œì‚° í˜ë„í‹°
        kl_div = (current_log_probs - ref_log_probs).mean()
        kl_penalty = self.grpo_config.kl_coef * kl_div
        
        # ì´ ì†ì‹¤ (ì—”íŠ¸ë¡œí”¼ëŠ” QWEN ìì²´ì—ì„œ ì œê³µ)
        total_loss = policy_loss + kl_penalty
        
        # ì—­ì „íŒŒ
        self.grpo_optimizer.zero_grad()
        total_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
        self.grpo_optimizer.step()
        
        # ë©”íŠ¸ë¦­ ë°˜í™˜
        return {
            'policy_loss': policy_loss.item(),
            'kl_div': kl_div.item(),
            'total_loss': total_loss.item(),
            'mean_reward': rewards.mean().item(),
            'baseline': baseline.item(),
            'mean_advantage': advantages.mean().item()
        }

    def _post_process_output(self, raw_output):
        """ìƒì„±ëœ ì¶œë ¥ í›„ì²˜ë¦¬"""
        # ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±°
        enhanced = raw_output.strip()
        
        # "Enhanced prompt:" ë“±ì˜ ë ˆì´ë¸” ì œê±°
        enhanced = re.sub(r'^(Enhanced prompt:|Prompt:|Result:)\s*', '', enhanced, flags=re.IGNORECASE)
        enhanced = enhanced.strip()
        
        # ë”°ì˜´í‘œ ì œê±°
        enhanced = enhanced.strip('"\'')
        
        return enhanced
    
    def enhance_prompts_batch(self, user_prompts):
        """ë°°ì¹˜ë¡œ ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ í–¥ìƒ"""
        results = []
        for prompt in user_prompts:
            result = self.enhance_prompt(prompt)
            results.append(result)
        return results
