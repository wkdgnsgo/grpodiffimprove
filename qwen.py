import torch
import torch.nn as nn
import torch.nn.functional as F
import logging
import copy
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor, GenerationConfig
from peft import LoraConfig, get_peft_model, TaskType
import re
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class QWENGRPOConfig:
    """QWEN GRPO í†µí•© ì„¤ì • (CartPole GRPO í˜¸í™˜ + EasyR1 ì•ˆì •ì„±)"""
    learning_rate: float = 2e-4  # LoRAëŠ” ë” ë†’ì€ í•™ìŠµë¥  ì‚¬ìš© ê°€ëŠ¥
    batch_size: int = 2  # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ë°°ì¹˜ í¬ê¸° ê°ì†Œ (8 â†’ 2)
    num_rollouts: int = 2  # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ë¡¤ì•„ì›ƒ ìˆ˜ ê°ì†Œ (6 â†’ 2)
    max_prompt_length: int = 77
    max_new_tokens: int = 20
    temperature: float = 1.0
    top_p: float = 0.9
    top_k: int = 100
    kl_coef: float = 0.02
    clip_ratio: float = 0.1
    entropy_coef: float = 0.02
    save_images: bool = True
    log_dir: str = "grpo_results"
    
    # CartPole GRPO í˜¸í™˜ ì¶”ê°€ ì„¤ì •
    gamma: float = 0.995  # í• ì¸ íŒ©í„° (VLMì€ CartPoleë³´ë‹¤ ë†’ê²Œ)
    grpo_epochs: int = 10  # ë‹¤ì¤‘ ì—í¬í¬ ì—…ë°ì´íŠ¸
    update_ref_model_freq: int = 1  # Reference ëª¨ë¸ ì—…ë°ì´íŠ¸ ë¹ˆë„
    epsilon_std: float = 1e-8  # ì •ê·œí™” ì•ˆì •ì„±
    
    # EasyR1 ìŠ¤íƒ€ì¼ ìˆ˜ì¹˜ì  ì•ˆì •ì„± ì„¤ì •
    use_adaptive_grad_clip: bool = True  # ì ì‘ì  ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
    grad_clip_ema_beta: float = 0.99  # ê·¸ë˜ë””ì–¸íŠ¸ norm EMA ê³„ìˆ˜
    grad_clip_coef: float = 1.5  # ì ì‘ì  í´ë¦¬í•‘ ê³„ìˆ˜
    use_grad_centralization: bool = True  # ê·¸ë˜ë””ì–¸íŠ¸ ì¤‘ì•™í™”
    use_grad_normalization: bool = True  # ê·¸ë˜ë””ì–¸íŠ¸ ì •ê·œí™”
    grad_norm_alpha: float = 0.5  # ì •ê·œí™” ê°•ë„
    use_stochastic_rounding: bool = True  # í™•ë¥ ì  ë°˜ì˜¬ë¦¼ (ì‹œë®¬ë ˆì´ì…˜)
    logits_clip_range: float = 20.0  # logits í´ë¦¬í•‘ ë²”ìœ„ (ë” ë³´ìˆ˜ì ìœ¼ë¡œ)
    stable_log_prob_min: float = -50.0  # ì•ˆì „í•œ ë¡œê·¸ í™•ë¥  ìµœì†Œê°’

class QWENModel:

    def __init__(self, model_name = "Qwen/Qwen2-VL-7B-Instruct", device = "cuda", temperature = 0.7, grpo_config: QWENGRPOConfig = None):
        self.model_name = model_name
        self.device = device
        self.temperature = temperature
        self.grpo_config = grpo_config or QWENGRPOConfig()  # Accelerate ë©”ì¸ í”„ë¡œì„¸ìŠ¤ ì—¬ë¶€

        self._load_model()
        self._setup_prompt_template()
        
        # GRPO ê´€ë ¨ ì´ˆê¸°í™”
        if grpo_config:
            self._setup_grpo_components()
        
        logger.info(f"Qwen init : {self.model_name}")

    def _load_model(self):
        
        self.processor = AutoProcessor.from_pretrained(
                self.model_name,
                trust_remote_code=True
            )
        self.tokenizer = self.processor.tokenizer
  
        # Accelerate í˜¸í™˜ì„ ìœ„í•œ ëª¨ë¸ ë¡œë”© ì„¤ì •
        if self.device in ["cpu", "accelerate"]:
            # Accelerateê°€ ê´€ë¦¬í•  ê²½ìš° device_map ì—†ì´ ë¡œë”©
            model_kwargs = {
                'torch_dtype': torch.float16,
                'trust_remote_code': True,
                'low_cpu_mem_usage': True,
                # device_mapì„ ì œê±°í•˜ì—¬ Accelerateê°€ ë¶„ì‚° ê´€ë¦¬í•˜ë„ë¡ í•¨
            }
        else:
            # ê¸°ë³¸ GPU ì‚¬ìš© ì‹œ (ë‹¨ì¼ GPU ëª¨ë“œ) - ë©”ëª¨ë¦¬ ìµœì í™”
            model_kwargs = {
                'torch_dtype': torch.float16,
                'trust_remote_code': True,
                'low_cpu_mem_usage': True,
                'use_cache': False,  # ìºì‹œ ë¹„í™œì„±í™”ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
                # device_map ì œê±°í•˜ì—¬ ë‹¨ì¼ GPU ì‚¬ìš©
            }

        logger.info("ğŸ”§ QWEN 7B ëª¨ë¸ ë¡œë”© ì¤‘... (ë‹¨ì¼ GPU ëª¨ë“œ)")
        
        self.model = Qwen2VLForConditionalGeneration.from_pretrained(
                self.model_name,
                **model_kwargs
        )
        
        # ëª¨ë¸ì„ ì§€ì •ëœ GPUë¡œ ì´ë™ (Accelerate ëª¨ë“œê°€ ì•„ë‹ ë•Œë§Œ)
        if self.device != "accelerate":
            self.model = self.model.to(self.device)
            logger.info(f"âœ… ëª¨ë¸ì„ {self.device}ë¡œ ì´ë™")
        
        # LoRA ì„¤ì • ë° ì ìš© - ê³ ì„±ëŠ¥ ì„¤ì • (ë©”ëª¨ë¦¬ ì—¬ìœ ë¶„ í™œìš©)
        lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            r=8,  # LoRA rank ëŒ€í­ ê°ì†Œ (32 â†’ 8) - ë©”ëª¨ë¦¬ ì ˆì•½
            lora_alpha=16,  # LoRA scaling parameter ê°ì†Œ (64 â†’ 16)
            lora_dropout=0.1,  # ë“œë¡­ì•„ì›ƒ ì¦ê°€
            target_modules=[
                # Attention ëª¨ë“ˆë“¤ë§Œ (MLP ì œê±°ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½)
                "q_proj", "v_proj", "k_proj", "o_proj",
                # Vision ê´€ë ¨ ëª¨ë“ˆë“¤ ì¶”ê°€
                "visual_proj", "lm_head"
            ],
            bias="lora_only",  # biasë„ LoRAë¡œ í•™ìŠµ
            inference_mode=False,
            modules_to_save=["embed_tokens"],  # ì„ë² ë”©ë„ í•™ìŠµ
        )
        
        # LoRA ì–´ëŒ‘í„° ì ìš©
        self.model = get_peft_model(self.model, lora_config)
        logger.info("âœ… LoRA ì–´ëŒ‘í„° ì ìš© ì™„ë£Œ")
        
        # LoRA íŒŒë¼ë¯¸í„° ì •ë³´ ì¶œë ¥ (ìƒì„¸ ë¶„ì„)
        try:
            # ì „ì²´ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
            total_params = sum(p.numel() for p in self.model.parameters())
            
            # í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
            
            # ê° LoRA ë ˆì´ì–´ë³„ íŒŒë¼ë¯¸í„° ìˆ˜ ë¶„ì„
            lora_details = {}
            for name, param in self.model.named_parameters():
                if param.requires_grad and ('lora' in name.lower() or 'adapter' in name.lower()):
                    module_name = name.split('.')[0] if '.' in name else name
                    if module_name not in lora_details:
                        lora_details[module_name] = 0
                    lora_details[module_name] += param.numel()
            
            trainable_percentage = 100 * trainable_params / total_params if total_params > 0 else 0
            
            logger.info(f"ğŸ“Š LoRA íŒŒë¼ë¯¸í„° ìƒì„¸ ì •ë³´:")
            logger.info(f"  - ì „ì²´ ëª¨ë¸ íŒŒë¼ë¯¸í„°: {total_params:,}")
            logger.info(f"  - í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°: {trainable_params:,}")
            logger.info(f"  - í•™ìŠµ ë¹„ìœ¨: {trainable_percentage:.4f}%")
            logger.info(f"  - LoRA ì„¤ì •:")
            logger.info(f"    * Rank (r): {lora_config.r}")
            logger.info(f"    * Alpha: {lora_config.lora_alpha}")
            logger.info(f"    * Dropout: {lora_config.lora_dropout}")
            logger.info(f"    * íƒ€ê²Ÿ ëª¨ë“ˆ: {lora_config.target_modules}")
            
            if lora_details:
                logger.info(f"  - LoRA ë ˆì´ì–´ë³„ íŒŒë¼ë¯¸í„°:")
                for module, count in lora_details.items():
                    logger.info(f"    * {module}: {count:,} íŒŒë¼ë¯¸í„°")
                    
        except Exception as e:
            logger.warning(f"âš ï¸ LoRA íŒŒë¼ë¯¸í„° ì •ë³´ ì¶œë ¥ ì‹¤íŒ¨: {e}")
            logger.info("âœ… LoRA ì–´ëŒ‘í„°ëŠ” ì •ìƒì ìœ¼ë¡œ ì ìš©ë¨")
        
        # ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •
        if hasattr(self.model, 'gradient_checkpointing_enable'):
            self.model.gradient_checkpointing_enable()
            logger.info("âœ… Gradient checkpointing í™œì„±í™”")

        if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
        # ìƒì„± ì„¤ì • - ê·¹í•œ ë©”ëª¨ë¦¬ ìµœì í™”
        self.generation_config = GenerationConfig(
            max_new_tokens=20,  # í† í° ìˆ˜ ë” ì œí•œ (30 â†’ 20)
            temperature=self.temperature,
            top_p=0.9,
            do_sample=True,
            pad_token_id=self.tokenizer.pad_token_id,
            eos_token_id=self.tokenizer.eos_token_id,
            use_cache=False,  # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ìºì‹œ ë¹„í™œì„±í™” ìœ ì§€
            output_hidden_states=False,  # hidden states ì¶œë ¥ ë¹„í™œì„±í™”
            output_attentions=False,  # attention weights ì¶œë ¥ ë¹„í™œì„±í™”
        )
        
        logger.info("âœ… QWEN ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ë©”ëª¨ë¦¬ ìµœì í™” ì ìš©)")

    def _setup_prompt_template(self):
         
        # ìƒì„± ì§€ì‹œë¥¼ ìœ„í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        self.system_prompt = """You are an expert at enhancing image generation prompts. 
        Given a simple user prompt, expand it into a detailed, and high-quality prompt for image generation.

        Guidelines:
        - Keep the original concept unchanged
        - Only using English words and sentences
        - Do not use any other languages
        - Do not use any special characters
        - Add artistic style, mood, and atmosphere
        - Include technical specifications (lighting, composition, resolution)
        - Add creative details that make the image more realistic
        - Make each enhancement unique and varied
        - Be descriptive but concise (aim for 20-40 additional words) """

        # ì‚¬ìš©ì ì…ë ¥ í…œí”Œë¦¿
        self.user_template = """Original prompt: {user_prompt}

        Enhanced version:"""

    def _setup_grpo_components(self):
        """GRPO ê´€ë ¨ ì»´í¬ë„ŒíŠ¸ ì„¤ì • - ë©”ëª¨ë¦¬ ìµœì í™” ë²„ì „"""
        self.hidden_size = self.model.config.hidden_size
        self.vocab_size = len(self.tokenizer.get_vocab())
        
        logger.info("ğŸ”§ GRPO ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì¤‘... (ë©”ëª¨ë¦¬ ìµœì í™”)")
        
        # Reference ëª¨ë¸ì€ í•­ìƒ í™œì„±í™” (KL penalty í•„ìš”)
        logger.info("ğŸ¯ Reference ëª¨ë¸ í™œì„±í™” (KL penalty ê³„ì‚°ìš©)")
        
        # Reference ëª¨ë¸ì„ CLIP GPUë¡œ ì´ë™ (GPU 1) - ë‹¨ì¼ GPU ëª¨ë“œ
        logger.info("ğŸ”§ Reference ëª¨ë¸ ìƒì„± ì¤‘... (CLIP GPUë¡œ ì´ë™)")
        self.ref_model = copy.deepcopy(self.model)
        self.ref_model.eval()
        
        # Reference ëª¨ë¸ì„ GPU 1 (CLIP GPU)ë¡œ ì´ë™
        if torch.cuda.is_available() and torch.cuda.device_count() > 1:
            self.ref_model = self.ref_model.to("cuda:1")
            logger.info("âœ… Reference ëª¨ë¸ì„ GPU 1 (CLIP GPU)ë¡œ ì´ë™")
        else:
            logger.info("âœ… Reference ëª¨ë¸ ìƒì„± ì™„ë£Œ (í˜„ì¬ ë””ë°”ì´ìŠ¤)")
        
        # ì˜µí‹°ë§ˆì´ì € (LoRA íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµ)
        # LoRA íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµí•˜ë„ë¡ í•„í„°ë§
        trainable_params = []
        total_trainable_params = 0
        
        logger.info("ğŸ” LoRA í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ë¶„ì„:")
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                trainable_params.append(param)
                param_count = param.numel()
                total_trainable_params += param_count
                logger.info(f"  ğŸ“Œ {name}: {param_count:,} íŒŒë¼ë¯¸í„° (shape: {param.shape})")
        
        logger.info(f"ğŸ“Š LoRA í•™ìŠµ íŒŒë¼ë¯¸í„° ì´ê³„:")
        logger.info(f"  - í•™ìŠµ ê°€ëŠ¥í•œ ë ˆì´ì–´ ìˆ˜: {len(trainable_params)}")
        logger.info(f"  - ì´ í•™ìŠµ íŒŒë¼ë¯¸í„° ìˆ˜: {total_trainable_params:,}")
        
        self.grpo_optimizer = torch.optim.AdamW(
            trainable_params, 
            lr=self.grpo_config.learning_rate,
            weight_decay=0.01
        )
        
        # EasyR1 ìŠ¤íƒ€ì¼ ìˆ˜ì¹˜ì  ì•ˆì •ì„± ë³€ìˆ˜ ì´ˆê¸°í™”
        self.grad_norm_ema = 0.0  # ê·¸ë˜ë””ì–¸íŠ¸ normì˜ ì§€ìˆ˜ ì´ë™ í‰ê· 
        self.training_step = 0  # íŠ¸ë ˆì´ë‹ ìŠ¤í… ì¹´ìš´í„°
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            logger.info("ğŸ§¹ GPU ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
        
        logger.info("âœ… GRPO ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì™„ë£Œ (ë©”ëª¨ë¦¬ ìµœì í™” + EasyR1 ì•ˆì •ì„±)")
        logger.info(f"ğŸ“Š QWEN ëª¨ë¸ ì§ì ‘ í•™ìŠµ ë°©ì‹ìœ¼ë¡œ ì„¤ì •")
        logger.info(f"ğŸ”§ EasyR1 ì•ˆì •ì„± ê¸°ë²•:")
        logger.info(f"  - ì ì‘ì  ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘: {self.grpo_config.use_adaptive_grad_clip}")
        logger.info(f"  - ê·¸ë˜ë””ì–¸íŠ¸ ì¤‘ì•™í™”: {self.grpo_config.use_grad_centralization}")
        logger.info(f"  - í™•ë¥ ì  ë°˜ì˜¬ë¦¼: {self.grpo_config.use_stochastic_rounding}")
        logger.info(f"  - Logits í´ë¦¬í•‘ ë²”ìœ„: Â±{self.grpo_config.logits_clip_range}")
        logger.info(f"  - ì•ˆì „í•œ ë¡œê·¸ í™•ë¥  ìµœì†Œê°’: {self.grpo_config.stable_log_prob_min}")

    def _init_grpo_weights(self):
        """GRPO ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” (QWEN ì§ì ‘ í•™ìŠµ ë°©ì‹ì—ì„œëŠ” ë¶ˆí•„ìš”)"""
        pass
    
    def _get_model_for_generation(self):
        """Accelerate/DDP ë˜í•‘ëœ ëª¨ë¸ì—ì„œ ì›ë³¸ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°"""
        # DistributedDataParallelë¡œ ë˜í•‘ëœ ê²½ìš° .moduleë¡œ ì ‘ê·¼
        if hasattr(self.model, 'module'):
            return self.model.module
        # ì¼ë°˜ ëª¨ë¸ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ë°˜í™˜
        return self.model

    def enhance_prompt(self, user_prompt):
        """ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ í–¥ìƒ (GRPO ì—†ì´)"""
        # VLMì— ì…ë ¥í•  ë©”ì‹œì§€ êµ¬ì„±
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": self.user_template.format(user_prompt=user_prompt)}
        ]
        
        # í…œí”Œë¦¿ ì ìš©
        prompt = self.tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        # í† í¬ë‚˜ì´ì§•
        inputs = self.tokenizer(
            prompt, 
            return_tensors="pt", 
            padding=True, 
            truncation=True
        )
        
        # Accelerate í™˜ê²½ì´ ì•„ë‹ ë•Œë§Œ ëª…ì‹œì ìœ¼ë¡œ ë””ë°”ì´ìŠ¤ ì´ë™
        if self.device != "accelerate":
            inputs = inputs.to(self.device)
        
        # ìƒì„±
        with torch.no_grad():
            model_for_gen = self._get_model_for_generation()
            outputs = model_for_gen.generate(
                **inputs,
                generation_config=self.generation_config,
                pad_token_id=self.tokenizer.pad_token_id
            )
        
        # ë””ì½”ë”©
        generated_text = self.tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[1]:], 
            skip_special_tokens=True
        ).strip()
        
        # í›„ì²˜ë¦¬
        enhanced_prompt = self._post_process_output(generated_text)
        
        result = {
            'original_prompt': user_prompt,
            'enhanced_prompt': enhanced_prompt,
            'raw_output': generated_text
        }
        
        logger.info(f"Enhanced prompt: '{user_prompt}' -> '{enhanced_prompt[:50]}...'")
        return result

    def generate_enhancement_candidates(self, user_prompt: str, num_candidates: int = None, 
                                        use_semantic_filtering: bool = True, 
                                        semantic_threshold: float = 0.7) -> List[str]:
        """ì—¬ëŸ¬ ê°œì˜ í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ í›„ë³´ ìƒì„± (semantic filtering í¬í•¨)"""
        if num_candidates is None:
            num_candidates = self.grpo_config.num_enhancement_candidates
        
        candidates = []
        
        # VLMì— ì…ë ¥í•  ë©”ì‹œì§€ êµ¬ì„±
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": self.user_template.format(user_prompt=user_prompt)}
        ]
        
        # í…œí”Œë¦¿ ì ìš©
        prompt = self.tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        # í† í¬ë‚˜ì´ì§•
        inputs = self.tokenizer(
            prompt, 
            return_tensors="pt", 
            padding=True, 
            truncation=True
        )
        
        # Accelerate í™˜ê²½ì´ ì•„ë‹ ë•Œë§Œ ëª…ì‹œì ìœ¼ë¡œ ë””ë°”ì´ìŠ¤ ì´ë™
        if self.device != "accelerate":
            inputs = inputs.to(self.device)
        
        # Semantic filtering ì œê±° - GRPOê°€ ì§ì ‘ explorationê³¼ exploitation ê· í˜•ì„ í•™ìŠµ
        # ë‹¤ì–‘í•œ í›„ë³´ ìƒì„±ì„ ìœ„í•´ ë” ë§ì€ ì‹œë„
        total_generations = num_candidates * 2
        
        raw_candidates = []
        
        # ì—¬ëŸ¬ í›„ë³´ ìƒì„±
        for i in range(total_generations):
            # ë‹¤ì–‘ì„±ì„ ìœ„í•´ temperatureì™€ top_p ì¡°ì •
            temp_config = GenerationConfig(
                max_new_tokens=77,
                temperature=self.temperature + (i * 0.05),  # ë” ì„¸ë°€í•œ ì¡°ì •
                top_p=0.85 + (i % 3) * 0.05,  # 0.85, 0.9, 0.95 ìˆœí™˜
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )
            
            with torch.no_grad():
                model_for_gen = self._get_model_for_generation()
                outputs = model_for_gen.generate(
                    **inputs,
                    generation_config=temp_config,
                    pad_token_id=self.tokenizer.pad_token_id
                )
            
            # ë””ì½”ë”©
            generated_text = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:], 
                skip_special_tokens=True
            ).strip()
            
            # í›„ì²˜ë¦¬
            enhanced_prompt = self._post_process_output(generated_text)
            
            # ì¤‘ë³µ ì œê±°
            if enhanced_prompt not in raw_candidates and enhanced_prompt != user_prompt:
                raw_candidates.append(enhanced_prompt)
        
        # Semantic filtering ì œê±° - ë‹¤ì–‘í•œ í›„ë³´ë“¤ì„ GRPOê°€ ì§ì ‘ í‰ê°€í•˜ë„ë¡
        # ì¤‘ë³µ ì œê±°í•˜ê³  ë‹¤ì–‘ì„± í™•ë³´ë¥¼ ìœ„í•´ ì…”í”Œ
        import random
        random.shuffle(raw_candidates)
        candidates = raw_candidates[:num_candidates]
        
        logger.info(f"ğŸ¯ ë‹¤ì–‘í•œ í›„ë³´ ìƒì„±: {len(raw_candidates)}ê°œ â†’ {num_candidates}ê°œ ì„ íƒ (semantic filtering ì—†ìŒ)")
        
        # í›„ë³´ê°€ ë¶€ì¡±í•˜ë©´ ì›ë³¸ í”„ë¡¬í”„íŠ¸ë¡œ ì±„ì›€
        while len(candidates) < num_candidates:
            candidates.append(user_prompt)
            logger.warning(f"âš ï¸ í›„ë³´ ë¶€ì¡±ìœ¼ë¡œ ì›ë³¸ í”„ë¡¬í”„íŠ¸ ì¶”ê°€: {len(candidates)}/{num_candidates}")
        
        return candidates[:num_candidates]

    def get_grpo_state_representation(self, user_prompt: str) -> torch.Tensor:
        """GRPOë¥¼ ìœ„í•œ ìƒíƒœ í‘œí˜„ ìƒì„± (QWEN ëª¨ë¸ë„ í•™ìŠµ ê°€ëŠ¥)"""
        # VLMì— ì…ë ¥í•  ë©”ì‹œì§€ êµ¬ì„±
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": self.user_template.format(user_prompt=user_prompt)}
        ]
        
        # í…œí”Œë¦¿ ì ìš©
        prompt = self.tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        # í† í¬ë‚˜ì´ì§•
        inputs = self.tokenizer(
            prompt, 
            return_tensors="pt", 
            padding=True, 
            truncation=True
        )
        
        # Accelerate í™˜ê²½ì´ ì•„ë‹ ë•Œë§Œ ëª…ì‹œì ìœ¼ë¡œ ë””ë°”ì´ìŠ¤ ì´ë™
        if self.device != "accelerate":
            inputs = inputs.to(self.device)
        
        # íˆë“  ìŠ¤í…Œì´íŠ¸ ì¶”ì¶œ (QWEN ëª¨ë¸ë„ í•™ìŠµë˜ë„ë¡ gradient ê³„ì‚°)
        outputs = self.model(
            input_ids=inputs['input_ids'],
            attention_mask=inputs['attention_mask'],
            output_hidden_states=True
        )
        
        if hasattr(outputs, 'last_hidden_state') and outputs.last_hidden_state is not None:
            hidden_states = outputs.last_hidden_state
        elif hasattr(outputs, 'hidden_states') and outputs.hidden_states is not None:
            hidden_states = outputs.hidden_states[-1]
        else:
            raise AttributeError("Cannot find hidden states in model output")
        
        # ë§ˆì§€ë§‰ í† í°ì˜ íˆë“  ìŠ¤í…Œì´íŠ¸ ì‚¬ìš©
        if inputs['attention_mask'] is not None:
            last_valid_indices = inputs['attention_mask'].sum(dim=1) - 1
            last_valid_indices = torch.clamp(last_valid_indices, min=0)
            state_repr = hidden_states[torch.arange(hidden_states.size(0)), last_valid_indices]
        else:
            state_repr = hidden_states[:, -1, :]
        
        return state_repr.squeeze(0)  # [hidden_size]

    def generate_grpo_enhanced_prompt(self, user_prompt: str) -> Tuple[str, torch.Tensor]:
        """GRPOë¥¼ í†µí•´ í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ ìƒì„± (QWEN ì§ì ‘ í•™ìŠµ)"""
        # VLMì— ì…ë ¥í•  ë©”ì‹œì§€ êµ¬ì„±
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": self.user_template.format(user_prompt=user_prompt)}
        ]
        
        # í…œí”Œë¦¿ ì ìš©
        prompt = self.tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        # í† í¬ë‚˜ì´ì§•
        inputs = self.tokenizer(
            prompt, 
            return_tensors="pt", 
            padding=True, 
            truncation=True
        )
        
        # Accelerate í™˜ê²½ì´ ì•„ë‹ ë•Œë§Œ ëª…ì‹œì ìœ¼ë¡œ ë””ë°”ì´ìŠ¤ ì´ë™
        if self.device != "accelerate":
            inputs = inputs.to(self.device)
        
        # QWEN ëª¨ë¸ë¡œ ì§ì ‘ ìƒì„± (gradient ê³„ì‚°)
        model_for_gen = self._get_model_for_generation()
        outputs = model_for_gen.generate(
            **inputs,
            generation_config=self.generation_config,
            pad_token_id=self.tokenizer.pad_token_id,
            return_dict_in_generate=True,
            output_scores=True
        )
        
        # ìƒì„±ëœ í…ìŠ¤íŠ¸ ë””ì½”ë”©
        generated_text = self.tokenizer.decode(
            outputs.sequences[0][inputs['input_ids'].shape[1]:], 
            skip_special_tokens=True
        ).strip()
        
        # í›„ì²˜ë¦¬
        enhanced_prompt = self._post_process_output(generated_text)
        
        # ë¡œê·¸ í™•ë¥  ê³„ì‚° (ìƒì„±ëœ í† í°ë“¤ì˜ ì´ ë¡œê·¸ í™•ë¥ ) - GRPO ì •í™•í•œ ê³„ì‚°
        if hasattr(outputs, 'scores') and outputs.scores:
            log_probs = []
            for i, score in enumerate(outputs.scores):
                try:
                    token_id = outputs.sequences[0][inputs['input_ids'].shape[1] + i]
                    
                    # ì•ˆì „í•œ log_softmax ê³„ì‚°
                    # scoreì— inf, nanì´ ìˆëŠ”ì§€ í™•ì¸
                    if torch.isnan(score).any() or torch.isinf(score).any():
                        logger.warning(f"âš ï¸ Scoreì— nan/inf ë°œê²¬, í´ë¦¬í•‘ ì ìš©")
                        score = torch.clamp(score, min=-100, max=100)
                    
                    # log_softmax ê³„ì‚°
                    log_softmax_scores = F.log_softmax(score, dim=-1)
                    
                    # ê²°ê³¼ ê²€ì¦
                    if torch.isnan(log_softmax_scores).any() or torch.isinf(log_softmax_scores).any():
                        logger.warning(f"âš ï¸ Log softmaxì— nan/inf ë°œê²¬, ì•ˆì „í•œ ê°’ìœ¼ë¡œ ëŒ€ì²´")
                        log_prob = torch.tensor(-10.0, device=score.device)  # ì•ˆì „í•œ ê¸°ë³¸ê°’
                    else:
                        log_prob = log_softmax_scores[0, token_id]
                        
                        # ì¶”ê°€ ì•ˆì „ì„± ê²€ì‚¬
                        if torch.isnan(log_prob) or torch.isinf(log_prob):
                            log_prob = torch.tensor(-10.0, device=score.device)
                    
                    log_probs.append(log_prob)
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ ë¡œê·¸ í™•ë¥  ê³„ì‚° ì˜¤ë¥˜: {e}, ê¸°ë³¸ê°’ ì‚¬ìš©")
                    if self.device == "accelerate":
                        log_probs.append(torch.tensor(-10.0))
                    else:
                        log_probs.append(torch.tensor(-10.0, device=self.device))
            
            if log_probs:
                # GRPOì—ì„œëŠ” ëª¨ë“  í† í°ì˜ ë¡œê·¸ í™•ë¥  í•©ì„ ì‚¬ìš© (í‰ê· ì´ ì•„ë‹˜)
                total_log_prob = torch.stack(log_probs).sum()
                
                # ìµœì¢… ì•ˆì „ì„± ê²€ì‚¬
                if torch.isnan(total_log_prob) or torch.isinf(total_log_prob):
                    logger.warning("âš ï¸ ì´ ë¡œê·¸ í™•ë¥ ì— nan/inf ë°œê²¬, ì•ˆì „í•œ ê°’ìœ¼ë¡œ ëŒ€ì²´")
                    if self.device == "accelerate":
                        total_log_prob = torch.tensor(-10.0)
                    else:
                        total_log_prob = torch.tensor(-10.0, device=self.device)
                        
                logger.debug(f"ğŸ” Generation: {len(log_probs)} tokens, total_log_prob={total_log_prob:.4f}")
                avg_log_prob = total_log_prob  # ë³€ìˆ˜ëª… ìœ ì§€ (í•˜ìœ„ í˜¸í™˜ì„±)
            else:
                # ë¡œê·¸ í™•ë¥  ê³„ì‚° ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
                if self.device == "accelerate":
                    avg_log_prob = torch.tensor(-10.0)
                else:
                    avg_log_prob = torch.tensor(-10.0, device=self.device)
        else:
            # fallback: ì•ˆì „í•œ ê¸°ë³¸ ë¡œê·¸ í™•ë¥ 
            if self.device == "accelerate":
                avg_log_prob = torch.tensor(-10.0)  # Accelerateê°€ ë””ë°”ì´ìŠ¤ ê´€ë¦¬
            else:
                avg_log_prob = torch.tensor(-10.0, device=self.device)
        
        return enhanced_prompt, avg_log_prob

    def calculate_log_prob_for_grpo(self, user_prompt: str, enhanced_prompt: str) -> torch.Tensor:
        """í˜„ì¬ ëª¨ë¸ì˜ ë¡œê·¸ í™•ë¥  ê³„ì‚° (gradient ê³„ì‚° í•„ìš”) - GRPO ì •í™•í•œ êµ¬í˜„"""
        # VLMì— ì…ë ¥í•  ë©”ì‹œì§€ êµ¬ì„±
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": self.user_template.format(user_prompt=user_prompt)}
        ]
        
        # í…œí”Œë¦¿ ì ìš©
        prompt = self.tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        # í”„ë¡¬í”„íŠ¸ë§Œ í† í¬ë‚˜ì´ì§• (ìƒì„± ì‹œì‘ì  í™•ì¸ìš©)
        prompt_inputs = self.tokenizer(
            prompt, 
            return_tensors="pt", 
            padding=True, 
            truncation=True
        )
        prompt_length = prompt_inputs['input_ids'].shape[1]
        
        # ì „ì²´ ì‹œí€€ìŠ¤ (í”„ë¡¬í”„íŠ¸ + ìƒì„±ëœ í…ìŠ¤íŠ¸)
        full_text = prompt + enhanced_prompt
        
        # ì „ì²´ ì‹œí€€ìŠ¤ í† í¬ë‚˜ì´ì§•
        inputs = self.tokenizer(
            full_text, 
            return_tensors="pt", 
            padding=True, 
            truncation=True
        )
        
        # Accelerate í™˜ê²½ì´ ì•„ë‹ ë•Œë§Œ ëª…ì‹œì ìœ¼ë¡œ ë””ë°”ì´ìŠ¤ ì´ë™
        if self.device != "accelerate":
            inputs = inputs.to(self.device)
        
        # í˜„ì¬ ëª¨ë¸ë¡œ ë¡œê·¸ í™•ë¥  ê³„ì‚° (gradient ê³„ì‚°)
        model_for_gen = self._get_model_for_generation()
        outputs = model_for_gen(
            input_ids=inputs['input_ids'],
            attention_mask=inputs['attention_mask']
        )
        
        # ìƒì„±ëœ ë¶€ë¶„ì˜ ë¡œê·¸ í™•ë¥ ë§Œ ê³„ì‚° - GRPO ì •í™•í•œ ë°©ì‹
        try:
            # ìƒì„±ëœ í† í°ë“¤ (í”„ë¡¬í”„íŠ¸ ì´í›„ ë¶€ë¶„)
            generated_tokens = inputs['input_ids'][0, prompt_length:]
            
            if len(generated_tokens) == 0:
                logger.warning("âš ï¸ ìƒì„±ëœ í† í°ì´ ì—†ìŒ, ê¸°ë³¸ê°’ ë°˜í™˜")
                return torch.tensor(-10.0, device=self.device, requires_grad=True)
            
            # ìƒì„±ëœ í† í°ì— ëŒ€ì‘í•˜ëŠ” logits (shift by 1)
            # logits[i]ëŠ” token[i+1]ì„ ì˜ˆì¸¡í•˜ë¯€ë¡œ, prompt_length-1ë¶€í„° ì‹œì‘
            generated_logits = outputs.logits[0, prompt_length-1:prompt_length-1+len(generated_tokens)]
            
            # EasyR1 ìŠ¤íƒ€ì¼ ì•ˆì „ì„± ê²€ì‚¬ - ë” ë³´ìˆ˜ì ì¸ í´ë¦¬í•‘
            if torch.isnan(generated_logits).any() or torch.isinf(generated_logits).any():
                logger.warning("âš ï¸ Generated logitsì— nan/inf ë°œê²¬, EasyR1 ìŠ¤íƒ€ì¼ í´ë¦¬í•‘ ì ìš©")
                generated_logits = torch.clamp(generated_logits, 
                                               min=-self.grpo_config.logits_clip_range, 
                                               max=self.grpo_config.logits_clip_range)
            else:
                # ì˜ˆë°©ì  í´ë¦¬í•‘ (EasyR1 ìŠ¤íƒ€ì¼)
                generated_logits = torch.clamp(generated_logits, 
                                               min=-self.grpo_config.logits_clip_range, 
                                               max=self.grpo_config.logits_clip_range)
            
            # í™•ë¥ ì  ë°˜ì˜¬ë¦¼ ì‹œë®¬ë ˆì´ì…˜ (EasyR1 ìŠ¤íƒ€ì¼)
            if self.grpo_config.use_stochastic_rounding and self.training:
                # ì‘ì€ ë…¸ì´ì¦ˆ ì¶”ê°€ë¡œ stochastic rounding íš¨ê³¼ ì‹œë®¬ë ˆì´ì…˜
                noise = torch.randn_like(generated_logits) * 1e-6
                generated_logits = generated_logits + noise
            
            # ê° í† í°ì— ëŒ€í•œ ë¡œê·¸ í™•ë¥  ê³„ì‚° (gradient ìœ ì§€)
            log_probs = F.log_softmax(generated_logits, dim=-1)
            
            # log_softmax ê²°ê³¼ ê²€ì¦
            if torch.isnan(log_probs).any() or torch.isinf(log_probs).any():
                logger.warning("âš ï¸ Log probabilitiesì— nan/inf ë°œê²¬, ì•ˆì „í•œ ê°’ìœ¼ë¡œ ëŒ€ì²´")
                return torch.tensor(-10.0, device=generated_logits.device, requires_grad=True)
            
            # ê° ìƒì„±ëœ í† í°ì˜ ë¡œê·¸ í™•ë¥  ì¶”ì¶œ
            token_log_probs = log_probs.gather(1, generated_tokens.unsqueeze(1)).squeeze(1)
            
            # GRPOì—ì„œëŠ” ëª¨ë“  í† í°ì˜ ë¡œê·¸ í™•ë¥  í•©ì„ ì‚¬ìš© (í‰ê· ì´ ì•„ë‹˜)
            total_log_prob = token_log_probs.sum()
            
            # ìµœì¢… ê²°ê³¼ ê²€ì¦
            if torch.isnan(total_log_prob) or torch.isinf(total_log_prob):
                logger.warning("âš ï¸ ì´ ë¡œê·¸ í™•ë¥ ì— nan/inf ë°œê²¬, ì•ˆì „í•œ ê°’ìœ¼ë¡œ ëŒ€ì²´")
                return torch.tensor(-10.0, device=generated_logits.device, requires_grad=True)
            
            logger.debug(f"ğŸ” Current model: {len(generated_tokens)} tokens, total_log_prob={total_log_prob:.4f}")
            return total_log_prob
            
        except Exception as e:
            logger.warning(f"âš ï¸ ë¡œê·¸ í™•ë¥  ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}, ì•ˆì „í•œ ê¸°ë³¸ê°’ ë°˜í™˜")
            if self.device == "accelerate":
                return torch.tensor(-10.0, requires_grad=True)
            else:
                return torch.tensor(-10.0, device=self.device, requires_grad=True)

    def get_ref_model_log_prob(self, user_prompt: str, enhanced_prompt: str) -> torch.Tensor:
        """ì°¸ì¡° ëª¨ë¸ì˜ ë¡œê·¸ í™•ë¥  ê³„ì‚° (ë‹¨ì¼ GPU ëª¨ë“œ)"""
        # Reference modelì´ ì—†ìœ¼ë©´ ë”ë¯¸ ê°’ ë°˜í™˜ (ë‹¨ì¼ GPU ëª¨ë“œì—ì„œëŠ” í•­ìƒ ìˆì–´ì•¼ í•¨)
        if self.ref_model is None:
            logger.warning("âš ï¸ Reference ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤!")
            return torch.tensor(0.0, device=self.device, dtype=torch.float16)
        
        # VLMì— ì…ë ¥í•  ë©”ì‹œì§€ êµ¬ì„±
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": self.user_template.format(user_prompt=user_prompt)}
        ]
        
        # í…œí”Œë¦¿ ì ìš©
        prompt = self.tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        # ì „ì²´ ì‹œí€€ìŠ¤ (í”„ë¡¬í”„íŠ¸ + ìƒì„±ëœ í…ìŠ¤íŠ¸)
        full_text = prompt + enhanced_prompt
        
        # Reference modelì˜ ë””ë°”ì´ìŠ¤ í™•ì¸ (GPU 1ì— ìˆìŒ)
        ref_device = next(self.ref_model.parameters()).device
        logger.info(f"ğŸ” Reference ëª¨ë¸ ë””ë°”ì´ìŠ¤: {ref_device}")
        
        # í† í¬ë‚˜ì´ì§• (Reference model ë””ë°”ì´ìŠ¤ì— ë§ì¶¤)
        inputs = self.tokenizer(
            full_text, 
            return_tensors="pt", 
            padding=True, 
            truncation=True
        ).to(ref_device)
        
        # ì°¸ì¡° ëª¨ë¸ë¡œ ë¡œê·¸ í™•ë¥  ê³„ì‚°
        with torch.no_grad():
            outputs = self.ref_model(
                input_ids=inputs['input_ids'],
                attention_mask=inputs['attention_mask']
            )
            
            # ìƒì„±ëœ ë¶€ë¶„ì˜ ë¡œê·¸ í™•ë¥ ë§Œ ê³„ì‚° - GRPO ì •í™•í•œ ë°©ì‹
            # í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ê³„ì‚° (current modelê³¼ ë™ì¼í•œ ë°©ì‹)
            prompt_inputs = self.tokenizer(prompt, return_tensors="pt")
            prompt_length = prompt_inputs['input_ids'].shape[1]
            
            # ìƒì„±ëœ í† í°ë“¤ (í”„ë¡¬í”„íŠ¸ ì´í›„ ë¶€ë¶„)
            generated_tokens = inputs['input_ids'][0, prompt_length:]
            
            if len(generated_tokens) == 0:
                logger.warning("âš ï¸ Reference model: ìƒì„±ëœ í† í°ì´ ì—†ìŒ")
                return torch.tensor(-10.0, device=self.device)
            
            # ìƒì„±ëœ í† í°ì— ëŒ€ì‘í•˜ëŠ” logits (shift by 1)
            generated_logits = outputs.logits[0, prompt_length-1:prompt_length-1+len(generated_tokens)]
            
            # EasyR1 ìŠ¤íƒ€ì¼ ì•ˆì „í•œ ë¡œê·¸ í™•ë¥  ê³„ì‚°
            if torch.isnan(generated_logits).any() or torch.isinf(generated_logits).any():
                logger.warning("âš ï¸ Reference model logitsì— nan/inf ë°œê²¬, EasyR1 ìŠ¤íƒ€ì¼ í´ë¦¬í•‘ ì ìš©")
                generated_logits = torch.clamp(generated_logits, 
                                               min=-self.grpo_config.logits_clip_range, 
                                               max=self.grpo_config.logits_clip_range)
            else:
                # ì˜ˆë°©ì  í´ë¦¬í•‘ (EasyR1 ìŠ¤íƒ€ì¼)
                generated_logits = torch.clamp(generated_logits, 
                                               min=-self.grpo_config.logits_clip_range, 
                                               max=self.grpo_config.logits_clip_range)
            
            log_probs = F.log_softmax(generated_logits, dim=-1)
            
            # log_softmax ê²°ê³¼ ê²€ì¦
            if torch.isnan(log_probs).any() or torch.isinf(log_probs).any():
                logger.warning("âš ï¸ Reference log probabilitiesì— nan/inf ë°œê²¬, ì•ˆì „í•œ ê°’ìœ¼ë¡œ ëŒ€ì²´")
                return torch.tensor(-10.0, device=self.device)
            
            # ê° ìƒì„±ëœ í† í°ì˜ ë¡œê·¸ í™•ë¥  ì¶”ì¶œ
            token_log_probs = log_probs.gather(1, generated_tokens.unsqueeze(1)).squeeze(1)
            
            # GRPOì—ì„œëŠ” ëª¨ë“  í† í°ì˜ ë¡œê·¸ í™•ë¥  í•©ì„ ì‚¬ìš© (í‰ê· ì´ ì•„ë‹˜)
            total_log_prob = token_log_probs.sum()
            
            # ìµœì¢… ê²°ê³¼ ê²€ì¦
            if torch.isnan(total_log_prob) or torch.isinf(total_log_prob):
                logger.warning("âš ï¸ Reference ì´ ë¡œê·¸ í™•ë¥ ì— nan/inf ë°œê²¬, ì•ˆì „í•œ ê°’ìœ¼ë¡œ ëŒ€ì²´")
                return torch.tensor(-10.0, device=self.device)
            
            # ê²°ê³¼ë¥¼ main model deviceë¡œ ì´ë™
            result = total_log_prob.to(self.device)
            logger.debug(f"ğŸ” Reference model: {len(generated_tokens)} tokens, total_log_prob={result:.4f}")
            return result

    def calculate_discounted_returns(self, rewards: List[float], gamma: float = None) -> torch.Tensor:
        """í• ì¸ëœ ë¦¬í„´ ê³„ì‚° (CartPole GRPO ë°©ì‹)"""
        if gamma is None:
            gamma = self.grpo_config.gamma
        
        returns = torch.zeros(len(rewards), device=self.device, dtype=torch.float32)
        discounted_return = 0.0
        
        # ì—­ìˆœìœ¼ë¡œ í• ì¸ëœ ë¦¬í„´ ê³„ì‚°
        for t in reversed(range(len(rewards))):
            discounted_return = rewards[t] + gamma * discounted_return
            returns[t] = discounted_return
        
        return returns
    
    def calculate_normalized_advantages(self, all_returns: torch.Tensor) -> torch.Tensor:
        """ì „ì²´ ê·¸ë£¹ì— ëŒ€í•œ ì •ê·œí™” (CartPole GRPO ë°©ì‹)"""
        if len(all_returns) <= 1:
            return all_returns
        
        mean_return = torch.mean(all_returns)
        std_return = torch.std(all_returns)
        
        # ì •ê·œí™”
        normalized_advantages = (all_returns - mean_return) / (std_return + self.grpo_config.epsilon_std)
        return normalized_advantages

    def update_grpo_policy(self, experiences: List[Dict]) -> Dict:
        """GRPO ì •ì±… ì—…ë°ì´íŠ¸ (CartPole GRPO í˜¸í™˜)"""
        if not experiences:
            return {}
        
        # ê²½í—˜ ë°ì´í„° ì¤€ë¹„
        user_prompts = []
        enhanced_prompts = []
        old_log_probs = []
        rewards = []
        
        for exp in experiences:
            user_prompts.append(exp['user_prompt'])
            enhanced_prompts.append(exp['enhanced_prompt'])
            old_log_probs.append(exp['log_prob'])
            rewards.append(exp['reward'])
        
        # í…ì„œë¡œ ë³€í™˜ (gradient ê³„ì‚°ì„ ìœ„í•´ float32 ì‚¬ìš©)
        old_log_probs = torch.stack(old_log_probs)
        
        # í• ì¸ëœ ë¦¬í„´ ê³„ì‚° (CartPole GRPO ë°©ì‹)
        discounted_returns = self.calculate_discounted_returns(rewards)
        
        # ì •ê·œí™”ëœ advantage ê³„ì‚° (CartPole GRPO ë°©ì‹)
        advantages = self.calculate_normalized_advantages(discounted_returns)
        
        logger.info(f"ğŸ“Š í• ì¸ëœ ë¦¬í„´ í†µê³„:")
        logger.info(f"  ì›ë³¸ ë¦¬ì›Œë“œ: mean={sum(rewards)/len(rewards):.4f}, std={torch.tensor(rewards).std():.4f}")
        logger.info(f"  í• ì¸ëœ ë¦¬í„´: mean={discounted_returns.mean():.4f}, std={discounted_returns.std():.4f}")
        logger.info(f"  ì •ê·œí™”ëœ advantage: mean={advantages.mean():.4f}, std={advantages.std():.4f}")
        
        baseline = sum(rewards) / len(rewards)  # ë¡œê¹…ìš©
        
        # í˜„ì¬ ëª¨ë¸ê³¼ ì°¸ì¡° ëª¨ë¸ì˜ ë¡œê·¸ í™•ë¥  ê³„ì‚° (gradient ê³„ì‚° í•„ìš”)
        current_log_probs = []
        ref_log_probs = []
        
        logger.info("ğŸ” ë¡œê·¸ í™•ë¥  ê³„ì‚° ì¤‘...")
        for i, (user_prompt, enhanced_prompt) in enumerate(zip(user_prompts, enhanced_prompts)):
            # í˜„ì¬ ëª¨ë¸ì˜ ë¡œê·¸ í™•ë¥  ê³„ì‚° (gradient í•„ìš”)
            current_log_prob = self.calculate_log_prob_for_grpo(user_prompt, enhanced_prompt)
            current_log_probs.append(current_log_prob)
            
            # ì°¸ì¡° ëª¨ë¸ì˜ ë¡œê·¸ í™•ë¥  (gradient ë¶ˆí•„ìš”)
            ref_log_prob = self.get_ref_model_log_prob(user_prompt, enhanced_prompt)
            ref_log_probs.append(ref_log_prob)
            
            logger.info(f"  ê²½í—˜ {i+1}: current_log_prob={current_log_prob:.4f}, ref_log_prob={ref_log_prob:.4f}")
        
        current_log_probs = torch.stack(current_log_probs)
        ref_log_probs = torch.stack(ref_log_probs)
        
        logger.info(f"ğŸ“Š ë¡œê·¸ í™•ë¥  í†µê³„:")
        logger.info(f"  Current log probs: mean={current_log_probs.mean():.4f}, std={current_log_probs.std():.4f}")
        logger.info(f"  Ref log probs: mean={ref_log_probs.mean():.4f}, std={ref_log_probs.std():.4f}")
        logger.info(f"  Old log probs: mean={old_log_probs.mean():.4f}, std={old_log_probs.std():.4f}")
        logger.info(f"  Advantages: mean={advantages.mean():.4f}, std={advantages.std():.4f}")
        
        # ì¤‘ìš”ë„ ë¹„ìœ¨ ê³„ì‚° - ì•ˆì „í•œ ê³„ì‚°
        log_ratio = current_log_probs - old_log_probs
        
        logger.info(f"ğŸ” ì¤‘ìš”ë„ ë¹„ìœ¨ ê³„ì‚°:")
        logger.info(f"  Log ratio: mean={log_ratio.mean():.6f}, std={log_ratio.std():.6f}")
        
        # ì•ˆì „ì„± ê²€ì‚¬
        if torch.isnan(log_ratio).any() or torch.isinf(log_ratio).any():
            logger.warning("âš ï¸ Log ratioì— nan/inf ë°œê²¬, í´ë¦¬í•‘ ì ìš©")
            log_ratio = torch.clamp(log_ratio, min=-10, max=10)
        
        ratio = torch.exp(log_ratio)
        logger.info(f"  Ratio: mean={ratio.mean():.6f}, std={ratio.std():.6f}")
        
        # ratio ì•ˆì „ì„± ê²€ì‚¬
        if torch.isnan(ratio).any() or torch.isinf(ratio).any():
            logger.warning("âš ï¸ Ratioì— nan/inf ë°œê²¬, ì•ˆì „í•œ ê°’ìœ¼ë¡œ ëŒ€ì²´")
            ratio = torch.clamp(ratio, min=0.1, max=10.0)
        
        # PPO í´ë¦½ëœ ëª©ì  í•¨ìˆ˜
        clipped_ratio = torch.clamp(ratio, 1 - self.grpo_config.clip_ratio, 1 + self.grpo_config.clip_ratio)
        logger.info(f"  Clipped ratio: mean={clipped_ratio.mean():.6f}, std={clipped_ratio.std():.6f}")
        
        # ì •ì±… ì†ì‹¤ ê³„ì‚°
        policy_obj1 = ratio * advantages
        policy_obj2 = clipped_ratio * advantages
        
        # ì•ˆì „ì„± ê²€ì‚¬ - 0ìœ¼ë¡œ ì„¤ì •í•˜ì§€ ë§ê³  ì‘ì€ ê°’ìœ¼ë¡œ ì„¤ì •
        if torch.isnan(policy_obj1).any() or torch.isinf(policy_obj1).any():
            logger.warning("âš ï¸ Policy objective 1ì— nan/inf ë°œê²¬, ì‘ì€ ê°’ìœ¼ë¡œ ëŒ€ì²´")
            policy_obj1 = advantages * 0.01  # ì‘ì€ ì‹ í˜¸ ìœ ì§€
            
        if torch.isnan(policy_obj2).any() or torch.isinf(policy_obj2).any():
            logger.warning("âš ï¸ Policy objective 2ì— nan/inf ë°œê²¬, ì‘ì€ ê°’ìœ¼ë¡œ ëŒ€ì²´")
            policy_obj2 = advantages * 0.01  # ì‘ì€ ì‹ í˜¸ ìœ ì§€
        
        policy_loss = -torch.min(policy_obj1, policy_obj2).mean()
        
        logger.info(f"ğŸ” ì •ì±… ì†ì‹¤ ê³„ì‚°:")
        logger.info(f"  Policy objective 1 mean: {policy_obj1.mean():.6f}")
        logger.info(f"  Policy objective 2 mean: {policy_obj2.mean():.6f}")
        logger.info(f"  Policy loss: {policy_loss:.6f}")
        
        # KL ë°œì‚° í˜ë„í‹° (CartPole GRPO ì •í™•í•œ ë°©ì‹)
        if self.ref_model is not None:
            # ì •í™•í•œ KL divergence ì¶”ì •ê¸° (CartPole GRPO ë°©ì‹)
            with torch.no_grad():
                log_ratio_ref_curr = ref_log_probs - current_log_probs.detach()
            
            # KL(ref || current) = exp(log_ratio) - log_ratio - 1
            kl_div_estimate = torch.exp(log_ratio_ref_curr) - log_ratio_ref_curr - 1
            kl_div = torch.relu(kl_div_estimate.mean())  # ìŒìˆ˜ ë°©ì§€
            
            # KL divergence ì•ˆì „ì„± ê²€ì‚¬
            if torch.isnan(kl_div) or torch.isinf(kl_div):
                logger.warning("âš ï¸ KL divergenceì— nan/inf ë°œê²¬, ì•ˆì „í•œ ê°’ìœ¼ë¡œ ëŒ€ì²´")
                kl_div = torch.tensor(0.01, device=self.device)  # ì‘ì€ ê°’ìœ¼ë¡œ ë³€ê²½
            
            kl_penalty = self.grpo_config.kl_coef * kl_div
            logger.info(f"  KL divergence (ì •í™•í•œ ì¶”ì •): {kl_div:.6f}")
            logger.info(f"  KL penalty: {kl_penalty:.6f}")
        else:
            kl_div = torch.tensor(0.0, device=self.device)
            kl_penalty = torch.tensor(0.0, device=self.device)
            logger.info("  Reference model ì—†ìŒ, KL penalty = 0")
        
        # ì—”íŠ¸ë¡œí”¼ ë³´ë„ˆìŠ¤ ê³„ì‚° (CartPole GRPO ë°©ì‹)
        # í˜„ì¬ ì •ì±…ì˜ ì—”íŠ¸ë¡œí”¼ ì¶”ì • (ë¡œê·¸ í™•ë¥ ì˜ ë¶„ì‚° ê¸°ë°˜)
        entropy_estimate = current_log_probs.var()
        entropy_bonus = self.grpo_config.entropy_coef * entropy_estimate
        
        # ì´ ì†ì‹¤ (ì •ì±… ì†ì‹¤ + KL í˜ë„í‹° - ì—”íŠ¸ë¡œí”¼ ë³´ë„ˆìŠ¤)
        total_loss = policy_loss + kl_penalty - entropy_bonus
        
        logger.info(f"  Entropy estimate: {entropy_estimate:.6f}")
        logger.info(f"  Entropy bonus: {entropy_bonus:.6f}")
        logger.info(f"  Total loss: {total_loss:.6f}")
        
        # ìµœì¢… ì†ì‹¤ ì•ˆì „ì„± ê²€ì‚¬
        if torch.isnan(total_loss) or torch.isinf(total_loss):
            logger.error("ğŸš¨ ì´ ì†ì‹¤ì— nan/inf ë°œê²¬! í•™ìŠµ ê±´ë„ˆë›°ê¸°")
            return {
                'policy_loss': 0.0,
                'kl_div': 0.0,
                'total_loss': 0.0,
                'mean_reward': rewards.mean().item(),
                'baseline': baseline.item(),
                'mean_advantage': advantages.mean().item(),
                'error': 'nan_inf_in_loss'
            }
        
        # ì—­ì „íŒŒ
        self.grpo_optimizer.zero_grad()
        total_loss.backward()
        
        # EasyR1 ìŠ¤íƒ€ì¼ ê·¸ë˜ë””ì–¸íŠ¸ ì•ˆì •ì„± ê¸°ë²• ì ìš©
        self.training_step += 1
        
        # 1. ê·¸ë˜ë””ì–¸íŠ¸ ì¤‘ì•™í™” (Gradient Centralization)
        if self.grpo_config.use_grad_centralization:
            self._apply_gradient_centralization()
        
        # 2. ê·¸ë˜ë””ì–¸íŠ¸ ì •ê·œí™” (Adaptive Gradient Normalization)
        if self.grpo_config.use_grad_normalization:
            self._apply_gradient_normalization()
        
        # 3. ì ì‘ì  ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ (AdaGC)
        if self.grpo_config.use_adaptive_grad_clip:
            grad_norm = self._apply_adaptive_gradient_clipping()
        else:
            # ê¸°ë³¸ ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
            grad_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
        
        logger.info(f"ğŸ”§ ê·¸ë˜ë””ì–¸íŠ¸ ì²˜ë¦¬:")
        logger.info(f"  - ê·¸ë˜ë””ì–¸íŠ¸ norm: {grad_norm:.6f}")
        logger.info(f"  - ì ì‘ì  í´ë¦¬í•‘ ì„ê³„ê°’: {self.grad_norm_ema * self.grpo_config.grad_clip_coef:.6f}")
        
        self.grpo_optimizer.step()
        
        # ë©”íŠ¸ë¦­ ì €ì¥ (ë©”ëª¨ë¦¬ ì •ë¦¬ ì „ì—)
        metrics = {
            'policy_loss': policy_loss.item(),
            'kl_div': kl_div.item(),
            'entropy': entropy_estimate.item(),
            'entropy_bonus': entropy_bonus.item(),
            'total_loss': total_loss.item(),
            'mean_reward': sum(rewards) / len(rewards),
            'baseline': baseline,
            'mean_advantage': advantages.mean().item()
        }
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del current_log_probs, ref_log_probs, ratio, clipped_ratio
        del policy_loss, kl_penalty, total_loss, rewards, advantages
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        return metrics
    
    def update_grpo_policy_multiple_epochs(self, experiences: List[Dict]) -> Dict:
        """ë‹¤ì¤‘ ì—í¬í¬ GRPO ì—…ë°ì´íŠ¸ (CartPole GRPO ë°©ì‹)"""
        if not experiences:
            return {}
        
        num_epochs = self.grpo_config.grpo_epochs
        logger.info(f"ğŸ”„ ë‹¤ì¤‘ ì—í¬í¬ GRPO ì—…ë°ì´íŠ¸ ì‹œì‘ ({num_epochs} ì—í¬í¬)")
        
        # ëˆ„ì  ë©”íŠ¸ë¦­
        total_policy_loss = 0.0
        total_kl_div = 0.0
        total_entropy = 0.0
        
        for epoch in range(num_epochs):
            logger.info(f"  ì—í¬í¬ {epoch + 1}/{num_epochs}")
            
            # ë‹¨ì¼ ì—í¬í¬ ì—…ë°ì´íŠ¸
            metrics = self.update_grpo_policy(experiences)
            
            # ë©”íŠ¸ë¦­ ëˆ„ì 
            total_policy_loss += metrics.get('policy_loss', 0.0)
            total_kl_div += metrics.get('kl_div', 0.0)
            total_entropy += metrics.get('entropy', 0.0)
            
            logger.info(f"    Policy loss: {metrics.get('policy_loss', 0.0):.6f}")
            logger.info(f"    KL div: {metrics.get('kl_div', 0.0):.6f}")
            logger.info(f"    Entropy: {metrics.get('entropy', 0.0):.6f}")
        
        # í‰ê·  ë©”íŠ¸ë¦­ ê³„ì‚°
        avg_metrics = {
            'avg_policy_loss': total_policy_loss / num_epochs,
            'avg_kl_div': total_kl_div / num_epochs,
            'avg_entropy': total_entropy / num_epochs,
            'num_epochs': num_epochs,
            'total_experiences': len(experiences)
        }
        
        logger.info(f"âœ… ë‹¤ì¤‘ ì—í¬í¬ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
        logger.info(f"  í‰ê·  Policy loss: {avg_metrics['avg_policy_loss']:.6f}")
        logger.info(f"  í‰ê·  KL div: {avg_metrics['avg_kl_div']:.6f}")
        
        return avg_metrics

    def _post_process_output(self, raw_output):
        """ìƒì„±ëœ ì¶œë ¥ í›„ì²˜ë¦¬"""
        # ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±°
        enhanced = raw_output.strip()
        
        # "Enhanced prompt:" ë“±ì˜ ë ˆì´ë¸” ì œê±°
        enhanced = re.sub(r'^(Enhanced prompt:|Prompt:|Result:)\s*', '', enhanced, flags=re.IGNORECASE)
        enhanced = enhanced.strip()
        
        # ë”°ì˜´í‘œ ì œê±°
        enhanced = enhanced.strip('"\'')
        
        return enhanced
    
    def enhance_prompts_batch(self, user_prompts):
        """ë°°ì¹˜ë¡œ ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ í–¥ìƒ"""
        results = []
        for prompt in user_prompts:
            result = self.enhance_prompt(prompt)
            results.append(result)
        return results

    def update_reference_model(self):
        """ë§¤ iterationë§ˆë‹¤ í˜„ì¬ ëª¨ë¸ì„ referenceë¡œ ë³µì‚¬ (CartPole GRPO ë°©ì‹)"""
        if self.ref_model is not None:
            logger.info("ğŸ”„ Reference ëª¨ë¸ ì—…ë°ì´íŠ¸ ì¤‘...")
            self.ref_model.load_state_dict(self.model.state_dict())
            self.ref_model.eval()
            logger.info("âœ… Reference ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
        else:
            logger.warning("âš ï¸ Reference ëª¨ë¸ì´ ì—†ì–´ ì—…ë°ì´íŠ¸ ê±´ë„ˆë›°ê¸°")
    
    def move_ref_model_to_device(self, device: str):
        """Reference ëª¨ë¸ì„ ì§€ì •ëœ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™ (ì „ì²´ í•™ìŠµì—ì„œëŠ” ë¹„í™œì„±í™”)"""
        if self.ref_model is not None:
            logger.info(f"ğŸ”§ Reference ëª¨ë¸ì„ {device}ë¡œ ì´ë™")
            self.ref_model = self.ref_model.to(device)
        else:
            logger.info("ğŸ¯ ì „ì²´ í•™ìŠµ ëª¨ë“œ: Reference ëª¨ë¸ ì´ë™ ê±´ë„ˆë›°ê¸°")
    
    def save_lora_model(self, save_path: str):
        """LoRA ì–´ëŒ‘í„° ì €ì¥"""
        try:
            self.model.save_pretrained(save_path)
            logger.info(f"âœ… LoRA ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {save_path}")
        except Exception as e:
            logger.error(f"âŒ LoRA ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def load_lora_model(self, load_path: str):
        """LoRA ì–´ëŒ‘í„° ë¡œë“œ"""
        try:
            from peft import PeftModel
            self.model = PeftModel.from_pretrained(self.model, load_path)
            logger.info(f"âœ… LoRA ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {load_path}")
        except Exception as e:
            logger.error(f"âŒ LoRA ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def get_lora_trainable_params(self):
        """LoRA í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ì •ë³´ ë°˜í™˜"""
        trainable_params = 0
        all_param = 0
        for _, param in self.model.named_parameters():
            all_param += param.numel()
            if param.requires_grad:
                trainable_params += param.numel()
        
        return {
            'trainable_params': trainable_params,
            'all_params': all_param,
            'trainable_percentage': 100 * trainable_params / all_param
        }
    
    def _apply_gradient_centralization(self):
        """ê·¸ë˜ë””ì–¸íŠ¸ ì¤‘ì•™í™” (EasyR1 ìŠ¤íƒ€ì¼)"""
        for param in self.model.parameters():
            if param.grad is not None and param.grad.dim() > 1:
                # ê·¸ë˜ë””ì–¸íŠ¸ì˜ í‰ê· ì„ ë¹¼ì„œ ì¤‘ì•™í™”
                grad_mean = param.grad.mean(dim=tuple(range(1, param.grad.dim())), keepdim=True)
                param.grad = param.grad - grad_mean
    
    def _apply_gradient_normalization(self):
        """ì ì‘ì  ê·¸ë˜ë””ì–¸íŠ¸ ì •ê·œí™” (EasyR1 ìŠ¤íƒ€ì¼)"""
        alpha = self.grpo_config.grad_norm_alpha
        
        for param in self.model.parameters():
            if param.grad is not None:
                grad = param.grad
                grad_std = grad.std()
                
                # í‘œì¤€í¸ì°¨ê°€ 0ì´ ì•„ë‹ ë•Œë§Œ ì •ê·œí™” ì ìš©
                if grad_std > 1e-8:
                    normalized_grad = grad / (grad_std + 1e-8)
                    param.grad = (1 - alpha) * grad + alpha * normalized_grad
    
    def _apply_adaptive_gradient_clipping(self) -> float:
        """ì ì‘ì  ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ (AdaGC ìŠ¤íƒ€ì¼)"""
        # í˜„ì¬ ê·¸ë˜ë””ì–¸íŠ¸ norm ê³„ì‚°
        grad_norm = 0.0
        for param in self.model.parameters():
            if param.grad is not None:
                grad_norm += param.grad.data.norm().item() ** 2
        grad_norm = grad_norm ** 0.5
        
        # ì§€ìˆ˜ ì´ë™ í‰ê·  ì—…ë°ì´íŠ¸
        if self.grad_norm_ema == 0.0:
            self.grad_norm_ema = grad_norm
        else:
            beta = self.grpo_config.grad_clip_ema_beta
            self.grad_norm_ema = beta * self.grad_norm_ema + (1 - beta) * grad_norm
        
        # ì ì‘ì  í´ë¦¬í•‘ ì„ê³„ê°’ ê³„ì‚°
        clip_threshold = self.grpo_config.grad_clip_coef * self.grad_norm_ema
        
        # í´ë¦¬í•‘ ì ìš©
        if grad_norm > clip_threshold:
            clip_coef = clip_threshold / (grad_norm + 1e-8)
            for param in self.model.parameters():
                if param.grad is not None:
                    param.grad.data.mul_(clip_coef)
            
            logger.info(f"ğŸ”§ ì ì‘ì  ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ ì ìš©: {grad_norm:.6f} -> {clip_threshold:.6f}")
        
        return grad_norm
