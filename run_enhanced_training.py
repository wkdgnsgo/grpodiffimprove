#!/usr/bin/env python3
"""
Enhanced VLM GRPO Training Script
=================================

MS Swift CoZ GRPOë¥¼ ì°¸ì¡°í•˜ì—¬ ê°œì„ ëœ VLM GRPO í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.
LoRA ë²„ì „ê³¼ ì „ì²´ í•™ìŠµ ë²„ì „ì„ ëª¨ë‘ ì§€ì›í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    # LoRA í•™ìŠµ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
    python run_enhanced_training.py --train_type lora --lora_rank 8 --lora_alpha 32
    
    # ì „ì²´ í•™ìŠµ (ê³ ì„±ëŠ¥)
    python run_enhanced_training.py --train_type full --use_deepspeed
    
    # QLoRA í•™ìŠµ (ì–‘ìí™” + LoRA)
    python run_enhanced_training.py --train_type qlora --lora_rank 16

MS Swift í˜¸í™˜ ì˜µì…˜:
    --train_type {full,lora,qlora}
    --lora_rank 8
    --lora_alpha 32
    --target_modules all-linear
    --learning_rate 1e-5
    --per_device_train_batch_size 4
    --num_generations 4
    --temperature 0.9
    --deepspeed zero2
    --use_vllm
    --log_completions

Author: AI Assistant (Based on MS Swift CoZ GRPO)
Date: 2025-01-22
"""

import argparse
import sys
import os
import json
import logging
from pathlib import Path
from typing import Dict, Any, Optional

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python pathì— ì¶”ê°€
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir))

def setup_logging(debug: bool = False):
    """ë¡œê¹… ì„¤ì •"""
    level = logging.DEBUG if debug else logging.INFO
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler('vlm_grpo_training.log')
        ]
    )

def create_ms_swift_style_args():
    """MS Swift ìŠ¤íƒ€ì¼ ëª…ë ¹í–‰ ì¸ì ìƒì„±"""
    parser = argparse.ArgumentParser(
        description="Enhanced VLM GRPO Training (MS Swift Style)",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # ê¸°ë³¸ ëª¨ë¸ ì„¤ì •
    parser.add_argument(
        "--model", "--vlm_model",
        type=str,
        default="microsoft/DialoGPT-medium",
        help="VLM ëª¨ë¸ ì´ë¦„"
    )
    
    parser.add_argument(
        "--sd_model",
        type=str,
        default="stabilityai/stable-diffusion-3-medium",
        help="Stable Diffusion ëª¨ë¸ ì´ë¦„"
    )
    
    parser.add_argument(
        "--clip_model",
        type=str,
        default="openai/clip-vit-base-patch32",
        help="CLIP ëª¨ë¸ ì´ë¦„"
    )
    
    # í•™ìŠµ íƒ€ì… ì„¤ì • (MS Swift ìŠ¤íƒ€ì¼)
    parser.add_argument(
        "--train_type",
        type=str,
        choices=["full", "lora", "qlora"],
        default="lora",
        help="í•™ìŠµ íƒ€ì…: full(ì „ì²´), lora(LoRA), qlora(QLoRA)"
    )
    
    parser.add_argument(
        "--torch_dtype",
        type=str,
        choices=["float16", "bfloat16", "float32"],
        default="bfloat16",
        help="ëª¨ë¸ ë°ì´í„° íƒ€ì…"
    )
    
    # LoRA ì„¤ì • (MS Swift ê¸°ë³¸ê°’)
    parser.add_argument(
        "--lora_rank",
        type=int,
        default=8,
        help="LoRA rank (ë‚®ì„ìˆ˜ë¡ ë©”ëª¨ë¦¬ íš¨ìœ¨ì )"
    )
    
    parser.add_argument(
        "--lora_alpha",
        type=int,
        default=32,
        help="LoRA alpha (í•™ìŠµ ê°•ë„ ì¡°ì ˆ)"
    )
    
    parser.add_argument(
        "--lora_dropout",
        type=float,
        default=0.1,
        help="LoRA dropout ë¹„ìœ¨"
    )
    
    parser.add_argument(
        "--target_modules",
        type=str,
        default="all-linear",
        help="LoRA ì ìš© ëª¨ë“ˆ (all-linear ë˜ëŠ” íŠ¹ì • ëª¨ë“ˆëª…)"
    )
    
    # í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°
    parser.add_argument(
        "--learning_rate",
        type=float,
        default=1e-5,
        help="í•™ìŠµë¥ "
    )
    
    parser.add_argument(
        "--num_train_epochs", "--num_iterations",
        type=int,
        default=20,
        help="í•™ìŠµ ë°˜ë³µ íšŸìˆ˜"
    )
    
    parser.add_argument(
        "--per_device_train_batch_size",
        type=int,
        default=4,
        help="ë””ë°”ì´ìŠ¤ë‹¹ ë°°ì¹˜ í¬ê¸°"
    )
    
    parser.add_argument(
        "--gradient_accumulation_steps",
        type=int,
        default=1,
        help="ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ìŠ¤í…"
    )
    
    parser.add_argument(
        "--max_grad_norm",
        type=float,
        default=1.0,
        help="ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ ì„ê³„ê°’"
    )
    
    parser.add_argument(
        "--warmup_ratio",
        type=float,
        default=0.05,
        help="ì›œì—… ë¹„ìœ¨"
    )
    
    # GRPO íŠ¹í™” ì„¤ì •
    parser.add_argument(
        "--group_size",
        type=int,
        default=4,
        help="GRPO ê·¸ë£¹ í¬ê¸°"
    )
    
    parser.add_argument(
        "--num_generations",
        type=int,
        default=4,
        help="ìƒì„±í•  í›„ë³´ ìˆ˜"
    )
    
    parser.add_argument(
        "--temperature",
        type=float,
        default=0.8,
        help="ìƒì„± ì˜¨ë„"
    )
    
    parser.add_argument(
        "--top_p",
        type=float,
        default=0.9,
        help="Top-p ìƒ˜í”Œë§"
    )
    
    parser.add_argument(
        "--top_k",
        type=int,
        default=50,
        help="Top-k ìƒ˜í”Œë§"
    )
    
    # ë°ì´í„° ì„¤ì •
    parser.add_argument(
        "--dataset", "--train_data_path",
        type=str,
        default="train_prompts.jsonl",
        help="í•™ìŠµ ë°ì´í„° ê²½ë¡œ"
    )
    
    parser.add_argument(
        "--val_data_path",
        type=str,
        default="val_prompts.jsonl",
        help="ê²€ì¦ ë°ì´í„° ê²½ë¡œ"
    )
    
    parser.add_argument(
        "--max_length",
        type=int,
        default=2048,
        help="ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´"
    )
    
    parser.add_argument(
        "--max_completion_length",
        type=int,
        default=1024,
        help="ìµœëŒ€ ìƒì„± ê¸¸ì´"
    )
    
    # í‰ê°€ ë° ì €ì¥ ì„¤ì •
    parser.add_argument(
        "--eval_steps",
        type=int,
        default=100,
        help="í‰ê°€ ì£¼ê¸°"
    )
    
    parser.add_argument(
        "--save_steps",
        type=int,
        default=100,
        help="ì €ì¥ ì£¼ê¸°"
    )
    
    parser.add_argument(
        "--save_total_limit",
        type=int,
        default=2,
        help="ì €ì¥í•  ì²´í¬í¬ì¸íŠ¸ ìˆ˜"
    )
    
    parser.add_argument(
        "--logging_steps",
        type=int,
        default=5,
        help="ë¡œê¹… ì£¼ê¸°"
    )
    
    parser.add_argument(
        "--validation_interval",
        type=int,
        default=5,
        help="ê²€ì¦ ì£¼ê¸°"
    )
    
    parser.add_argument(
        "--checkpoint_interval",
        type=int,
        default=10,
        help="ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì£¼ê¸°"
    )
    
    # ì¶œë ¥ ì„¤ì •
    parser.add_argument(
        "--output_dir",
        type=str,
        default="vlm_grpo_results_enhanced",
        help="ì¶œë ¥ ë””ë ‰í† ë¦¬"
    )
    
    parser.add_argument(
        "--log_completions",
        action="store_true",
        help="ìƒì„± ê²°ê³¼ ë¡œê¹…"
    )
    
    # ë¶„ì‚° í•™ìŠµ ì„¤ì •
    parser.add_argument(
        "--deepspeed",
        type=str,
        choices=["zero2", "zero3"],
        help="DeepSpeed ì„¤ì •"
    )
    
    parser.add_argument(
        "--use_deepspeed",
        action="store_true",
        help="DeepSpeed ì‚¬ìš©"
    )
    
    # vLLM ì„¤ì •
    parser.add_argument(
        "--use_vllm",
        action="store_true",
        help="vLLM ê°€ì† ì‚¬ìš©"
    )
    
    parser.add_argument(
        "--vllm_gpu_memory_utilization",
        type=float,
        default=0.7,
        help="vLLM GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ "
    )
    
    parser.add_argument(
        "--vllm_max_model_len",
        type=int,
        default=4096,
        help="vLLM ìµœëŒ€ ëª¨ë¸ ê¸¸ì´"
    )
    
    # í•˜ë“œì›¨ì–´ ìµœì í™”
    parser.add_argument(
        "--device",
        type=str,
        default="auto",
        help="ë””ë°”ì´ìŠ¤ ì„¤ì • (auto/cuda/mps/cpu)"
    )
    
    parser.add_argument(
        "--use_flash_attention",
        action="store_true",
        default=True,
        help="Flash Attention ì‚¬ìš©"
    )
    
    parser.add_argument(
        "--gradient_checkpointing",
        action="store_true",
        default=True,
        help="ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… ì‚¬ìš©"
    )
    
    # ì‹¤í—˜ ì¶”ì 
    parser.add_argument(
        "--use_wandb", "--report_to",
        action="store_true",
        help="Wandb ì‚¬ìš©"
    )
    
    parser.add_argument(
        "--wandb_project",
        type=str,
        default="vlm-grpo-enhanced",
        help="Wandb í”„ë¡œì íŠ¸ëª…"
    )
    
    parser.add_argument(
        "--wandb_run_name",
        type=str,
        help="Wandb ì‹¤í–‰ëª…"
    )
    
    # ë³´ìƒ í•¨ìˆ˜ ì„¤ì •
    parser.add_argument(
        "--reward_funcs",
        type=str,
        nargs="+",
        default=["clip_similarity", "image_quality"],
        help="ë³´ìƒ í•¨ìˆ˜ ëª©ë¡"
    )
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    parser.add_argument(
        "--system",
        type=str,
        help="ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ íŒŒì¼ ê²½ë¡œ"
    )
    
    # ì‹¤í–‰ ëª¨ë“œ
    parser.add_argument(
        "--test",
        action="store_true",
        help="í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ë¹ ë¥¸ ì‹¤í–‰)"
    )
    
    parser.add_argument(
        "--debug",
        action="store_true",
        help="ë””ë²„ê·¸ ëª¨ë“œ"
    )
    
    parser.add_argument(
        "--config",
        type=str,
        help="ì„¤ì • íŒŒì¼ ê²½ë¡œ"
    )
    
    return parser

def apply_test_mode_settings(args):
    """í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì„¤ì • ì ìš©"""
    if args.test:
        print("ğŸ§ª Test mode enabled - applying fast settings")
        args.num_train_epochs = 3
        args.group_size = 2
        args.validation_interval = 2
        args.checkpoint_interval = 2
        args.logging_steps = 1
        args.per_device_train_batch_size = min(args.per_device_train_batch_size, 2)

def create_config_from_args(args) -> Dict[str, Any]:
    """ëª…ë ¹í–‰ ì¸ìì—ì„œ ì„¤ì • ë”•ì…”ë„ˆë¦¬ ìƒì„±"""
    config = {
        # ê¸°ë³¸ ëª¨ë¸ ì„¤ì •
        "vlm_model": args.model,
        "sd_model": args.sd_model,
        "clip_model": args.clip_model,
        
        # í•™ìŠµ íƒ€ì… ì„¤ì •
        "train_type": args.train_type,
        "torch_dtype": args.torch_dtype,
        
        # LoRA ì„¤ì •
        "lora_rank": args.lora_rank,
        "lora_alpha": args.lora_alpha,
        "lora_dropout": args.lora_dropout,
        "target_modules": args.target_modules,
        
        # í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°
        "learning_rate": args.learning_rate,
        "num_iterations": args.num_train_epochs,
        "per_device_train_batch_size": args.per_device_train_batch_size,
        "gradient_accumulation_steps": args.gradient_accumulation_steps,
        "max_grad_norm": args.max_grad_norm,
        "warmup_ratio": args.warmup_ratio,
        
        # GRPO íŠ¹í™” ì„¤ì •
        "group_size": args.group_size,
        "num_generations": args.num_generations,
        "temperature": args.temperature,
        "top_p": args.top_p,
        "grpo_epochs": 2,
        "kl_beta": 0.01,
        "clip_epsilon": 0.2,
        
        # ë°ì´í„° ì„¤ì •
        "train_data_path": args.dataset,
        "val_data_path": args.val_data_path,
        "max_length": args.max_length,
        "max_completion_length": args.max_completion_length,
        
        # í‰ê°€ ë° ì €ì¥ ì„¤ì •
        "validation_interval": args.validation_interval,
        "checkpoint_interval": args.checkpoint_interval,
        "eval_steps": args.eval_steps,
        "save_steps": args.save_steps,
        "save_total_limit": args.save_total_limit,
        "logging_steps": args.logging_steps,
        
        # ì¶œë ¥ ì„¤ì •
        "output_dir": args.output_dir,
        "log_completions": args.log_completions,
        
        # ë¶„ì‚° í•™ìŠµ ì„¤ì •
        "use_deepspeed": args.use_deepspeed or bool(args.deepspeed),
        "deepspeed_config": args.deepspeed or "zero2",
        
        # í•˜ë“œì›¨ì–´ ìµœì í™”
        "device": args.device,
        "use_flash_attention": args.use_flash_attention,
        "gradient_checkpointing": args.gradient_checkpointing,
        
        # ë³´ìƒ í•¨ìˆ˜ ì„¤ì •
        "reward_weights": {
            "clip_similarity": 0.6,
            "image_quality": 0.3,
            "semantic_consistency": 0.1
        },
        
        # ì‹¤í—˜ ì¶”ì 
        "use_wandb": args.use_wandb,
        "wandb_project": args.wandb_project,
        "wandb_run_name": args.wandb_run_name,
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        "system_prompt": args.system
    }
    
    return config

def print_training_info(args):
    """í•™ìŠµ ì •ë³´ ì¶œë ¥"""
    print("ğŸš€ Enhanced VLM GRPO Training")
    print("=" * 60)
    print(f"ğŸ“‹ Configuration:")
    print(f"   - Train Type: {args.train_type}")
    print(f"   - Model: {args.model}")
    print(f"   - Device: {args.device}")
    print(f"   - Torch Dtype: {args.torch_dtype}")
    
    if args.train_type in ["lora", "qlora"]:
        print(f"ğŸ¯ LoRA Settings:")
        print(f"   - Rank: {args.lora_rank}")
        print(f"   - Alpha: {args.lora_alpha}")
        print(f"   - Target Modules: {args.target_modules}")
    
    print(f"âš™ï¸ Training Settings:")
    print(f"   - Learning Rate: {args.learning_rate}")
    print(f"   - Batch Size: {args.per_device_train_batch_size}")
    print(f"   - Iterations: {args.num_train_epochs}")
    print(f"   - Group Size: {args.group_size}")
    print(f"   - Generations: {args.num_generations}")
    
    if args.use_deepspeed or args.deepspeed:
        print(f"âš¡ DeepSpeed: {args.deepspeed or 'zero2'}")
    
    if args.use_vllm:
        print(f"ğŸš„ vLLM: Enabled")
    
    print(f"ğŸ“Š Output:")
    print(f"   - Directory: {args.output_dir}")
    print(f"   - Wandb: {args.use_wandb}")
    print(f"   - Test Mode: {args.test}")
    print("=" * 60)

def check_dependencies():
    """í•„ìˆ˜ ì˜ì¡´ì„± í™•ì¸"""
    missing_deps = []
    
    try:
        import torch
    except ImportError:
        missing_deps.append("torch")
    
    try:
        import transformers
    except ImportError:
        missing_deps.append("transformers")
    
    try:
        import diffusers
    except ImportError:
        missing_deps.append("diffusers")
    
    try:
        from PIL import Image
    except ImportError:
        missing_deps.append("pillow")
    
    if missing_deps:
        print("âŒ Missing dependencies:")
        for dep in missing_deps:
            print(f"   - {dep}")
        print("\nğŸ’¡ Install with:")
        print(f"   pip install {' '.join(missing_deps)}")
        return False
    
    return True

def create_sample_data_if_needed(train_path: str, val_path: str):
    """í•„ìš”ì‹œ ìƒ˜í”Œ ë°ì´í„° ìƒì„±"""
    if not Path(train_path).exists() or not Path(val_path).exists():
        print("ğŸ“Š Creating sample data...")
        try:
            from utils.data_loader import create_sample_data
            create_sample_data(train_path, val_path)
            print("âœ… Sample data created")
        except Exception as e:
            print(f"âš ï¸ Could not create sample data: {e}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ëª…ë ¹í–‰ ì¸ì íŒŒì‹±
    parser = create_ms_swift_style_args()
    args = parser.parse_args()
    
    # ë¡œê¹… ì„¤ì •
    setup_logging(args.debug)
    
    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì„¤ì • ì ìš©
    apply_test_mode_settings(args)
    
    # í•™ìŠµ ì •ë³´ ì¶œë ¥
    print_training_info(args)
    
    # ì˜ì¡´ì„± í™•ì¸
    if not check_dependencies():
        sys.exit(1)
    
    # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    create_sample_data_if_needed(args.dataset, args.val_data_path)
    
    try:
        # ì„¤ì • ìƒì„±
        config = create_config_from_args(args)
        
        # ì„¤ì • íŒŒì¼ì—ì„œ ë¡œë“œ (ìˆëŠ” ê²½ìš°)
        if args.config and Path(args.config).exists():
            with open(args.config, 'r', encoding='utf-8') as f:
                file_config = json.load(f)
            config.update(file_config)
        
        # Enhanced VLM GRPO System ì„í¬íŠ¸ ë° ì‹¤í–‰
        from integration.main_trainer_enhanced import EnhancedVLMGRPOSystem
        
        # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        system = EnhancedVLMGRPOSystem(**config)
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        print("\nğŸ”§ Initializing components...")
        system.initialize_components()
        
        # ì‹œìŠ¤í…œ ìš”ì•½ ì¶œë ¥
        system.print_system_summary()
        
        # í•™ìŠµ ì‹¤í–‰
        print("\nğŸš€ Starting training...")
        system.run_training()
        
        # ìµœì¢… í†µê³„ ì¶œë ¥
        final_stats = system.get_training_stats()
        print("\nğŸ‰ Training completed!")
        print(f"   - Total samples: {final_stats['total_samples']}")
        print(f"   - Best reward: {final_stats['best_reward']:.4f}")
        print(f"   - Training time: {final_stats['training_time']:.2f}s")
        print(f"   - Results saved to: {args.output_dir}")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ Training interrupted by user")
        sys.exit(1)
        
    except Exception as e:
        print(f"\nâŒ Training failed: {e}")
        if args.debug:
            import traceback
            traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main() 