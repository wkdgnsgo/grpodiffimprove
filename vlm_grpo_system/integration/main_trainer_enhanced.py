"""
Enhanced VLM GRPO Main Trainer
==============================

MS Swift CoZ GRPOë¥¼ ì°¸ì¡°í•˜ì—¬ ê°œì„ ëœ í†µí•© VLM GRPO ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
LoRA ë²„ì „ê³¼ ì „ì²´ í•™ìŠµ ë²„ì „ì„ ëª¨ë‘ ì§€ì›í•©ë‹ˆë‹¤.

ì£¼ìš” ê°œì„ ì‚¬í•­:
1. MS Swift ìŠ¤íƒ€ì¼ ì„¤ì • ì§€ì›
2. LoRA/QLoRA/Full í•™ìŠµ ëª¨ë“œ ì„ íƒ
3. ìë™ ë””ë°”ì´ìŠ¤ ìµœì í™” (MPS/CUDA/CPU)
4. ë©”ëª¨ë¦¬ íš¨ìœ¨ì  í•™ìŠµ
5. ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
6. Wandb í†µí•©

ì‚¬ìš©ë²•:
    # LoRA ë²„ì „ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
    trainer = EnhancedVLMGRPOSystem(train_type="lora", lora_rank=8)
    
    # ì „ì²´ í•™ìŠµ ë²„ì „ (ê³ ì„±ëŠ¥)
    trainer = EnhancedVLMGRPOSystem(train_type="full", use_deepspeed=True)

Author: AI Assistant (Based on MS Swift CoZ GRPO)
Date: 2025-01-22
"""

import torch
import torch.nn as nn
import json
import logging
import time
from pathlib import Path
from typing import Dict, List, Optional, Any, Literal
from dataclasses import dataclass, field
import numpy as np

# ê¸°ì¡´ ëª¨ë“ˆë“¤ ì„í¬íŠ¸
from ..models.vlm_wrapper import VLMWrapper
from ..models.sd_generator import SD3Generator
from ..models.clip_reward import CLIPRewardCalculator
from ..training.grpo_trainer import GRPOTrainer, GRPOConfig
from ..utils.data_loader import VLMDataLoader
from ..evaluation.validator import VLMValidator
from ..integration.wandb_logger import WandbLogger

# LoRA ê´€ë ¨ ì„í¬íŠ¸ (ì„ íƒì )
try:
    from peft import (
        LoraConfig, 
        get_peft_model, 
        TaskType,
        PeftModel,
        prepare_model_for_kbit_training
    )
    PEFT_AVAILABLE = True
except ImportError:
    PEFT_AVAILABLE = False

logger = logging.getLogger(__name__)

@dataclass
class EnhancedVLMGRPOConfig:
    """
    ê°œì„ ëœ VLM GRPO ì‹œìŠ¤í…œ ì„¤ì •
    
    MS Swift ìŠ¤íƒ€ì¼ ì„¤ì •ì„ í¬í•¨í•˜ì—¬ LoRAì™€ ì „ì²´ í•™ìŠµì„ ëª¨ë‘ ì§€ì›í•©ë‹ˆë‹¤.
    """
    # ê¸°ë³¸ ëª¨ë¸ ì„¤ì •
    vlm_model: str = "microsoft/DialoGPT-medium"
    sd_model: str = "stabilityai/stable-diffusion-3-medium"
    clip_model: str = "openai/clip-vit-base-patch32"
    
    # í•™ìŠµ íƒ€ì… ì„¤ì • (MS Swift ìŠ¤íƒ€ì¼)
    train_type: Literal["full", "lora", "qlora"] = "lora"
    
    # LoRA ì„¤ì • (MS Swift ê¸°ë³¸ê°’)
    lora_rank: int = 8
    lora_alpha: int = 32
    lora_dropout: float = 0.1
    target_modules: str = "all-linear"  # MS Swift ìŠ¤íƒ€ì¼
    
    # í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°
    learning_rate: float = 1e-5
    num_iterations: int = 20
    group_size: int = 4
    per_device_train_batch_size: int = 4
    gradient_accumulation_steps: int = 1
    max_grad_norm: float = 1.0
    warmup_ratio: float = 0.05
    
    # GRPO íŠ¹í™” ì„¤ì •
    num_generations: int = 4
    temperature: float = 0.8
    top_p: float = 0.9
    grpo_epochs: int = 2
    kl_beta: float = 0.01
    clip_epsilon: float = 0.2
    
    # ë°ì´í„° ì„¤ì •
    train_data_path: str = "train_prompts.jsonl"
    val_data_path: str = "val_prompts.jsonl"
    max_length: int = 2048
    max_completion_length: int = 1024
    
    # í‰ê°€ ë° ì €ì¥ ì„¤ì •
    validation_interval: int = 5
    checkpoint_interval: int = 10
    eval_steps: int = 100
    save_steps: int = 100
    save_total_limit: int = 2
    logging_steps: int = 5
    
    # ì¶œë ¥ ì„¤ì •
    output_dir: str = "vlm_grpo_results"
    log_completions: bool = True
    
    # ë¶„ì‚° í•™ìŠµ ì„¤ì •
    use_deepspeed: bool = False
    deepspeed_config: str = "zero2"  # "zero2", "zero3"
    
    # í•˜ë“œì›¨ì–´ ìµœì í™”
    device: str = "auto"
    torch_dtype: str = "bfloat16"
    use_flash_attention: bool = True
    gradient_checkpointing: bool = True
    
    # ë³´ìƒ í•¨ìˆ˜ ì„¤ì •
    reward_weights: Dict[str, float] = field(default_factory=lambda: {
        "clip_similarity": 0.6,
        "image_quality": 0.3,
        "semantic_consistency": 0.1
    })
    
    # ì‹¤í—˜ ì¶”ì 
    use_wandb: bool = True
    wandb_project: str = "vlm-grpo-enhanced"
    wandb_run_name: Optional[str] = None
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    system_prompt: Optional[str] = None

class EnhancedVLMGRPOSystem:
    """
    ê°œì„ ëœ VLM GRPO í†µí•© ì‹œìŠ¤í…œ
    
    MS Swift CoZ GRPOë¥¼ ì°¸ì¡°í•˜ì—¬ êµ¬í˜„ëœ ê°œì„ ëœ ì‹œìŠ¤í…œìœ¼ë¡œ,
    LoRAì™€ ì „ì²´ í•™ìŠµì„ ëª¨ë‘ ì§€ì›í•©ë‹ˆë‹¤.
    
    ì£¼ìš” íŠ¹ì§•:
    1. MS Swift ìŠ¤íƒ€ì¼ ì„¤ì •
    2. ìë™ ë©”ëª¨ë¦¬ ìµœì í™”
    3. ìœ ì—°í•œ í•™ìŠµ ëª¨ë“œ ì„ íƒ
    4. ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
    5. ì™„ì „í•œ ì‹¤í—˜ ì¶”ì 
    """
    
    def __init__(self, config_path: Optional[str] = None, **kwargs):
        """
        Enhanced VLM GRPO System ì´ˆê¸°í™”
        
        Args:
            config_path (Optional[str]): ì„¤ì • íŒŒì¼ ê²½ë¡œ
            **kwargs: ì§ì ‘ ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ
        """
        # ì„¤ì • ë¡œë“œ
        self.config = self._load_config(config_path, **kwargs)
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.vlm = None
        self.sd_generator = None
        self.clip_calculator = None
        self.grpo_trainer = None
        self.data_loader = None
        self.validator = None
        self.wandb_logger = None
        
        # í•™ìŠµ ìƒíƒœ
        self.training_stats = {
            'current_iteration': 0,
            'total_samples': 0,
            'best_reward': 0.0,
            'training_time': 0.0
        }
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self._setup_device()
        
        logger.info("ğŸš€ Enhanced VLM GRPO System Initialized")
        self._print_system_info()
    
    def _load_config(self, config_path: Optional[str], **kwargs) -> EnhancedVLMGRPOConfig:
        """ì„¤ì • ë¡œë“œ ë° ì˜¤ë²„ë¼ì´ë“œ ì ìš©"""
        if config_path and Path(config_path).exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                config_dict = json.load(f)
            config = EnhancedVLMGRPOConfig(**config_dict)
        else:
            config = EnhancedVLMGRPOConfig()
        
        # kwargsë¡œ ì˜¤ë²„ë¼ì´ë“œ
        for key, value in kwargs.items():
            if hasattr(config, key):
                setattr(config, key, value)
        
        return config
    
    def _setup_device(self):
        """ë””ë°”ì´ìŠ¤ ìë™ ì„¤ì •"""
        if self.config.device == "auto":
            if torch.backends.mps.is_available():
                self.device = torch.device("mps")
                logger.info("ğŸ Using Apple Silicon MPS")
            elif torch.cuda.is_available():
                self.device = torch.device("cuda")
                logger.info("ğŸš€ Using CUDA GPU")
            else:
                self.device = torch.device("cpu")
                logger.info("ğŸ’» Using CPU")
        else:
            self.device = torch.device(self.config.device)
    
    def _print_system_info(self):
        """ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥"""
        logger.info("=" * 70)
        logger.info("ğŸ”§ Enhanced VLM GRPO System Configuration")
        logger.info("=" * 70)
        logger.info(f"ğŸ“‹ Training Configuration:")
        logger.info(f"   - Train Type: {self.config.train_type}")
        logger.info(f"   - VLM Model: {self.config.vlm_model}")
        logger.info(f"   - Device: {self.device}")
        logger.info(f"   - Torch Dtype: {self.config.torch_dtype}")
        
        if self.config.train_type in ["lora", "qlora"]:
            logger.info(f"ğŸ¯ LoRA Configuration:")
            logger.info(f"   - LoRA Rank: {self.config.lora_rank}")
            logger.info(f"   - LoRA Alpha: {self.config.lora_alpha}")
            logger.info(f"   - Target Modules: {self.config.target_modules}")
        
        logger.info(f"âš™ï¸ Training Parameters:")
        logger.info(f"   - Learning Rate: {self.config.learning_rate}")
        logger.info(f"   - Batch Size: {self.config.per_device_train_batch_size}")
        logger.info(f"   - Num Iterations: {self.config.num_iterations}")
        logger.info(f"   - Group Size: {self.config.group_size}")
        
        if self.config.use_deepspeed:
            logger.info(f"âš¡ DeepSpeed: {self.config.deepspeed_config}")
        
        logger.info(f"ğŸ“Š Experiment Tracking:")
        logger.info(f"   - Wandb: {self.config.use_wandb}")
        logger.info(f"   - Output Dir: {self.config.output_dir}")
        logger.info("=" * 70)
    
    def initialize_components(self):
        """ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        logger.info("ğŸ”§ Initializing system components...")
        
        try:
            # 1. VLM ì´ˆê¸°í™” (LoRA ë˜ëŠ” ì „ì²´ í•™ìŠµ)
            self._initialize_vlm()
            
            # 2. SD3 Generator ì´ˆê¸°í™”
            self._initialize_sd_generator()
            
            # 3. CLIP Reward Calculator ì´ˆê¸°í™”
            self._initialize_clip_calculator()
            
            # 4. GRPO Trainer ì´ˆê¸°í™”
            self._initialize_grpo_trainer()
            
            # 5. Data Loader ì´ˆê¸°í™”
            self._initialize_data_loader()
            
            # 6. Validator ì´ˆê¸°í™”
            self._initialize_validator()
            
            # 7. Wandb Logger ì´ˆê¸°í™”
            if self.config.use_wandb:
                self._initialize_wandb_logger()
            
            logger.info("âœ… All components initialized successfully!")
            
        except Exception as e:
            logger.error(f"âŒ Component initialization failed: {e}")
            raise
    
    def _initialize_vlm(self):
        """VLM ì´ˆê¸°í™” (LoRA ë˜ëŠ” ì „ì²´ í•™ìŠµ ì§€ì›)"""
        logger.info(f"ğŸ“¥ Initializing VLM: {self.config.vlm_model}")
        
        # ê¸°ë³¸ VLM ë˜í¼ ìƒì„±
        self.vlm = VLMWrapper(
            model_name=self.config.vlm_model,
            device=str(self.device),
            max_new_tokens=self.config.max_completion_length,
            temperature=self.config.temperature,
            top_p=self.config.top_p
        )
        
        # LoRA ì„¤ì • ì ìš©
        if self.config.train_type in ["lora", "qlora"]:
            self._apply_lora_to_vlm()
        
        logger.info(f"âœ… VLM initialized with {self.config.train_type} training")
    
    def _apply_lora_to_vlm(self):
        """VLMì— LoRA ì ìš©"""
        if not PEFT_AVAILABLE:
            logger.warning("âš ï¸ PEFT not available. Falling back to full training.")
            self.config.train_type = "full"
            return
        
        try:
            # LoRA ì„¤ì •
            target_modules = self._get_target_modules()
            
            lora_config = LoraConfig(
                task_type=TaskType.CAUSAL_LM,
                r=self.config.lora_rank,
                lora_alpha=self.config.lora_alpha,
                lora_dropout=self.config.lora_dropout,
                target_modules=target_modules,
                bias="none",
                inference_mode=False
            )
            
            # QLoRA ì „ì²˜ë¦¬
            if self.config.train_type == "qlora":
                self.vlm.model = prepare_model_for_kbit_training(
                    self.vlm.model,
                    use_gradient_checkpointing=self.config.gradient_checkpointing
                )
            
            # LoRA ëª¨ë¸ ìƒì„±
            self.vlm.model = get_peft_model(self.vlm.model, lora_config)
            
            # íŒŒë¼ë¯¸í„° í†µê³„
            trainable_params = sum(p.numel() for p in self.vlm.model.parameters() if p.requires_grad)
            total_params = sum(p.numel() for p in self.vlm.model.parameters())
            
            logger.info(f"ğŸ¯ LoRA applied successfully:")
            logger.info(f"   - Trainable params: {trainable_params:,}")
            logger.info(f"   - Total params: {total_params:,}")
            logger.info(f"   - Trainable ratio: {100 * trainable_params / total_params:.2f}%")
            
        except Exception as e:
            logger.error(f"âŒ LoRA application failed: {e}")
            logger.warning("âš ï¸ Falling back to full training")
            self.config.train_type = "full"
    
    def _get_target_modules(self) -> List[str]:
        """MS Swift ìŠ¤íƒ€ì¼ target_modules ì²˜ë¦¬"""
        if self.config.target_modules == "all-linear":
            # ëª¨ë“  ì„ í˜• ë ˆì´ì–´ ì°¾ê¸°
            linear_modules = []
            for name, module in self.vlm.model.named_modules():
                if isinstance(module, nn.Linear):
                    module_name = name.split('.')[-1]
                    if module_name not in linear_modules:
                        linear_modules.append(module_name)
            
            # ì¼ë°˜ì ì¸ Transformer ëª¨ë“ˆë“¤
            common_modules = [
                "q_proj", "k_proj", "v_proj", "o_proj",
                "gate_proj", "up_proj", "down_proj",
                "fc1", "fc2", "c_attn", "c_proj"
            ]
            
            # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ëª¨ë“ˆë§Œ ë°˜í™˜
            found_modules = [m for m in common_modules if m in linear_modules]
            return found_modules or linear_modules
        else:
            return [self.config.target_modules]
    
    def _initialize_sd_generator(self):
        """SD3 Generator ì´ˆê¸°í™”"""
        logger.info(f"ğŸ¨ Initializing SD3 Generator: {self.config.sd_model}")
        
        self.sd_generator = SD3Generator(
            model_name=self.config.sd_model,
            device=str(self.device),
            torch_dtype=self.config.torch_dtype,
            enable_memory_efficient_attention=True,
            enable_attention_slicing=True
        )
        
        logger.info("âœ… SD3 Generator initialized")
    
    def _initialize_clip_calculator(self):
        """CLIP Reward Calculator ì´ˆê¸°í™”"""
        logger.info(f"ğŸ” Initializing CLIP Calculator: {self.config.clip_model}")
        
        self.clip_calculator = CLIPRewardCalculator(
            model_name=self.config.clip_model,
            device=str(self.device),
            reward_weights=self.config.reward_weights
        )
        
        logger.info("âœ… CLIP Calculator initialized")
    
    def _initialize_grpo_trainer(self):
        """GRPO Trainer ì´ˆê¸°í™”"""
        logger.info("ğŸ¯ Initializing GRPO Trainer...")
        
        # GRPO ì„¤ì • ìƒì„±
        grpo_config = GRPOConfig(
            learning_rate=self.config.learning_rate,
            group_size=self.config.group_size,
            num_iterations=self.config.num_iterations,
            grpo_epochs=self.config.grpo_epochs,
            kl_beta=self.config.kl_beta,
            clip_epsilon=self.config.clip_epsilon,
            max_grad_norm=self.config.max_grad_norm,
            max_new_tokens=self.config.max_completion_length,
            temperature=self.config.temperature,
            device=str(self.device)
        )
        
        self.grpo_trainer = GRPOTrainer(
            vlm_model=self.vlm,
            config=grpo_config
        )
        
        logger.info("âœ… GRPO Trainer initialized")
    
    def _initialize_data_loader(self):
        """Data Loader ì´ˆê¸°í™”"""
        logger.info("ğŸ“Š Initializing Data Loader...")
        
        self.data_loader = VLMDataLoader(
            train_path=self.config.train_data_path,
            val_path=self.config.val_data_path,
            batch_size=self.config.per_device_train_batch_size
        )
        
        logger.info("âœ… Data Loader initialized")
    
    def _initialize_validator(self):
        """Validator ì´ˆê¸°í™”"""
        logger.info("ğŸ”¬ Initializing Validator...")
        
        self.validator = VLMValidator(
            vlm=self.vlm,
            sd_generator=self.sd_generator,
            clip_calculator=self.clip_calculator,
            validation_interval=self.config.validation_interval
        )
        
        logger.info("âœ… Validator initialized")
    
    def _initialize_wandb_logger(self):
        """Wandb Logger ì´ˆê¸°í™”"""
        logger.info("ğŸ“ˆ Initializing Wandb Logger...")
        
        # ì‹¤í–‰ ì´ë¦„ ìƒì„±
        run_name = self.config.wandb_run_name or f"vlm-grpo-{self.config.train_type}-{int(time.time())}"
        
        # Wandb ì„¤ì •
        wandb_config = {
            "train_type": self.config.train_type,
            "vlm_model": self.config.vlm_model,
            "learning_rate": self.config.learning_rate,
            "group_size": self.config.group_size,
            "num_iterations": self.config.num_iterations,
            "device": str(self.device)
        }
        
        # LoRA ì„¤ì • ì¶”ê°€
        if self.config.train_type in ["lora", "qlora"]:
            wandb_config.update({
                "lora_rank": self.config.lora_rank,
                "lora_alpha": self.config.lora_alpha,
                "target_modules": self.config.target_modules
            })
        
        self.wandb_logger = WandbLogger(
            project_name=self.config.wandb_project,
            run_name=run_name,
            config=wandb_config
        )
        
        logger.info("âœ… Wandb Logger initialized")
    
    def run_training(self):
        """ë©”ì¸ í•™ìŠµ ì‹¤í–‰"""
        logger.info("ğŸš€ Starting Enhanced VLM GRPO Training...")
        logger.info(f"   - Train Type: {self.config.train_type}")
        logger.info(f"   - Iterations: {self.config.num_iterations}")
        logger.info(f"   - Group Size: {self.config.group_size}")
        
        start_time = time.time()
        
        try:
            for iteration in range(1, self.config.num_iterations + 1):
                self.training_stats['current_iteration'] = iteration
                
                # í•™ìŠµ ë°°ì¹˜ ìƒì„±
                train_batch = self.data_loader.get_train_batch(
                    group_size=self.config.group_size
                )
                
                # GRPO í•™ìŠµ ìŠ¤í…
                step_stats = self._run_training_step(train_batch)
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                self._update_training_stats(step_stats)
                
                # ë¡œê¹…
                if iteration % self.config.logging_steps == 0:
                    self._log_training_progress(iteration, step_stats)
                
                # ê²€ì¦
                if iteration % self.config.validation_interval == 0:
                    val_stats = self._run_validation()
                    self._log_validation_results(iteration, val_stats)
                
                # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
                if iteration % self.config.checkpoint_interval == 0:
                    self._save_checkpoint(iteration)
                
                # Wandb ë¡œê¹…
                if self.wandb_logger:
                    self._log_to_wandb(iteration, step_stats)
            
            # ìµœì¢… ê²°ê³¼ ì €ì¥
            self._save_final_results()
            
            total_time = time.time() - start_time
            self.training_stats['training_time'] = total_time
            
            logger.info("ğŸ‰ Training completed successfully!")
            logger.info(f"   - Total time: {total_time:.2f}s")
            logger.info(f"   - Best reward: {self.training_stats['best_reward']:.4f}")
            
        except KeyboardInterrupt:
            logger.info("âš ï¸ Training interrupted by user")
            self._save_checkpoint(self.training_stats['current_iteration'], final=True)
            
        except Exception as e:
            logger.error(f"âŒ Training failed: {e}")
            raise
        
        finally:
            if self.wandb_logger:
                self.wandb_logger.finish()
    
    def _run_training_step(self, train_batch: List[str]) -> Dict[str, float]:
        """ë‹¨ì¼ í•™ìŠµ ìŠ¤í… ì‹¤í–‰"""
        # GRPO í•™ìŠµ ìŠ¤í… ì‹¤í–‰
        step_stats = self.grpo_trainer.train_step(train_batch)
        
        # ìƒ˜í”Œ ìˆ˜ ì—…ë°ì´íŠ¸
        self.training_stats['total_samples'] += len(train_batch)
        
        return step_stats
    
    def _update_training_stats(self, step_stats: Dict[str, float]):
        """í•™ìŠµ í†µê³„ ì—…ë°ì´íŠ¸"""
        if step_stats['avg_reward'] > self.training_stats['best_reward']:
            self.training_stats['best_reward'] = step_stats['avg_reward']
    
    def _log_training_progress(self, iteration: int, stats: Dict[str, float]):
        """í•™ìŠµ ì§„í–‰ ìƒí™© ë¡œê¹…"""
        logger.info(f"ğŸ“Š Iteration {iteration}/{self.config.num_iterations}")
        logger.info(f"   - Avg Reward: {stats['avg_reward']:.4f}")
        logger.info(f"   - Policy Loss: {stats['policy_loss']:.4f}")
        logger.info(f"   - KL Divergence: {stats['kl_divergence']:.4f}")
        logger.info(f"   - Entropy: {stats['entropy']:.4f}")
        logger.info(f"   - Grad Norm: {stats['grad_norm']:.4f}")
    
    def _run_validation(self) -> Dict[str, Any]:
        """ê²€ì¦ ì‹¤í–‰"""
        logger.info("ğŸ”¬ Running validation...")
        
        val_batch = self.data_loader.get_val_batch()
        val_stats = self.validator.validate(val_batch)
        
        logger.info(f"âœ… Validation completed: {val_stats['success_rate']:.2%}")
        return val_stats
    
    def _log_validation_results(self, iteration: int, val_stats: Dict[str, Any]):
        """ê²€ì¦ ê²°ê³¼ ë¡œê¹…"""
        logger.info(f"ğŸ”¬ Validation Results (Iteration {iteration}):")
        logger.info(f"   - Success Rate: {val_stats['success_rate']:.2%}")
        logger.info(f"   - Avg Quality: {val_stats['avg_quality_score']:.4f}")
        logger.info(f"   - Avg Similarity: {val_stats['avg_similarity_score']:.4f}")
    
    def _save_checkpoint(self, iteration: int, final: bool = False):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        checkpoint_dir = Path(self.config.output_dir) / f"checkpoint_iter_{iteration}"
        checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        # ëª¨ë¸ ì €ì¥ (LoRA ë˜ëŠ” ì „ì²´ ëª¨ë¸)
        if self.config.train_type in ["lora", "qlora"]:
            # LoRA ì–´ëŒ‘í„°ë§Œ ì €ì¥
            self.vlm.model.save_pretrained(checkpoint_dir)
            logger.info(f"ğŸ’¾ LoRA checkpoint saved: {checkpoint_dir}")
        else:
            # ì „ì²´ ëª¨ë¸ ì €ì¥
            torch.save(self.vlm.model.state_dict(), checkpoint_dir / "model.pt")
            logger.info(f"ğŸ’¾ Full model checkpoint saved: {checkpoint_dir}")
        
        # í•™ìŠµ í†µê³„ ì €ì¥
        with open(checkpoint_dir / "training_stats.json", 'w') as f:
            json.dump(self.training_stats, f, indent=2)
        
        # ì„¤ì • ì €ì¥
        with open(checkpoint_dir / "config.json", 'w') as f:
            json.dump(self.config.__dict__, f, indent=2, default=str)
        
        if final:
            logger.info(f"ğŸ’¾ Final checkpoint saved: {checkpoint_dir}")
    
    def _log_to_wandb(self, iteration: int, stats: Dict[str, float]):
        """Wandb ë¡œê¹…"""
        if self.wandb_logger:
            log_data = {
                "iteration": iteration,
                "train/avg_reward": stats['avg_reward'],
                "train/policy_loss": stats['policy_loss'],
                "train/kl_divergence": stats['kl_divergence'],
                "train/entropy": stats['entropy'],
                "train/grad_norm": stats['grad_norm'],
                "system/total_samples": self.training_stats['total_samples']
            }
            
            self.wandb_logger.log_metrics(log_data)
    
    def _save_final_results(self):
        """ìµœì¢… ê²°ê³¼ ì €ì¥"""
        results_dir = Path(self.config.output_dir)
        results_dir.mkdir(parents=True, exist_ok=True)
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
        if self.config.train_type in ["lora", "qlora"]:
            self.vlm.model.save_pretrained(results_dir / "best_model")
        else:
            torch.save(self.vlm.model.state_dict(), results_dir / "best_model.pt")
        
        # ìµœì¢… í†µê³„ ì €ì¥
        final_stats = {
            **self.training_stats,
            "config": self.config.__dict__,
            "train_type": self.config.train_type,
            "total_parameters": sum(p.numel() for p in self.vlm.model.parameters()),
            "trainable_parameters": sum(p.numel() for p in self.vlm.model.parameters() if p.requires_grad)
        }
        
        with open(results_dir / "final_results.json", 'w') as f:
            json.dump(final_stats, f, indent=2, default=str)
        
        logger.info(f"ğŸ’¾ Final results saved to: {results_dir}")
    
    def get_training_stats(self) -> Dict[str, Any]:
        """í•™ìŠµ í†µê³„ ë°˜í™˜"""
        return self.training_stats.copy()
    
    def print_system_summary(self):
        """ì‹œìŠ¤í…œ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "=" * 70)
        print("ğŸ” Enhanced VLM GRPO System Summary")
        print("=" * 70)
        print(f"Train Type: {self.config.train_type}")
        print(f"VLM Model: {self.config.vlm_model}")
        print(f"Device: {self.device}")
        
        if self.vlm and hasattr(self.vlm, 'model'):
            total_params = sum(p.numel() for p in self.vlm.model.parameters())
            trainable_params = sum(p.numel() for p in self.vlm.model.parameters() if p.requires_grad)
            print(f"Total Parameters: {total_params:,}")
            print(f"Trainable Parameters: {trainable_params:,}")
            print(f"Trainable Ratio: {100 * trainable_params / total_params:.2f}%")
        
        if self.config.train_type in ["lora", "qlora"]:
            print(f"LoRA Rank: {self.config.lora_rank}")
            print(f"LoRA Alpha: {self.config.lora_alpha}")
        
        print(f"Learning Rate: {self.config.learning_rate}")
        print(f"Batch Size: {self.config.per_device_train_batch_size}")
        print(f"Iterations: {self.config.num_iterations}")
        print(f"Output Directory: {self.config.output_dir}")
        print("=" * 70)


# í¸ì˜ í•¨ìˆ˜ë“¤
def create_lora_trainer(lora_rank: int = 8, lora_alpha: int = 32, **kwargs) -> EnhancedVLMGRPOSystem:
    """LoRA íŠ¸ë ˆì´ë„ˆ ìƒì„± í¸ì˜ í•¨ìˆ˜"""
    config = EnhancedVLMGRPOConfig(
        train_type="lora",
        lora_rank=lora_rank,
        lora_alpha=lora_alpha,
        **kwargs
    )
    return EnhancedVLMGRPOSystem(config=config)

def create_full_trainer(use_deepspeed: bool = True, **kwargs) -> EnhancedVLMGRPOSystem:
    """ì „ì²´ í•™ìŠµ íŠ¸ë ˆì´ë„ˆ ìƒì„± í¸ì˜ í•¨ìˆ˜"""
    config = EnhancedVLMGRPOConfig(
        train_type="full",
        use_deepspeed=use_deepspeed,
        learning_rate=1e-6,  # ì „ì²´ í•™ìŠµì€ ë” ì‘ì€ í•™ìŠµë¥  ì‚¬ìš©
        **kwargs
    )
    return EnhancedVLMGRPOSystem(config=config)


# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    print("âœ… Enhanced VLM GRPO System Ready!")
    print("\nğŸ“‹ Available Training Modes:")
    print("1. LoRA Training (Memory Efficient)")
    print("   trainer = create_lora_trainer(lora_rank=8)")
    print("\n2. Full Training (High Performance)")
    print("   trainer = create_full_trainer(use_deepspeed=True)")
    print("\n3. Custom Configuration")
    print("   trainer = EnhancedVLMGRPOSystem(train_type='lora', lora_rank=16)") 