"""
Validation Evaluator
===================

VLM GRPO í•™ìŠµì˜ ê²€ì¦ ë° í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ëŠ” ëª¨ë“ˆì…ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
1. ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
2. ë‹¤ì–‘í•œ í‰ê°€ ë©”íŠ¸ë¦­
3. ì¹´í…Œê³ ë¦¬/ë‚œì´ë„ë³„ ë¶„ì„
4. ì‹œê°í™” ë° ë¦¬í¬íŠ¸ ìƒì„±

Author: AI Assistant
Date: 2025-01-22
"""

import logging
from typing import List, Dict, Any, Optional
import numpy as np
import time

logger = logging.getLogger(__name__)

class ValidationEvaluator:
    """
    ê²€ì¦ í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ëŠ” í´ë˜ìŠ¤
    
    ì´ í´ë˜ìŠ¤ëŠ”:
    1. VLM ì„±ëŠ¥ í‰ê°€
    2. ì´ë¯¸ì§€ ìƒì„± í’ˆì§ˆ í‰ê°€
    3. ì „ì²´ íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥ ì¸¡ì •
    4. ìƒì„¸í•œ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±
    """
    
    def __init__(self, vlm, sd_generator, clip_calculator):
        """
        Validator ì´ˆê¸°í™”
        
        Args:
            vlm: VLM ëª¨ë¸
            sd_generator: SD3 ìƒì„±ê¸°
            clip_calculator: CLIP ë³´ìƒ ê³„ì‚°ê¸°
        """
        self.vlm = vlm
        self.sd_generator = sd_generator
        self.clip_calculator = clip_calculator
        
        # í‰ê°€ í†µê³„
        self.evaluation_history = []
        
        logger.info("âœ… Validation Evaluator initialized")
    
    def evaluate_batch(self, validation_data: List[Dict]) -> Dict[str, Any]:
        """
        ë°°ì¹˜ ë°ì´í„°ì— ëŒ€í•œ ì¢…í•© í‰ê°€
        
        Args:
            validation_data (List[Dict]): ê²€ì¦ ë°ì´í„°
            
        Returns:
            Dict[str, Any]: í‰ê°€ ê²°ê³¼
        """
        logger.info(f"ğŸ” Evaluating batch of {len(validation_data)} items")
        
        results = {
            'timestamp': time.time(),
            'total_samples': len(validation_data),
            'success_rate': 0.0,
            'avg_clip_score': 0.0,
            'quality_score': 0.0,
            'processing_time': 0.0,
            'category_results': {},
            'difficulty_results': {},
            'detailed_results': []
        }
        
        start_time = time.time()
        successful_evaluations = 0
        clip_scores = []
        quality_scores = []
        
        for item in validation_data:
            try:
                # ê°œë³„ ì•„ì´í…œ í‰ê°€
                item_result = self._evaluate_single_item(item)
                results['detailed_results'].append(item_result)
                
                if item_result['success']:
                    successful_evaluations += 1
                    clip_scores.append(item_result['clip_score'])
                    quality_scores.append(item_result['quality_score'])
                
                # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
                category = item.get('category', 'unknown')
                if category not in results['category_results']:
                    results['category_results'][category] = {'count': 0, 'success': 0}
                results['category_results'][category]['count'] += 1
                if item_result['success']:
                    results['category_results'][category]['success'] += 1
                
                # ë‚œì´ë„ë³„ í†µê³„
                difficulty = item.get('difficulty', 'unknown')
                if difficulty not in results['difficulty_results']:
                    results['difficulty_results'][difficulty] = {'count': 0, 'success': 0}
                results['difficulty_results'][difficulty]['count'] += 1
                if item_result['success']:
                    results['difficulty_results'][difficulty]['success'] += 1
                
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to evaluate item: {e}")
                results['detailed_results'].append({
                    'prompt': item.get('user_prompt', ''),
                    'success': False,
                    'error': str(e)
                })
        
        # ì „ì²´ í†µê³„ ê³„ì‚°
        results['success_rate'] = successful_evaluations / len(validation_data)
        results['avg_clip_score'] = np.mean(clip_scores) if clip_scores else 0.0
        results['quality_score'] = np.mean(quality_scores) if quality_scores else 0.0
        results['processing_time'] = time.time() - start_time
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì„±ê³µë¥  ê³„ì‚°
        for category, stats in results['category_results'].items():
            stats['success_rate'] = stats['success'] / stats['count'] if stats['count'] > 0 else 0
        
        # ë‚œì´ë„ë³„ ì„±ê³µë¥  ê³„ì‚°
        for difficulty, stats in results['difficulty_results'].items():
            stats['success_rate'] = stats['success'] / stats['count'] if stats['count'] > 0 else 0
        
        # íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        self.evaluation_history.append(results)
        
        logger.info(f"âœ… Evaluation completed: {results['success_rate']:.2%} success rate")
        return results
    
    def _evaluate_single_item(self, item: Dict) -> Dict[str, Any]:
        """
        ë‹¨ì¼ ì•„ì´í…œ í‰ê°€
        
        Args:
            item (Dict): í‰ê°€í•  ì•„ì´í…œ
            
        Returns:
            Dict[str, Any]: í‰ê°€ ê²°ê³¼
        """
        user_prompt = item.get('user_prompt', '')
        category = item.get('category', 'unknown')
        difficulty = item.get('difficulty', 'unknown')
        
        result = {
            'prompt': user_prompt,
            'category': category,
            'difficulty': difficulty,
            'success': False,
            'enhanced_prompt': '',
            'clip_score': 0.0,
            'quality_score': 0.0,
            'improvement_length': 0,
            'processing_time': 0.0
        }
        
        try:
            start_time = time.time()
            
            # 1. VLMìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ê°œì„ 
            enhanced_prompt = self.vlm.enhance_prompt(user_prompt)
            result['enhanced_prompt'] = enhanced_prompt
            result['improvement_length'] = len(enhanced_prompt) - len(user_prompt)
            
            # 2. SD3ë¡œ ì´ë¯¸ì§€ ìƒì„±
            image = self.sd_generator.generate_image(enhanced_prompt)
            
            # 3. CLIP ì ìˆ˜ ê³„ì‚°
            clip_score = self.clip_calculator.calculate_reward(image, enhanced_prompt)
            result['clip_score'] = clip_score
            
            # 4. í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            quality_score = self.clip_calculator.calculate_quality_reward(image)
            result['quality_score'] = quality_score
            
            # 5. ì²˜ë¦¬ ì‹œê°„ ê¸°ë¡
            result['processing_time'] = time.time() - start_time
            
            # ì„±ê³µ ê¸°ì¤€: CLIP ì ìˆ˜ê°€ ì„ê³„ê°’ ì´ìƒ
            if clip_score > 0.3:  # ì„ê³„ê°’ì€ ì¡°ì • ê°€ëŠ¥
                result['success'] = True
            
        except Exception as e:
            result['error'] = str(e)
            logger.warning(f"âš ï¸ Single item evaluation failed: {e}")
        
        return result
    
    def get_evaluation_summary(self) -> Dict[str, Any]:
        """
        ì „ì²´ í‰ê°€ íˆìŠ¤í† ë¦¬ ìš”ì•½
        
        Returns:
            Dict[str, Any]: í‰ê°€ ìš”ì•½
        """
        if not self.evaluation_history:
            return {'message': 'No evaluation history available'}
        
        # ìµœì‹  í‰ê°€ ê²°ê³¼
        latest = self.evaluation_history[-1]
        
        # ì„±ëŠ¥ íŠ¸ë Œë“œ ê³„ì‚°
        success_rates = [eval_result['success_rate'] for eval_result in self.evaluation_history]
        clip_scores = [eval_result['avg_clip_score'] for eval_result in self.evaluation_history]
        
        summary = {
            'total_evaluations': len(self.evaluation_history),
            'latest_success_rate': latest['success_rate'],
            'latest_clip_score': latest['avg_clip_score'],
            'avg_success_rate': np.mean(success_rates),
            'avg_clip_score': np.mean(clip_scores),
            'success_rate_trend': self._calculate_trend(success_rates),
            'clip_score_trend': self._calculate_trend(clip_scores),
            'best_evaluation': max(self.evaluation_history, key=lambda x: x['success_rate']),
            'category_performance': latest.get('category_results', {}),
            'difficulty_performance': latest.get('difficulty_results', {})
        }
        
        return summary
    
    def _calculate_trend(self, values: List[float]) -> str:
        """
        ê°’ë“¤ì˜ íŠ¸ë Œë“œ ê³„ì‚°
        
        Args:
            values (List[float]): ê°’ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            str: íŠ¸ë Œë“œ ("improving", "declining", "stable")
        """
        if len(values) < 3:
            return "insufficient_data"
        
        recent_avg = np.mean(values[-3:])
        earlier_avg = np.mean(values[:-3]) if len(values) > 3 else np.mean(values[:3])
        
        change = (recent_avg - earlier_avg) / earlier_avg if earlier_avg > 0 else 0
        
        if change > 0.05:
            return "improving"
        elif change < -0.05:
            return "declining"
        else:
            return "stable"
    
    def generate_report(self, save_path: Optional[str] = None) -> str:
        """
        ìƒì„¸í•œ í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„±
        
        Args:
            save_path (str, optional): ë¦¬í¬íŠ¸ ì €ì¥ ê²½ë¡œ
            
        Returns:
            str: ìƒì„±ëœ ë¦¬í¬íŠ¸ í…ìŠ¤íŠ¸
        """
        summary = self.get_evaluation_summary()
        
        report = f"""
VLM GRPO Validation Report
=========================

ğŸ“Š ì „ì²´ í†µê³„:
- ì´ í‰ê°€ íšŸìˆ˜: {summary.get('total_evaluations', 0)}
- ìµœì‹  ì„±ê³µë¥ : {summary.get('latest_success_rate', 0):.2%}
- í‰ê·  ì„±ê³µë¥ : {summary.get('avg_success_rate', 0):.2%}
- ìµœì‹  CLIP ì ìˆ˜: {summary.get('latest_clip_score', 0):.4f}
- í‰ê·  CLIP ì ìˆ˜: {summary.get('avg_clip_score', 0):.4f}

ğŸ“ˆ ì„±ëŠ¥ íŠ¸ë Œë“œ:
- ì„±ê³µë¥  íŠ¸ë Œë“œ: {summary.get('success_rate_trend', 'N/A')}
- CLIP ì ìˆ˜ íŠ¸ë Œë“œ: {summary.get('clip_score_trend', 'N/A')}

ğŸ“‚ ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥:
"""
        
        for category, stats in summary.get('category_performance', {}).items():
            success_rate = stats.get('success_rate', 0)
            count = stats.get('count', 0)
            report += f"- {category}: {success_rate:.2%} ({count} samples)\n"
        
        report += "\nğŸ¯ ë‚œì´ë„ë³„ ì„±ëŠ¥:\n"
        for difficulty, stats in summary.get('difficulty_performance', {}).items():
            success_rate = stats.get('success_rate', 0)
            count = stats.get('count', 0)
            report += f"- {difficulty}: {success_rate:.2%} ({count} samples)\n"
        
        # ë¦¬í¬íŠ¸ ì €ì¥
        if save_path:
            try:
                with open(save_path, 'w', encoding='utf-8') as f:
                    f.write(report)
                logger.info(f"ğŸ“‹ Report saved to {save_path}")
            except Exception as e:
                logger.error(f"âŒ Failed to save report: {e}")
        
        return report


if __name__ == "__main__":
    # Validator í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ğŸ§ª Validation Evaluator Test")
    print("=" * 35)
    
    try:
        # Mock ì»´í¬ë„ŒíŠ¸ë“¤
        class MockVLM:
            def enhance_prompt(self, prompt):
                return f"enhanced {prompt} with detailed description"
        
        class MockSDGenerator:
            def generate_image(self, prompt):
                return f"image_for_{prompt[:20]}"
        
        class MockCLIPCalculator:
            def calculate_reward(self, image, prompt):
                return 0.75  # ì„ì˜ì˜ ì ìˆ˜
            
            def calculate_quality_reward(self, image):
                return 0.65  # ì„ì˜ì˜ í’ˆì§ˆ ì ìˆ˜
        
        # Validator ì´ˆê¸°í™”
        validator = ValidationEvaluator(
            MockVLM(),
            MockSDGenerator(), 
            MockCLIPCalculator()
        )
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°
        test_data = [
            {"user_prompt": "a cat", "category": "basic", "difficulty": "easy"},
            {"user_prompt": "sunset", "category": "basic", "difficulty": "easy"},
            {"user_prompt": "abstract art", "category": "creative", "difficulty": "hard"}
        ]
        
        print("âœ… Validator initialized successfully")
        
        # ë°°ì¹˜ í‰ê°€ í…ŒìŠ¤íŠ¸
        print("\nğŸ”„ Testing batch evaluation:")
        results = validator.evaluate_batch(test_data)
        print(f"  Success rate: {results['success_rate']:.2%}")
        print(f"  Average CLIP score: {results['avg_clip_score']:.4f}")
        print(f"  Processing time: {results['processing_time']:.2f}s")
        
        # ìš”ì•½ í…ŒìŠ¤íŠ¸
        print("\nğŸ“Š Testing summary generation:")
        summary = validator.get_evaluation_summary()
        print(f"  Total evaluations: {summary['total_evaluations']}")
        print(f"  Success rate trend: {summary['success_rate_trend']}")
        
        # ë¦¬í¬íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸
        print("\nğŸ“‹ Testing report generation:")
        report = validator.generate_report()
        print("  Report generated successfully")
        print(f"  Report length: {len(report)} characters")
        
        print("\nâœ… All tests passed!")
        
    except Exception as e:
        print(f"âŒ Test failed: {e}")
    
    print("\nUsage:")
    print("from evaluation.validator import ValidationEvaluator")
    print("validator = ValidationEvaluator(vlm, sd_gen, clip_calc)")
    print("results = validator.evaluate_batch(validation_data)") 