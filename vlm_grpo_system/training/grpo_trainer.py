"""
GRPO (Group Relative Policy Optimization) Trainer
================================================

GRPO ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í•œ í•µì‹¬ í•™ìŠµ ëª¨ë“ˆì…ë‹ˆë‹¤.
VLM í”„ë¡¬í”„íŠ¸ ê°œì„ ì„ ìœ„í•œ ê°•í™”í•™ìŠµ íŠ¸ë ˆì´ë„ˆì…ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
1. GRPO ì •ì±… ì—…ë°ì´íŠ¸
2. ê·¸ë£¹ ê¸°ë°˜ ì–´ë“œë°´í‹°ì§€ ê³„ì‚°
3. KL ë°œì‚° í˜ë„í‹°
4. ì°¸ì¡° ëª¨ë¸ ê´€ë¦¬

GRPO vs PPO ì°¨ì´ì :
- PPO: ê°œë³„ ìƒ˜í”Œ ê¸°ë°˜ ì–´ë“œë°´í‹°ì§€
- GRPO: ê·¸ë£¹ ë‚´ ìƒëŒ€ì  ì–´ë“œë°´í‹°ì§€ (ë” ì•ˆì •ì )

Author: AI Assistant
Date: 2025-01-22
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from typing import List, Dict, Tuple, Optional, Any
import logging
import numpy as np
import copy
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class GRPOConfig:
    """GRPO í•™ìŠµ ì„¤ì • í´ë˜ìŠ¤"""
    learning_rate: float = 1e-5
    group_size: int = 4
    num_iterations: int = 20
    grpo_epochs: int = 2
    gamma: float = 0.99           # í• ì¸ íŒ©í„°
    kl_beta: float = 0.01         # KL ë°œì‚° í˜ë„í‹° ê³„ìˆ˜
    clip_epsilon: float = 0.2     # í´ë¦¬í•‘ ë²”ìœ„
    entropy_coeff: float = 0.01   # ì—”íŠ¸ë¡œí”¼ ë³´ë„ˆìŠ¤ ê³„ìˆ˜
    max_grad_norm: float = 1.0    # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
    
    # ìƒì„± íŒŒë¼ë¯¸í„°
    max_new_tokens: int = 50
    temperature: float = 0.8
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device: str = "auto"

class GRPOTrainer:
    """
    GRPO ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•œ VLM í•™ìŠµ í´ë˜ìŠ¤
    
    ì´ í´ë˜ìŠ¤ëŠ” GRPO ë…¼ë¬¸ì˜ í•µì‹¬ ì•„ì´ë””ì–´ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤:
    1. ê·¸ë£¹ ë‚´ì—ì„œ ìƒëŒ€ì  ì„±ëŠ¥ ë¹„êµ
    2. ì°¸ì¡° ëª¨ë¸ê³¼ì˜ KL ë°œì‚° ì œí•œ
    3. ì•ˆì •ì ì¸ ì •ì±… ì—…ë°ì´íŠ¸
    
    Attributes:
        config (GRPOConfig): í•™ìŠµ ì„¤ì •
        vlm: VLM ëª¨ë¸ (í•™ìŠµ ëŒ€ìƒ)
        vlm_ref: ì°¸ì¡° VLM ëª¨ë¸ (ê³ ì •)
        optimizer: ì˜µí‹°ë§ˆì´ì €
        device: ì—°ì‚° ë””ë°”ì´ìŠ¤
    """
    
    def __init__(self, 
                 vlm_model,
                 config: GRPOConfig):
        """
        GRPO Trainer ì´ˆê¸°í™”
        
        Args:
            vlm_model: í•™ìŠµí•  VLM ëª¨ë¸
            config (GRPOConfig): GRPO í•™ìŠµ ì„¤ì •
        """
        self.config = config
        self.vlm = vlm_model
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        if config.device == "auto":
            if torch.backends.mps.is_available():
                self.device = torch.device("mps")
                logger.info("ğŸ Using Apple Silicon MPS for GRPO")
            elif torch.cuda.is_available():
                self.device = torch.device("cuda")
                logger.info("ğŸš€ Using CUDA GPU for GRPO")
            else:
                self.device = torch.device("cpu")
                logger.info("ğŸ’» Using CPU for GRPO")
        else:
            self.device = torch.device(config.device)
        
        # VLMì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        self.vlm = self.vlm.to(self.device)
        
        # ì°¸ì¡° ëª¨ë¸ ìƒì„± (ë§¤ iterationë§ˆë‹¤ ì—…ë°ì´íŠ¸)
        self.vlm_ref = None
        
        # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
        self.optimizer = optim.AdamW(
            self.vlm.parameters(),
            lr=config.learning_rate,
            weight_decay=0.01
        )
        
        # í•™ìŠµ í†µê³„
        self.training_stats = {
            'iteration': 0,
            'total_samples': 0,
            'avg_reward': 0.0,
            'policy_loss': 0.0,
            'kl_divergence': 0.0,
            'entropy': 0.0
        }
        
        logger.info(f"ğŸ”§ GRPO Trainer initialized with config: {config}")
    
    def collect_group_data(self, prompts: List[str]) -> Dict[str, Any]:
        """
        ê·¸ë£¹ ë°ì´í„° ìˆ˜ì§‘: í”„ë¡¬í”„íŠ¸ ê°œì„  ë° ë³´ìƒ ê³„ì‚°
        
        ì´ ë©”ì„œë“œëŠ” GRPOì˜ í•µì‹¬ ë°ì´í„° ìˆ˜ì§‘ ë‹¨ê³„ì…ë‹ˆë‹¤:
        1. ê° í”„ë¡¬í”„íŠ¸ë¥¼ VLMìœ¼ë¡œ ê°œì„ 
        2. ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ë¡œ ì´ë¯¸ì§€ ìƒì„±
        3. CLIPìœ¼ë¡œ ë³´ìƒ ê³„ì‚°
        4. ë¡œê·¸ í™•ë¥  ê³„ì‚°
        
        Args:
            prompts (List[str]): ì…ë ¥ í”„ë¡¬í”„íŠ¸ ê·¸ë£¹
            
        Returns:
            Dict[str, Any]: ìˆ˜ì§‘ëœ ê·¸ë£¹ ë°ì´í„°
        """
        logger.debug(f"ğŸ“Š Collecting group data for {len(prompts)} prompts")
        
        group_data = {
            'prompts': prompts,
            'enhanced_prompts': [],
            'images': [],
            'rewards': [],
            'log_probs': [],
            'ref_log_probs': [],
            'advantages': [],
            'returns': []
        }
        
        # ì°¸ì¡° ëª¨ë¸ ì—…ë°ì´íŠ¸ (í˜„ì¬ ì •ì±…ì˜ ë³µì‚¬ë³¸)
        self._update_reference_model()
        
        # ê° í”„ë¡¬í”„íŠ¸ì— ëŒ€í•´ ë°ì´í„° ìˆ˜ì§‘
        for prompt in prompts:
            try:
                # 1. VLMìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ê°œì„ 
                enhanced_prompt, log_prob = self._enhance_prompt_with_logprob(prompt)
                
                # 2. ì°¸ì¡° ëª¨ë¸ë¡œ ë¡œê·¸ í™•ë¥  ê³„ì‚°
                ref_log_prob = self._calculate_reference_logprob(prompt, enhanced_prompt)
                
                # 3. ì´ë¯¸ì§€ ìƒì„± (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” SD3 ì‚¬ìš©)
                image = self._generate_image(enhanced_prompt)
                
                # 4. ë³´ìƒ ê³„ì‚° (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” CLIP ì‚¬ìš©)
                reward = self._calculate_reward(image, enhanced_prompt, prompt)
                
                # ë°ì´í„° ì €ì¥
                group_data['enhanced_prompts'].append(enhanced_prompt)
                group_data['images'].append(image)
                group_data['rewards'].append(reward)
                group_data['log_probs'].append(log_prob)
                group_data['ref_log_probs'].append(ref_log_prob)
                
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to process prompt '{prompt}': {e}")
                # ì‹¤íŒ¨í•œ ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©
                group_data['enhanced_prompts'].append(prompt)
                group_data['images'].append(None)
                group_data['rewards'].append(0.0)
                group_data['log_probs'].append(torch.tensor(0.0))
                group_data['ref_log_probs'].append(torch.tensor(0.0))
        
        # 5. ì–´ë“œë°´í‹°ì§€ ë° ë¦¬í„´ ê³„ì‚°
        self._calculate_advantages_and_returns(group_data)
        
        logger.debug(f"âœ… Group data collected: avg_reward={np.mean(group_data['rewards']):.4f}")
        return group_data
    
    def _enhance_prompt_with_logprob(self, prompt: str) -> Tuple[str, torch.Tensor]:
        """
        VLMìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ê°œì„  ë° ë¡œê·¸ í™•ë¥  ê³„ì‚°
        
        Args:
            prompt (str): ì›ë³¸ í”„ë¡¬í”„íŠ¸
            
        Returns:
            Tuple[str, torch.Tensor]: (ê°œì„ ëœ í”„ë¡¬í”„íŠ¸, ë¡œê·¸ í™•ë¥ )
        """
        try:
            # VLMìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ê°œì„  (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” VLMWrapper ì‚¬ìš©)
            enhanced_prompt = self.vlm.enhance_prompt(prompt)
            
            # ë¡œê·¸ í™•ë¥  ê³„ì‚° (ê°„ì†Œí™”ëœ ë²„ì „)
            # ì‹¤ì œë¡œëŠ” ìƒì„±ëœ í† í°ë“¤ì˜ ë¡œê·¸ í™•ë¥ ì„ ê³„ì‚°í•´ì•¼ í•¨
            log_prob = torch.tensor(0.0, device=self.device)  # í”Œë ˆì´ìŠ¤í™€ë”
            
            return enhanced_prompt, log_prob
            
        except Exception as e:
            logger.warning(f"âš ï¸ Prompt enhancement failed: {e}")
            return prompt, torch.tensor(0.0, device=self.device)
    
    def _calculate_reference_logprob(self, prompt: str, enhanced_prompt: str) -> torch.Tensor:
        """
        ì°¸ì¡° ëª¨ë¸ë¡œ ë¡œê·¸ í™•ë¥  ê³„ì‚°
        
        Args:
            prompt (str): ì›ë³¸ í”„ë¡¬í”„íŠ¸
            enhanced_prompt (str): ê°œì„ ëœ í”„ë¡¬í”„íŠ¸
            
        Returns:
            torch.Tensor: ì°¸ì¡° ëª¨ë¸ì˜ ë¡œê·¸ í™•ë¥ 
        """
        try:
            if self.vlm_ref is None:
                return torch.tensor(0.0, device=self.device)
            
            # ì°¸ì¡° ëª¨ë¸ë¡œ ë¡œê·¸ í™•ë¥  ê³„ì‚° (ê°„ì†Œí™”ëœ ë²„ì „)
            ref_log_prob = torch.tensor(0.0, device=self.device)  # í”Œë ˆì´ìŠ¤í™€ë”
            
            return ref_log_prob
            
        except Exception as e:
            logger.warning(f"âš ï¸ Reference log prob calculation failed: {e}")
            return torch.tensor(0.0, device=self.device)
    
    def _generate_image(self, prompt: str):
        """
        í”„ë¡¬í”„íŠ¸ë¡œë¶€í„° ì´ë¯¸ì§€ ìƒì„± (í”Œë ˆì´ìŠ¤í™€ë”)
        
        ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” SD3Generatorë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        
        Args:
            prompt (str): ì´ë¯¸ì§€ ìƒì„±ìš© í”„ë¡¬í”„íŠ¸
            
        Returns:
            ìƒì„±ëœ ì´ë¯¸ì§€ (í”Œë ˆì´ìŠ¤í™€ë”)
        """
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” SD3Generator.generate_image() í˜¸ì¶œ
        return f"image_for_{prompt[:20]}"  # í”Œë ˆì´ìŠ¤í™€ë”
    
    def _calculate_reward(self, image, enhanced_prompt: str, original_prompt: str) -> float:
        """
        ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê¸°ë°˜ ë³´ìƒ ê³„ì‚° (í”Œë ˆì´ìŠ¤í™€ë”)
        
        ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” CLIPRewardCalculatorë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        
        Args:
            image: ìƒì„±ëœ ì´ë¯¸ì§€
            enhanced_prompt (str): ê°œì„ ëœ í”„ë¡¬í”„íŠ¸
            original_prompt (str): ì›ë³¸ í”„ë¡¬í”„íŠ¸
            
        Returns:
            float: ê³„ì‚°ëœ ë³´ìƒ
        """
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” CLIPRewardCalculator.calculate_reward() í˜¸ì¶œ
        # ê°„ë‹¨í•œ ì‹œë®¬ë ˆì´ì…˜: í”„ë¡¬í”„íŠ¸ ê¸¸ì´ì— ë”°ë¥¸ ë³´ìƒ
        reward = min(len(enhanced_prompt) / 100.0, 1.0)
        return reward
    
    def _calculate_advantages_and_returns(self, group_data: Dict[str, Any]):
        """
        GRPOì˜ í•µì‹¬: ê·¸ë£¹ ê¸°ë°˜ ì–´ë“œë°´í‹°ì§€ ê³„ì‚°
        
        GRPOëŠ” ê·¸ë£¹ ë‚´ì—ì„œ ìƒëŒ€ì  ì„±ëŠ¥ì„ ë¹„êµí•˜ì—¬ ì–´ë“œë°´í‹°ì§€ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤:
        1. ê·¸ë£¹ í‰ê·  ë³´ìƒ ê³„ì‚°
        2. ê° ìƒ˜í”Œì˜ ìƒëŒ€ì  ì„±ëŠ¥ ì¸¡ì •
        3. í• ì¸ëœ ë¦¬í„´ ê³„ì‚°
        4. ì •ê·œí™”
        
        Args:
            group_data (Dict[str, Any]): ê·¸ë£¹ ë°ì´í„° (in-place ìˆ˜ì •)
        """
        rewards = np.array(group_data['rewards'])
        
        # 1. í• ì¸ëœ ë¦¬í„´ ê³„ì‚° (ë‹¨ìˆœí™”: ë‹¨ì¼ ìŠ¤í…)
        returns = rewards.copy()
        
        # 2. ê·¸ë£¹ ê¸°ë°˜ ì–´ë“œë°´í‹°ì§€ ê³„ì‚°
        # GRPOì˜ í•µì‹¬: ê·¸ë£¹ í‰ê·  ëŒ€ë¹„ ìƒëŒ€ì  ì„±ëŠ¥
        group_mean_reward = np.mean(rewards)
        advantages = rewards - group_mean_reward
        
        # 3. ì–´ë“œë°´í‹°ì§€ ì •ê·œí™” (í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ)
        if len(advantages) > 1:
            advantages = (advantages - np.mean(advantages)) / (np.std(advantages) + 1e-8)
        
        # 4. í…ì„œë¡œ ë³€í™˜
        group_data['returns'] = [torch.tensor(r, dtype=torch.float32, device=self.device) for r in returns]
        group_data['advantages'] = [torch.tensor(a, dtype=torch.float32, device=self.device) for a in advantages]
        
        logger.debug(f"ğŸ“Š Advantages calculated: mean={np.mean(advantages):.4f}, std={np.std(advantages):.4f}")
    
    def _update_reference_model(self):
        """
        ì°¸ì¡° ëª¨ë¸ ì—…ë°ì´íŠ¸ (í˜„ì¬ ì •ì±…ì˜ ë³µì‚¬ë³¸)
        
        GRPOì—ì„œ ì°¸ì¡° ëª¨ë¸ì€ KL ë°œì‚° ì œí•œì„ ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.
        ë§¤ iterationë§ˆë‹¤ í˜„ì¬ ì •ì±…ì„ ë³µì‚¬í•˜ì—¬ ì°¸ì¡° ëª¨ë¸ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
        """
        try:
            # í˜„ì¬ VLMì˜ ê¹Šì€ ë³µì‚¬ë³¸ ìƒì„±
            self.vlm_ref = copy.deepcopy(self.vlm)
            self.vlm_ref.eval()  # í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
            
            # ì°¸ì¡° ëª¨ë¸ì˜ ê·¸ë˜ë””ì–¸íŠ¸ ë¹„í™œì„±í™”
            for param in self.vlm_ref.parameters():
                param.requires_grad = False
            
            logger.debug("ğŸ”„ Reference model updated")
            
        except Exception as e:
            logger.warning(f"âš ï¸ Reference model update failed: {e}")
    
    def grpo_update(self, group_data: Dict[str, Any]) -> Dict[str, float]:
        """
        GRPO ì •ì±… ì—…ë°ì´íŠ¸ ìˆ˜í–‰
        
        ì´ ë©”ì„œë“œëŠ” GRPO ë…¼ë¬¸ì˜ í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í•©ë‹ˆë‹¤:
        1. ì •ì±… ë¹„ìœ¨ ê³„ì‚° (Ï€_Î¸ / Ï€_ref)
        2. í´ë¦¬í•‘ëœ ì„œë¡œê²Œì´íŠ¸ ì†ì‹¤
        3. KL ë°œì‚° í˜ë„í‹°
        4. ì—”íŠ¸ë¡œí”¼ ë³´ë„ˆìŠ¤
        
        Args:
            group_data (Dict[str, Any]): ìˆ˜ì§‘ëœ ê·¸ë£¹ ë°ì´í„°
            
        Returns:
            Dict[str, float]: í•™ìŠµ ë©”íŠ¸ë¦­
        """
        metrics = {
            'policy_loss': 0.0,
            'kl_div': 0.0,
            'entropy': 0.0,
            'total_loss': 0.0
        }
        
        # GRPO ì—í¬í¬ë§Œí¼ ë°˜ë³µ í•™ìŠµ
        for epoch in range(self.config.grpo_epochs):
            epoch_metrics = self._grpo_epoch_update(group_data)
            
            # ë©”íŠ¸ë¦­ ëˆ„ì 
            for key in metrics:
                metrics[key] += epoch_metrics[key]
        
        # í‰ê·  ê³„ì‚°
        for key in metrics:
            metrics[key] /= self.config.grpo_epochs
        
        # í•™ìŠµ í†µê³„ ì—…ë°ì´íŠ¸
        self.training_stats.update(metrics)
        self.training_stats['iteration'] += 1
        self.training_stats['total_samples'] += len(group_data['prompts'])
        self.training_stats['avg_reward'] = np.mean(group_data['rewards'])
        
        logger.info(f"ğŸ”„ GRPO update completed: loss={metrics['total_loss']:.4f}, "
                   f"reward={self.training_stats['avg_reward']:.4f}")
        
        return metrics
    
    def _grpo_epoch_update(self, group_data: Dict[str, Any]) -> Dict[str, float]:
        """
        ë‹¨ì¼ GRPO ì—í¬í¬ ì—…ë°ì´íŠ¸
        
        Args:
            group_data (Dict[str, Any]): ê·¸ë£¹ ë°ì´í„°
            
        Returns:
            Dict[str, float]: ì—í¬í¬ ë©”íŠ¸ë¦­
        """
        self.optimizer.zero_grad()
        
        # í˜„ì¬ ì •ì±…ìœ¼ë¡œ ë¡œê·¸ í™•ë¥  ì¬ê³„ì‚° (ì •ì±…ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìœ¼ë¯€ë¡œ)
        current_log_probs = []
        for i, prompt in enumerate(group_data['prompts']):
            enhanced_prompt = group_data['enhanced_prompts'][i]
            _, log_prob = self._enhance_prompt_with_logprob(prompt)
            current_log_probs.append(log_prob)
        
        # ì†ì‹¤ ê³„ì‚°
        policy_loss = 0.0
        kl_div = 0.0
        entropy = 0.0
        
        for i in range(len(group_data['prompts'])):
            # ì •ì±… ë¹„ìœ¨ ê³„ì‚°: Ï€_Î¸(a|s) / Ï€_ref(a|s)
            log_ratio = current_log_probs[i] - group_data['ref_log_probs'][i]
            ratio = torch.exp(log_ratio)
            
            # ì–´ë“œë°´í‹°ì§€
            advantage = group_data['advantages'][i]
            
            # í´ë¦¬í•‘ëœ ì„œë¡œê²Œì´íŠ¸ ì†ì‹¤ (PPO ìŠ¤íƒ€ì¼)
            surr1 = ratio * advantage
            surr2 = torch.clamp(ratio, 1 - self.config.clip_epsilon, 1 + self.config.clip_epsilon) * advantage
            policy_loss_i = -torch.min(surr1, surr2)
            
            # KL ë°œì‚° (ê·¼ì‚¬ì¹˜)
            kl_div_i = log_ratio  # ê·¼ì‚¬: log(Ï€/Ï€_ref) â‰ˆ KL(Ï€_ref, Ï€)
            
            # ì—”íŠ¸ë¡œí”¼ (ê°„ì†Œí™”)
            entropy_i = -current_log_probs[i]  # ê°„ì†Œí™”ëœ ì—”íŠ¸ë¡œí”¼
            
            policy_loss += policy_loss_i
            kl_div += kl_div_i
            entropy += entropy_i
        
        # ë°°ì¹˜ í¬ê¸°ë¡œ ì •ê·œí™”
        batch_size = len(group_data['prompts'])
        policy_loss /= batch_size
        kl_div /= batch_size
        entropy /= batch_size
        
        # ì´ ì†ì‹¤: ì •ì±… ì†ì‹¤ + KL í˜ë„í‹° - ì—”íŠ¸ë¡œí”¼ ë³´ë„ˆìŠ¤
        total_loss = policy_loss + self.config.kl_beta * kl_div - self.config.entropy_coeff * entropy
        
        # ì—­ì „íŒŒ
        total_loss.backward()
        
        # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
        torch.nn.utils.clip_grad_norm_(self.vlm.parameters(), self.config.max_grad_norm)
        
        # ì˜µí‹°ë§ˆì´ì € ìŠ¤í…
        self.optimizer.step()
        
        return {
            'policy_loss': policy_loss.item(),
            'kl_div': kl_div.item(),
            'entropy': entropy.item(),
            'total_loss': total_loss.item()
        }
    
    def get_training_stats(self) -> Dict[str, Any]:
        """í˜„ì¬ í•™ìŠµ í†µê³„ ë°˜í™˜"""
        return self.training_stats.copy()
    
    def save_checkpoint(self, checkpoint_path: str):
        """í•™ìŠµ ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        try:
            checkpoint = {
                'model_state_dict': self.vlm.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'config': self.config,
                'training_stats': self.training_stats
            }
            torch.save(checkpoint, checkpoint_path)
            logger.info(f"ğŸ’¾ Checkpoint saved: {checkpoint_path}")
        except Exception as e:
            logger.error(f"âŒ Failed to save checkpoint: {e}")
    
    def load_checkpoint(self, checkpoint_path: str):
        """í•™ìŠµ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        try:
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            self.vlm.load_state_dict(checkpoint['model_state_dict'])
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            self.training_stats = checkpoint['training_stats']
            logger.info(f"ğŸ“¥ Checkpoint loaded: {checkpoint_path}")
        except Exception as e:
            logger.error(f"âŒ Failed to load checkpoint: {e}")


if __name__ == "__main__":
    # GRPO Trainer í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ğŸ§ª GRPO Trainer Test")
    print("=" * 30)
    
    try:
        # Mock VLM ëª¨ë¸
        class MockVLM(nn.Module):
            def __init__(self):
                super().__init__()
                self.linear = nn.Linear(10, 10)
            
            def enhance_prompt(self, prompt):
                return f"enhanced: {prompt}"
        
        # ì„¤ì • ë° íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
        config = GRPOConfig(
            learning_rate=1e-4,
            group_size=3,
            num_iterations=2
        )
        
        mock_vlm = MockVLM()
        trainer = GRPOTrainer(mock_vlm, config)
        
        print("âœ… GRPO Trainer initialized successfully")
        print(f"ğŸ“Š Training stats: {trainer.get_training_stats()}")
        
        # í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
        test_prompts = ["a cat", "sunset", "mountain"]
        
        print("\nğŸ”„ Testing group data collection:")
        group_data = trainer.collect_group_data(test_prompts)
        print(f"  Collected data for {len(group_data['prompts'])} prompts")
        print(f"  Average reward: {np.mean(group_data['rewards']):.4f}")
        
        print("\nğŸ”„ Testing GRPO update:")
        metrics = trainer.grpo_update(group_data)
        print(f"  Update metrics: {metrics}")
        
        print("\nâœ… All tests passed!")
        
    except Exception as e:
        print(f"âŒ Test failed: {e}")
    
    print("\nUsage:")
    print("from training.grpo_trainer import GRPOTrainer, GRPOConfig")
    print("config = GRPOConfig()")
    print("trainer = GRPOTrainer(vlm_model, config)")
    print("group_data = trainer.collect_group_data(prompts)")
    print("metrics = trainer.grpo_update(group_data)") 