# GRPO êµ¬í˜„ ê°œì„  ë°©ì•ˆ

## ğŸ“Š CartPole GRPO vs VLM GRPO ë¹„êµ ë¶„ì„

### âœ… ì •í™•í•˜ê²Œ êµ¬í˜„ëœ ë¶€ë¶„ë“¤

- [x] Group-relative advantage ê¸°ë³¸ ê°œë…
- [x] PPO í´ë¦¬í•‘ ë©”ì»¤ë‹ˆì¦˜
- [x] ë¡œê·¸ í™•ë¥  ê³„ì‚° ì¼ê´€ì„±
- [x] Policy loss ê³„ì‚°
- [x] ê¸°ë³¸ì ì¸ KL penalty

### âŒ êµ¬í˜„ë˜ì§€ ì•Šì€ ì¤‘ìš”í•œ ë¶€ë¶„ë“¤

#### 1. í• ì¸ëœ ë¦¬í„´ (Discounted Returns) ê³„ì‚°

**í˜„ì¬ ìƒíƒœ**: ì¦‰ì‹œ ë¦¬ì›Œë“œë§Œ ì‚¬ìš©
**í•„ìš”í•œ ìˆ˜ì •**:

```python
def calculate_discounted_returns(rewards: List[float], gamma: float = 0.99) -> torch.Tensor:
    """í• ì¸ëœ ë¦¬í„´ ê³„ì‚° (CartPole GRPO ë°©ì‹)"""
    returns = torch.zeros(len(rewards))
    discounted_return = 0.0
    for t in reversed(range(len(rewards))):
        discounted_return = rewards[t] + gamma * discounted_return
        returns[t] = discounted_return
    return returns
```

#### 2. Reference ëª¨ë¸ ì—…ë°ì´íŠ¸

**í˜„ì¬ ìƒíƒœ**: ì´ˆê¸°í™” ì‹œì ì— í•œ ë²ˆë§Œ ìƒì„±
**í•„ìš”í•œ ìˆ˜ì •**:

```python
def update_reference_model(self):
    """ë§¤ iterationë§ˆë‹¤ í˜„ì¬ ëª¨ë¸ì„ referenceë¡œ ë³µì‚¬"""
    self.ref_model.load_state_dict(self.model.state_dict())
    self.ref_model.eval()
```

#### 3. ì •í™•í•œ KL Divergence ê³„ì‚°

**í˜„ì¬ ìƒíƒœ**: ë‹¨ìˆœí•œ ë¡œê·¸ í™•ë¥  ì°¨ì´
**í•„ìš”í•œ ìˆ˜ì •**:

```python
def calculate_kl_divergence(self, log_probs_new, log_probs_ref):
    """ì •í™•í•œ KL divergence ì¶”ì •ê¸°"""
    log_ratio = log_probs_ref - log_probs_new.detach()
    kl_div = torch.exp(log_ratio) - log_ratio - 1
    return torch.relu(kl_div.mean())
```

#### 4. ë‹¤ì¤‘ ì—í¬í¬ ì—…ë°ì´íŠ¸

**í˜„ì¬ ìƒíƒœ**: 1íšŒ ì—…ë°ì´íŠ¸ë§Œ ìˆ˜í–‰
**í•„ìš”í•œ ìˆ˜ì •**:

```python
def update_grpo_policy_multiple_epochs(self, experiences, num_epochs=10):
    """ê°™ì€ ë°ì´í„°ë¡œ ì—¬ëŸ¬ ë²ˆ ì—…ë°ì´íŠ¸"""
    for epoch in range(num_epochs):
        # ì •ì±… ì—…ë°ì´íŠ¸ ìˆ˜í–‰
        metrics = self.update_grpo_policy(experiences)
```

#### 5. ê·¸ë£¹ ì •ê·œí™” ê°œì„ 

**í˜„ì¬ ìƒíƒœ**: ë‹¨ìˆœí•œ ê·¸ë£¹ í‰ê·  ì°¨ì´
**í•„ìš”í•œ ìˆ˜ì •**:

```python
def calculate_normalized_advantages(self, all_returns):
    """ì „ì²´ ê·¸ë£¹ì— ëŒ€í•œ ì •ê·œí™”"""
    mean_return = torch.mean(all_returns)
    std_return = torch.std(all_returns)
    return (all_returns - mean_return) / (std_return + 1e-8)
```

## ğŸ¯ êµ¬í˜„ ìš°ì„ ìˆœìœ„

### 1ìˆœìœ„ (í•µì‹¬ ì•Œê³ ë¦¬ì¦˜)

1. **Reference ëª¨ë¸ ì—…ë°ì´íŠ¸** - ë§¤ iterationë§ˆë‹¤ ê°±ì‹ 
2. **ì •í™•í•œ KL Divergence** - ìˆ˜í•™ì ìœ¼ë¡œ ì˜¬ë°”ë¥¸ ê³„ì‚°
3. **í• ì¸ëœ ë¦¬í„´ ê³„ì‚°** - ì¥ê¸° ë¦¬ì›Œë“œ ê³ ë ¤

### 2ìˆœìœ„ (ì„±ëŠ¥ í–¥ìƒ)

4. **ë‹¤ì¤‘ ì—í¬í¬ ì—…ë°ì´íŠ¸** - ë°ì´í„° íš¨ìœ¨ì„± í–¥ìƒ
5. **ê·¸ë£¹ ì •ê·œí™” ê°œì„ ** - ë” ì•ˆì •ì ì¸ í•™ìŠµ

### 3ìˆœìœ„ (ìµœì í™”)

6. **ì—”íŠ¸ë¡œí”¼ ë³´ë„ˆìŠ¤** - íƒìƒ‰ í–¥ìƒ
7. **ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘** - í•™ìŠµ ì•ˆì •ì„±

## ğŸ”§ ìˆ˜ì • êµ¬í˜„ ì˜ˆì‹œ

### QWENGRPOConfig ì—…ë°ì´íŠ¸

```python
@dataclass
class QWENGRPOConfig:
    # ê¸°ì¡´ ì„¤ì •ë“¤...
    gamma: float = 0.99  # í• ì¸ íŒ©í„° ì¶”ê°€
    grpo_epochs: int = 10  # ë‹¤ì¤‘ ì—í¬í¬ ì¶”ê°€
    update_ref_model_freq: int = 1  # Reference ëª¨ë¸ ì—…ë°ì´íŠ¸ ë¹ˆë„
```

### ë©”ì¸ í•™ìŠµ ë£¨í”„ ìˆ˜ì •

```python
def train_with_proper_grpo(self, num_epochs: int = 5):
    for epoch in range(num_epochs):
        # 1. ê²½í—˜ ìˆ˜ì§‘
        experiences = self.collect_rollouts(prompts)

        # 2. í• ì¸ëœ ë¦¬í„´ ê³„ì‚°
        enhanced_experiences = self.calculate_discounted_advantages(experiences)

        # 3. Reference ëª¨ë¸ ì—…ë°ì´íŠ¸
        if epoch % self.config.update_ref_model_freq == 0:
            self.qwen_model.update_reference_model()

        # 4. ë‹¤ì¤‘ ì—í¬í¬ ì •ì±… ì—…ë°ì´íŠ¸
        for grpo_epoch in range(self.config.grpo_epochs):
            metrics = self.qwen_model.update_grpo_policy(enhanced_experiences)
```

## ğŸ“ˆ ì˜ˆìƒ ê°œì„  íš¨ê³¼

1. **í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ**: Reference ëª¨ë¸ ì—…ë°ì´íŠ¸ë¡œ KL penalty ì ì ˆíˆ ìœ ì§€
2. **ì¥ê¸° ê³„íš ëŠ¥ë ¥**: í• ì¸ëœ ë¦¬í„´ìœ¼ë¡œ ë¯¸ë˜ ë¦¬ì›Œë“œ ê³ ë ¤
3. **ë°ì´í„° íš¨ìœ¨ì„±**: ë‹¤ì¤‘ ì—í¬í¬ë¡œ ê°™ì€ ë°ì´í„° ì¬í™œìš©
4. **ìˆ˜ë ´ ì†ë„**: ì •í™•í•œ KL divergenceë¡œ ë” ë‚˜ì€ ì •ì±… ì—…ë°ì´íŠ¸

## ğŸš€ êµ¬í˜„ ë¡œë“œë§µ

### Phase 1: í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ ìˆ˜ì • (1-2ì¼)

- Reference ëª¨ë¸ ì—…ë°ì´íŠ¸ ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„
- ì •í™•í•œ KL divergence ê³„ì‚° ì ìš©
- í• ì¸ëœ ë¦¬í„´ ê³„ì‚° ì¶”ê°€

### Phase 2: ì„±ëŠ¥ ìµœì í™” (1ì¼)

- ë‹¤ì¤‘ ì—í¬í¬ ì—…ë°ì´íŠ¸ êµ¬í˜„
- ê·¸ë£¹ ì •ê·œí™” ê°œì„ 

### Phase 3: í…ŒìŠ¤íŠ¸ ë° ê²€ì¦ (1ì¼)

- CartPole GRPOì™€ ë™ì¼í•œ ê²°ê³¼ ê²€ì¦
- VLM íƒœìŠ¤í¬ì—ì„œ ì„±ëŠ¥ í–¥ìƒ í™•ì¸

## ğŸ“ ì£¼ìš” ì°¸ê³ ì‚¬í•­

1. **CartPole GRPO êµ¬í˜„**ì´ ìˆ˜í•™ì ìœ¼ë¡œ ì •í™•í•œ ì°¸ì¡° êµ¬í˜„
2. **í˜„ì¬ VLM GRPO**ëŠ” ê¸°ë³¸ êµ¬ì¡°ëŠ” ë§ì§€ë§Œ í•µì‹¬ ë””í…Œì¼ë“¤ì´ ëˆ„ë½
3. **ê°€ì¥ ì¤‘ìš”í•œ ê°œì„ ì **ì€ Reference ëª¨ë¸ ì—…ë°ì´íŠ¸ì™€ ì •í™•í•œ KL ê³„ì‚°
4. **í• ì¸ íŒ©í„°**ëŠ” VLM íƒœìŠ¤í¬ íŠ¹ì„±ìƒ 0.99ë³´ë‹¤ ë†’ê²Œ ì„¤ì • ê³ ë ¤ (0.995~0.999)
