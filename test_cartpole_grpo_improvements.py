#!/usr/bin/env python3
"""
CartPole GRPO í˜¸í™˜ ê°œì„ ì‚¬í•­ í…ŒìŠ¤íŠ¸
- Reference ëª¨ë¸ ì—…ë°ì´íŠ¸
- í• ì¸ëœ ë¦¬í„´ ê³„ì‚°
- ì •í™•í•œ KL divergence
- ë‹¤ì¤‘ ì—í¬í¬ ì—…ë°ì´íŠ¸
- ì—”íŠ¸ë¡œí”¼ ë³´ë„ˆìŠ¤
"""

import torch
import logging
from qwen import QWENModel, QWENGRPOConfig

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def test_cartpole_grpo_improvements():
    """CartPole GRPO í˜¸í™˜ ê°œì„ ì‚¬í•­ í…ŒìŠ¤íŠ¸"""
    print("ğŸš€ CartPole GRPO í˜¸í™˜ ê°œì„ ì‚¬í•­ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # 1. ì„¤ì • í™•ì¸
    config = QWENGRPOConfig(
        batch_size=1,
        num_rollouts=1,
        max_new_tokens=10,
        gamma=0.995,  # í• ì¸ íŒ©í„°
        grpo_epochs=3,  # ë‹¤ì¤‘ ì—í¬í¬ (í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 3ìœ¼ë¡œ ì„¤ì •)
        update_ref_model_freq=1,
        epsilon_std=1e-8
    )
    
    print("âœ… 1. ì„¤ì • í™•ì¸ ì™„ë£Œ")
    print(f"  - í• ì¸ íŒ©í„°: {config.gamma}")
    print(f"  - GRPO ì—í¬í¬: {config.grpo_epochs}")
    print(f"  - Reference ì—…ë°ì´íŠ¸ ë¹ˆë„: {config.update_ref_model_freq}")
    print(f"  - ì—”íŠ¸ë¡œí”¼ ê³„ìˆ˜: {config.entropy_coef}")
    
    # 2. ëª¨ë¸ ì´ˆê¸°í™”
    model = QWENModel(device="cuda" if torch.cuda.is_available() else "cpu", grpo_config=config)
    print("âœ… 2. ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
    
    # 3. Reference ëª¨ë¸ ì—…ë°ì´íŠ¸ í…ŒìŠ¤íŠ¸
    print("\nğŸ”„ 3. Reference ëª¨ë¸ ì—…ë°ì´íŠ¸ í…ŒìŠ¤íŠ¸")
    initial_ref_state = model.ref_model.state_dict() if model.ref_model else None
    
    # ëª¨ë¸ íŒŒë¼ë¯¸í„° ì•½ê°„ ë³€ê²½ (ì‹¤ì œ í•™ìŠµ ì‹œë®¬ë ˆì´ì…˜)
    if hasattr(model.model, 'lm_head') and hasattr(model.model.lm_head, 'weight'):
        with torch.no_grad():
            model.model.lm_head.weight += 0.001
    
    # Reference ëª¨ë¸ ì—…ë°ì´íŠ¸
    model.update_reference_model()
    
    # ì—…ë°ì´íŠ¸ í™•ì¸
    if model.ref_model and initial_ref_state:
        updated_ref_state = model.ref_model.state_dict()
        # ì²« ë²ˆì§¸ íŒŒë¼ë¯¸í„° ë¹„êµ
        first_param_key = list(initial_ref_state.keys())[0]
        if not torch.equal(initial_ref_state[first_param_key], updated_ref_state[first_param_key]):
            print("âœ… Reference ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë¨")
        else:
            print("âš ï¸ Reference ëª¨ë¸ ì—…ë°ì´íŠ¸ í™•ì¸ ë¶ˆê°€")
    
    # 4. í• ì¸ëœ ë¦¬í„´ ê³„ì‚° í…ŒìŠ¤íŠ¸
    print("\nğŸ“Š 4. í• ì¸ëœ ë¦¬í„´ ê³„ì‚° í…ŒìŠ¤íŠ¸")
    test_rewards = [1.0, 0.8, 0.6, 0.4, 0.2]
    discounted_returns = model.calculate_discounted_returns(test_rewards, gamma=0.9)
    
    print(f"  ì›ë³¸ ë¦¬ì›Œë“œ: {test_rewards}")
    print(f"  í• ì¸ëœ ë¦¬í„´: {discounted_returns.tolist()}")
    
    # ìˆ˜ë™ ê³„ì‚°ìœ¼ë¡œ ê²€ì¦
    expected_last = 0.2
    expected_second_last = 0.4 + 0.9 * 0.2
    print(f"  ê²€ì¦ - ë§ˆì§€ë§‰: {expected_last:.3f} vs {discounted_returns[-1]:.3f}")
    print(f"  ê²€ì¦ - ëì—ì„œ 2ë²ˆì§¸: {expected_second_last:.3f} vs {discounted_returns[-2]:.3f}")
    
    if abs(discounted_returns[-1] - expected_last) < 0.001:
        print("âœ… í• ì¸ëœ ë¦¬í„´ ê³„ì‚° ì •í™•")
    else:
        print("âŒ í• ì¸ëœ ë¦¬í„´ ê³„ì‚° ì˜¤ë¥˜")
    
    # 5. ì •ê·œí™”ëœ advantage í…ŒìŠ¤íŠ¸
    print("\nğŸ“ 5. ì •ê·œí™”ëœ advantage í…ŒìŠ¤íŠ¸")
    test_returns = torch.tensor([2.0, 1.5, 3.0, 0.5, 2.5])
    normalized_adv = model.calculate_normalized_advantages(test_returns)
    
    print(f"  ì›ë³¸ ë¦¬í„´: {test_returns.tolist()}")
    print(f"  ì •ê·œí™”ëœ advantage: {normalized_adv.tolist()}")
    print(f"  í‰ê· : {normalized_adv.mean():.6f} (0ì— ê°€ê¹Œì›Œì•¼ í•¨)")
    print(f"  í‘œì¤€í¸ì°¨: {normalized_adv.std():.6f} (1ì— ê°€ê¹Œì›Œì•¼ í•¨)")
    
    if abs(normalized_adv.mean()) < 0.001 and abs(normalized_adv.std() - 1.0) < 0.1:
        print("âœ… ì •ê·œí™” ì •í™•")
    else:
        print("âŒ ì •ê·œí™” ì˜¤ë¥˜")
    
    # 6. ê°€ì§œ ê²½í—˜ ë°ì´í„°ë¡œ ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
    print("\nğŸ”„ 6. ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸")
    
    # ê°€ì§œ ê²½í—˜ ìƒì„±
    experiences = []
    for i in range(2):
        user_prompt = f"test prompt {i}"
        enhanced_prompt, log_prob = model.generate_grpo_enhanced_prompt(user_prompt)
        
        experience = {
            'user_prompt': user_prompt,
            'enhanced_prompt': enhanced_prompt,
            'log_prob': log_prob,
            'reward': 0.5 + i * 0.1
        }
        experiences.append(experience)
    
    print(f"  ìƒì„±ëœ ê²½í—˜ ìˆ˜: {len(experiences)}")
    
    # 7. ë‹¤ì¤‘ ì—í¬í¬ ì—…ë°ì´íŠ¸ í…ŒìŠ¤íŠ¸
    print("\nğŸ”„ 7. ë‹¤ì¤‘ ì—í¬í¬ ì—…ë°ì´íŠ¸ í…ŒìŠ¤íŠ¸")
    
    try:
        metrics = model.update_grpo_policy_multiple_epochs(experiences)
        
        print("ğŸ“Š ë‹¤ì¤‘ ì—í¬í¬ ì—…ë°ì´íŠ¸ ê²°ê³¼:")
        for key, value in metrics.items():
            if isinstance(value, (int, float)):
                print(f"  {key}: {value:.6f}")
            else:
                print(f"  {key}: {value}")
        
        # í•„ìˆ˜ ë©”íŠ¸ë¦­ í™•ì¸
        required_metrics = ['avg_policy_loss', 'avg_kl_div', 'avg_entropy', 'num_epochs']
        missing_metrics = [m for m in required_metrics if m not in metrics]
        
        if not missing_metrics:
            print("âœ… ëª¨ë“  í•„ìˆ˜ ë©”íŠ¸ë¦­ ì¡´ì¬")
        else:
            print(f"âŒ ëˆ„ë½ëœ ë©”íŠ¸ë¦­: {missing_metrics}")
        
        # ì—í¬í¬ ìˆ˜ í™•ì¸
        if metrics.get('num_epochs') == config.grpo_epochs:
            print("âœ… ì •í™•í•œ ì—í¬í¬ ìˆ˜ ì‹¤í–‰")
        else:
            print(f"âŒ ì—í¬í¬ ìˆ˜ ë¶ˆì¼ì¹˜: {metrics.get('num_epochs')} vs {config.grpo_epochs}")
            
    except Exception as e:
        print(f"âŒ ë‹¤ì¤‘ ì—í¬í¬ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
    
    # 8. ê°œì„ ì‚¬í•­ ìš”ì•½
    print("\nğŸ¯ CartPole GRPO í˜¸í™˜ ê°œì„ ì‚¬í•­ ìš”ì•½:")
    improvements = [
        ("Reference ëª¨ë¸ ì—…ë°ì´íŠ¸", "update_reference_model()"),
        ("í• ì¸ëœ ë¦¬í„´ ê³„ì‚°", "calculate_discounted_returns()"),
        ("ì •ê·œí™”ëœ advantage", "calculate_normalized_advantages()"),
        ("ì •í™•í•œ KL divergence", "KL(ref||current) ì¶”ì •ê¸°"),
        ("ë‹¤ì¤‘ ì—í¬í¬ ì—…ë°ì´íŠ¸", "update_grpo_policy_multiple_epochs()"),
        ("ì—”íŠ¸ë¡œí”¼ ë³´ë„ˆìŠ¤", "entropy_coef * entropy_estimate")
    ]
    
    for i, (name, method) in enumerate(improvements, 1):
        print(f"  {i}. âœ… {name}: {method}")
    
    print("\nğŸ‰ CartPole GRPO í˜¸í™˜ ê°œì„ ì‚¬í•­ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("ğŸ“‹ ëª¨ë“  ì£¼ìš” êµ¬ì„± ìš”ì†Œê°€ CartPole GRPO ì°¸ì¡° êµ¬í˜„ê³¼ í˜¸í™˜ë©ë‹ˆë‹¤.")

def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    try:
        test_cartpole_grpo_improvements()
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 