"""
VLM GRPO Main Trainer
====================

ëª¨ë“  ì»´í¬ë„ŒíŠ¸ë¥¼ í†µí•©í•˜ì—¬ End-to-End VLM GRPO í•™ìŠµì„ ìˆ˜í–‰í•˜ëŠ” ë©”ì¸ ëª¨ë“ˆì…ë‹ˆë‹¤.

ì‹œìŠ¤í…œ êµ¬ì¡°:
User Prompt â†’ VLM â†’ Enhanced Prompt â†’ SD3 â†’ Image â†’ CLIP Reward â†’ GRPO Update

ì£¼ìš” ê¸°ëŠ¥:
1. ì „ì²´ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ë° ì„¤ì •
2. í•™ìŠµ ë£¨í”„ ì‹¤í–‰
3. ê²€ì¦ ë° í‰ê°€
4. ê²°ê³¼ ì €ì¥ ë° ì‹œê°í™”
5. Wandb í†µí•©

Author: AI Assistant
Date: 2025-01-22
"""

import os
import sys
import logging
import time
from typing import Dict, List, Optional, Any
from pathlib import Path
import json

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent))

# ê° ëª¨ë“ˆ ì„í¬íŠ¸
try:
    from models.vlm_wrapper import VLMWrapper
    from models.sd_generator import SD3Generator  
    from models.clip_reward import CLIPRewardCalculator, MultiRewardCalculator
    from training.grpo_trainer import GRPOTrainer, GRPOConfig
    from utils.data_loader import PromptDataLoader
    from evaluation.validator import ValidationEvaluator
    from integration.wandb_logger import WandbLogger
except ImportError as e:
    print(f"âš ï¸ Import warning: {e}")
    print("ì¼ë¶€ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹¤ì œ ì‹¤í–‰ ì‹œì—ëŠ” ëª¨ë“  ì˜ì¡´ì„±ì´ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.")

logger = logging.getLogger(__name__)

class VLMGRPOSystem:
    """
    VLM GRPO ì „ì²´ ì‹œìŠ¤í…œì„ ê´€ë¦¬í•˜ëŠ” ë©”ì¸ í´ë˜ìŠ¤
    
    ì´ í´ë˜ìŠ¤ëŠ”:
    1. ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ë° ì—°ê²°
    2. ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    3. ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…
    4. ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬
    5. ê²°ê³¼ ë¶„ì„ ë° ì €ì¥
    """
    
    def __init__(self, config_path: Optional[str] = None):
        """
        VLM GRPO System ì´ˆê¸°í™”
        
        Args:
            config_path (str, optional): ì„¤ì • íŒŒì¼ ê²½ë¡œ
        """
        # ì„¤ì • ë¡œë”©
        self.config = self._load_config(config_path)
        
        # ë¡œê¹… ì„¤ì •
        self._setup_logging()
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.vlm = None
        self.sd_generator = None
        self.clip_calculator = None
        self.multi_reward_calculator = None
        self.grpo_trainer = None
        self.data_loader = None
        self.validator = None
        self.wandb_logger = None
        
        # í•™ìŠµ ìƒíƒœ
        self.training_stats = {
            'iteration': 0,
            'total_time': 0,
            'best_reward': -float('inf'),
            'best_model_path': None
        }
        
        logger.info("ğŸš€ VLM GRPO System initialized")
    
    def _load_config(self, config_path: Optional[str]) -> Dict:
        """
        ì„¤ì • íŒŒì¼ ë¡œë”© ë˜ëŠ” ê¸°ë³¸ ì„¤ì • ìƒì„±
        
        Args:
            config_path (str, optional): ì„¤ì • íŒŒì¼ ê²½ë¡œ
            
        Returns:
            Dict: ì‹œìŠ¤í…œ ì„¤ì •
        """
        default_config = {
            # ëª¨ë¸ ì„¤ì •
            "vlm_model": "microsoft/DialoGPT-medium",
            "sd_model": "runwayml/stable-diffusion-v1-5", 
            "clip_model": "openai/clip-vit-base-patch32",
            
            # í•™ìŠµ ì„¤ì •
            "learning_rate": 1e-5,
            "group_size": 4,
            "num_iterations": 50,
            "grpo_epochs": 2,
            "validation_interval": 5,
            
            # ë°ì´í„° ì„¤ì •
            "train_data_path": "train_prompts.jsonl",
            "val_data_path": "val_prompts.jsonl",
            
            # ì¶œë ¥ ì„¤ì •
            "output_dir": "vlm_grpo_results",
            "checkpoint_interval": 10,
            "save_images": True,
            
            # Wandb ì„¤ì •
            "use_wandb": True,
            "wandb_project": "vlm-grpo-training",
            "wandb_entity": None,
            
            # ë””ë°”ì´ìŠ¤ ì„¤ì •
            "device": "auto"
        }
        
        if config_path and Path(config_path).exists():
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    user_config = json.load(f)
                default_config.update(user_config)
                logger.info(f"ğŸ“¥ Config loaded from {config_path}")
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to load config: {e}, using defaults")
        
        return default_config
    
    def _setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.StreamHandler(),
                logging.FileHandler('vlm_grpo_training.log')
            ]
        )
    
    def initialize_components(self):
        """
        ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        
        ì´ ë©”ì„œë“œëŠ” ì‹œìŠ¤í…œì˜ ëª¨ë“  êµ¬ì„± ìš”ì†Œë¥¼ ìˆœì„œëŒ€ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤:
        1. VLM (í”„ë¡¬í”„íŠ¸ ê°œì„ )
        2. SD3 Generator (ì´ë¯¸ì§€ ìƒì„±)
        3. CLIP Reward Calculator (ë³´ìƒ ê³„ì‚°)
        4. GRPO Trainer (ê°•í™”í•™ìŠµ)
        5. Data Loader (ë°ì´í„° ê´€ë¦¬)
        6. Validator (ê²€ì¦)
        7. Wandb Logger (ì‹¤í—˜ ì¶”ì )
        """
        try:
            logger.info("ğŸ”§ Initializing components...")
            
            # 1. VLM ì´ˆê¸°í™”
            logger.info("ğŸ“ Initializing VLM...")
            self.vlm = VLMWrapper(
                model_name=self.config["vlm_model"],
                device=self.config["device"]
            )
            
            # 2. SD3 Generator ì´ˆê¸°í™”
            logger.info("ğŸ¨ Initializing SD3 Generator...")
            self.sd_generator = SD3Generator(
                model_name=self.config["sd_model"],
                device=self.config["device"]
            )
            
            # 3. CLIP Reward Calculator ì´ˆê¸°í™”
            logger.info("ğŸ† Initializing CLIP Reward Calculator...")
            self.clip_calculator = CLIPRewardCalculator(
                model_name=self.config["clip_model"],
                device=self.config["device"]
            )
            
            # 4. Multi Reward Calculator ì´ˆê¸°í™”
            self.multi_reward_calculator = MultiRewardCalculator(
                self.clip_calculator
            )
            
            # 5. GRPO Trainer ì´ˆê¸°í™”
            logger.info("ğŸ¯ Initializing GRPO Trainer...")
            grpo_config = GRPOConfig(
                learning_rate=self.config["learning_rate"],
                group_size=self.config["group_size"],
                num_iterations=self.config["num_iterations"],
                grpo_epochs=self.config["grpo_epochs"],
                device=self.config["device"]
            )
            self.grpo_trainer = GRPOTrainer(self.vlm, grpo_config)
            
            # 6. Data Loader ì´ˆê¸°í™”
            logger.info("ğŸ“Š Initializing Data Loader...")
            self.data_loader = PromptDataLoader(
                train_data_path=self.config["train_data_path"],
                val_data_path=self.config["val_data_path"]
            )
            
            # 7. Validator ì´ˆê¸°í™”
            logger.info("âœ… Initializing Validator...")
            self.validator = ValidationEvaluator(
                vlm=self.vlm,
                sd_generator=self.sd_generator,
                clip_calculator=self.clip_calculator
            )
            
            # 8. Wandb Logger ì´ˆê¸°í™” (ì„ íƒì )
            if self.config["use_wandb"]:
                logger.info("ğŸ“ˆ Initializing Wandb Logger...")
                self.wandb_logger = WandbLogger(
                    project=self.config["wandb_project"],
                    entity=self.config["wandb_entity"],
                    config=self.config
                )
            
            # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
            os.makedirs(self.config["output_dir"], exist_ok=True)
            
            logger.info("âœ… All components initialized successfully!")
            
        except Exception as e:
            logger.error(f"âŒ Component initialization failed: {e}")
            raise
    
    def run_training(self):
        """
        ë©”ì¸ í•™ìŠµ ë£¨í”„ ì‹¤í–‰
        
        ì´ ë©”ì„œë“œëŠ” GRPO í•™ìŠµì˜ ì „ì²´ ê³¼ì •ì„ ì‹¤í–‰í•©ë‹ˆë‹¤:
        1. í•™ìŠµ ë°ì´í„° ë°°ì¹˜ ìƒì„±
        2. VLMìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ê°œì„ 
        3. SD3ë¡œ ì´ë¯¸ì§€ ìƒì„±
        4. CLIPìœ¼ë¡œ ë³´ìƒ ê³„ì‚°
        5. GRPO ì •ì±… ì—…ë°ì´íŠ¸
        6. ì£¼ê¸°ì  ê²€ì¦ ë° ì €ì¥
        """
        logger.info("ğŸš€ Starting VLM GRPO training...")
        start_time = time.time()
        
        try:
            for iteration in range(self.config["num_iterations"]):
                iteration_start = time.time()
                
                logger.info(f"ğŸ”„ Iteration {iteration + 1}/{self.config['num_iterations']}")
                
                # 1. í•™ìŠµ ë°°ì¹˜ ìƒì„±
                batch_prompts = self.data_loader.get_training_batch(
                    batch_size=self.config["group_size"]
                )
                
                if not batch_prompts:
                    logger.warning("âš ï¸ No training data available, skipping iteration")
                    continue
                
                # 2. ê·¸ë£¹ ë°ì´í„° ìˆ˜ì§‘ (VLM + SD3 + CLIP)
                group_data = self._collect_training_data(batch_prompts)
                
                # 3. GRPO ì—…ë°ì´íŠ¸
                training_metrics = self.grpo_trainer.grpo_update(group_data)
                
                # 4. ë©”íŠ¸ë¦­ ë¡œê¹…
                iteration_time = time.time() - iteration_start
                self._log_training_metrics(iteration + 1, training_metrics, iteration_time)
                
                # 5. ì£¼ê¸°ì  ê²€ì¦
                if (iteration + 1) % self.config["validation_interval"] == 0:
                    self._run_validation(iteration + 1)
                
                # 6. ì²´í¬í¬ì¸íŠ¸ ì €ì¥
                if (iteration + 1) % self.config["checkpoint_interval"] == 0:
                    self._save_checkpoint(iteration + 1)
                
                # 7. ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
                avg_reward = training_metrics.get('avg_reward', 0)
                if avg_reward > self.training_stats['best_reward']:
                    self.training_stats['best_reward'] = avg_reward
                    self._save_best_model(iteration + 1)
            
            # í•™ìŠµ ì™„ë£Œ
            total_time = time.time() - start_time
            self.training_stats['total_time'] = total_time
            
            logger.info(f"âœ… Training completed! Total time: {total_time:.2f}s")
            self._save_final_results()
            
        except Exception as e:
            logger.error(f"âŒ Training failed: {e}")
            raise
        finally:
            # Wandb ì„¸ì…˜ ì¢…ë£Œ
            if self.wandb_logger:
                self.wandb_logger.finish()
    
    def _collect_training_data(self, prompts: List[str]) -> Dict[str, Any]:
        """
        í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘: VLM + SD3 + CLIP íŒŒì´í”„ë¼ì¸
        
        Args:
            prompts (List[str]): ì…ë ¥ í”„ë¡¬í”„íŠ¸ë“¤
            
        Returns:
            Dict[str, Any]: ìˆ˜ì§‘ëœ í•™ìŠµ ë°ì´í„°
        """
        logger.debug(f"ğŸ“Š Collecting training data for {len(prompts)} prompts")
        
        group_data = {
            'prompts': prompts,
            'enhanced_prompts': [],
            'images': [],
            'rewards': [],
            'comprehensive_rewards': []
        }
        
        for prompt in prompts:
            try:
                # 1. VLMìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ê°œì„ 
                enhanced_prompt = self.vlm.enhance_prompt(prompt)
                
                # 2. SD3ë¡œ ì´ë¯¸ì§€ ìƒì„±
                image = self.sd_generator.generate_image(enhanced_prompt)
                
                # 3. ì¢…í•©ì  ë³´ìƒ ê³„ì‚°
                rewards = self.multi_reward_calculator.calculate_comprehensive_reward(
                    image, prompt, enhanced_prompt
                )
                
                # ë°ì´í„° ì €ì¥
                group_data['enhanced_prompts'].append(enhanced_prompt)
                group_data['images'].append(image)
                group_data['rewards'].append(rewards['final_reward'])
                group_data['comprehensive_rewards'].append(rewards)
                
                logger.debug(f"âœ… Processed: '{prompt}' â†’ reward: {rewards['final_reward']:.4f}")
                
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to process prompt '{prompt}': {e}")
                # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ì‚¬ìš©
                group_data['enhanced_prompts'].append(prompt)
                group_data['images'].append(None)
                group_data['rewards'].append(0.0)
                group_data['comprehensive_rewards'].append({'final_reward': 0.0})
        
        return group_data
    
    def _log_training_metrics(self, iteration: int, metrics: Dict, iteration_time: float):
        """
        í•™ìŠµ ë©”íŠ¸ë¦­ ë¡œê¹…
        
        Args:
            iteration (int): í˜„ì¬ ë°˜ë³µ íšŸìˆ˜
            metrics (Dict): í•™ìŠµ ë©”íŠ¸ë¦­
            iteration_time (float): ë°˜ë³µ ì‹œê°„
        """
        # ê¸°ë³¸ ë¡œê¹…
        logger.info(f"ğŸ“Š Iteration {iteration} metrics:")
        logger.info(f"  - Policy Loss: {metrics.get('policy_loss', 0):.6f}")
        logger.info(f"  - KL Divergence: {metrics.get('kl_div', 0):.6f}")
        logger.info(f"  - Entropy: {metrics.get('entropy', 0):.6f}")
        logger.info(f"  - Average Reward: {metrics.get('avg_reward', 0):.4f}")
        logger.info(f"  - Iteration Time: {iteration_time:.2f}s")
        
        # Wandb ë¡œê¹…
        if self.wandb_logger:
            wandb_metrics = {
                'iteration': iteration,
                'policy_loss': metrics.get('policy_loss', 0),
                'kl_divergence': metrics.get('kl_div', 0),
                'entropy': metrics.get('entropy', 0),
                'average_reward': metrics.get('avg_reward', 0),
                'iteration_time': iteration_time,
                'total_time': time.time() - self.start_time if hasattr(self, 'start_time') else 0
            }
            self.wandb_logger.log_training_metrics(wandb_metrics)
    
    def _run_validation(self, iteration: int):
        """
        ê²€ì¦ ì‹¤í–‰
        
        Args:
            iteration (int): í˜„ì¬ ë°˜ë³µ íšŸìˆ˜
        """
        logger.info(f"ğŸ” Running validation at iteration {iteration}")
        
        try:
            # ê²€ì¦ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            val_data = self.data_loader.get_validation_data()
            
            if not val_data:
                logger.warning("âš ï¸ No validation data available")
                return
            
            # ê²€ì¦ ì‹¤í–‰
            val_results = self.validator.evaluate_batch(val_data[:10])  # ì²˜ìŒ 10ê°œë§Œ
            
            # ê²°ê³¼ ë¡œê¹…
            logger.info(f"âœ… Validation results:")
            logger.info(f"  - Success Rate: {val_results.get('success_rate', 0):.2%}")
            logger.info(f"  - Average CLIP Score: {val_results.get('avg_clip_score', 0):.4f}")
            logger.info(f"  - Quality Score: {val_results.get('quality_score', 0):.4f}")
            
            # Wandb ë¡œê¹…
            if self.wandb_logger:
                self.wandb_logger.log_validation_results(val_results)
            
            # ê²€ì¦ ê²°ê³¼ ì €ì¥
            val_save_path = f"{self.config['output_dir']}/validation_iter_{iteration}.json"
            with open(val_save_path, 'w', encoding='utf-8') as f:
                json.dump(val_results, f, indent=2, ensure_ascii=False)
            
        except Exception as e:
            logger.error(f"âŒ Validation failed: {e}")
    
    def _save_checkpoint(self, iteration: int):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        checkpoint_path = f"{self.config['output_dir']}/checkpoint_iter_{iteration}.pt"
        self.grpo_trainer.save_checkpoint(checkpoint_path)
        logger.info(f"ğŸ’¾ Checkpoint saved: {checkpoint_path}")
    
    def _save_best_model(self, iteration: int):
        """ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥"""
        best_model_path = f"{self.config['output_dir']}/best_model.pt"
        self.grpo_trainer.save_checkpoint(best_model_path)
        self.training_stats['best_model_path'] = best_model_path
        logger.info(f"ğŸ† Best model saved: {best_model_path} (iteration {iteration})")
    
    def _save_final_results(self):
        """ìµœì¢… ê²°ê³¼ ì €ì¥"""
        results = {
            'config': self.config,
            'training_stats': self.training_stats,
            'final_metrics': self.grpo_trainer.get_training_stats()
        }
        
        results_path = f"{self.config['output_dir']}/final_results.json"
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ“‹ Final results saved: {results_path}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ VLM GRPO System Starting...")
    print("=" * 50)
    
    try:
        # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        system = VLMGRPOSystem()
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        system.initialize_components()
        
        # í•™ìŠµ ì‹¤í–‰
        system.run_training()
        
        print("\nâœ… Training completed successfully!")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ Training interrupted by user")
    except Exception as e:
        print(f"\nâŒ Training failed: {e}")
        raise


if __name__ == "__main__":
    main() 