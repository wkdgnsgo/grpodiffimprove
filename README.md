# VLM GRPO System ğŸš€

**Vision Language Model + Group Relative Policy Optimization**

VLMì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ë¥¼ ê°œì„ í•˜ê³ , Stable Diffusionìœ¼ë¡œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ë©°, CLIPìœ¼ë¡œ ë³´ìƒì„ ê³„ì‚°í•˜ëŠ” ê°•í™”í•™ìŠµ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

## ì‹œìŠ¤í…œ êµ¬ì¡° ğŸ“‹

```
User Prompt â†’ VLM â†’ Enhanced Prompt â†’ SD3 â†’ Image â†’ CLIP Reward â†’ GRPO Update
```

### í´ë” êµ¬ì¡°

```
vlm_grpo_system/
â”œâ”€â”€ models/                 # í•µì‹¬ ëª¨ë¸ë“¤
â”‚   â”œâ”€â”€ vlm_wrapper.py     # VLM í”„ë¡¬í”„íŠ¸ ê°œì„ 
â”‚   â”œâ”€â”€ sd_generator.py    # Stable Diffusion ì´ë¯¸ì§€ ìƒì„±
â”‚   â””â”€â”€ clip_reward.py     # CLIP ë³´ìƒ ê³„ì‚°
â”œâ”€â”€ training/              # í•™ìŠµ ì•Œê³ ë¦¬ì¦˜
â”‚   â””â”€â”€ grpo_trainer.py    # GRPO íŠ¸ë ˆì´ë„ˆ
â”œâ”€â”€ utils/                 # ìœ í‹¸ë¦¬í‹°
â”‚   â””â”€â”€ data_loader.py     # ë°ì´í„° ë¡œë”©
â”œâ”€â”€ evaluation/            # í‰ê°€ ì‹œìŠ¤í…œ
â”‚   â””â”€â”€ validator.py       # ê²€ì¦ í‰ê°€ê¸°
â”œâ”€â”€ integration/           # í†µí•© ì‹œìŠ¤í…œ
â”‚   â”œâ”€â”€ main_trainer.py    # ë©”ì¸ íŠ¸ë ˆì´ë„ˆ
â”‚   â””â”€â”€ wandb_logger.py    # Wandb ë¡œê±°
â”œâ”€â”€ config/                # ì„¤ì • íŒŒì¼
â”‚   â””â”€â”€ default_config.json
â””â”€â”€ data/                  # ë°ì´í„° íŒŒì¼
```

## ì£¼ìš” ê¸°ëŠ¥ âœ¨

### 1. Models í´ë” - í•µì‹¬ ëª¨ë¸ë“¤

#### VLM Wrapper (`models/vlm_wrapper.py`)

- **ëª©ì **: ì‚¬ìš©ìì˜ ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„¸í•œ í”„ë¡¬í”„íŠ¸ë¡œ ê°œì„ 
- **ì…ë ¥**: "a cat"
- **ì¶œë ¥**: "a fluffy orange tabby cat sitting gracefully on a windowsill, soft natural lighting, professional pet photography"
- **ê¸°ëŠ¥**:
  - í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì ìš©
  - í…ìŠ¤íŠ¸ ìƒì„± íŒŒë¼ë¯¸í„° ê´€ë¦¬
  - ë°°ì¹˜ ì²˜ë¦¬ ì§€ì›
  - ì‹¤íŒ¨ ì‹œ fallback ì²˜ë¦¬

#### SD3 Generator (`models/sd_generator.py`)

- **ëª©ì **: ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ë¡œ ê³ í’ˆì§ˆ ì´ë¯¸ì§€ ìƒì„±
- **ê¸°ëŠ¥**:
  - Stable Diffusion 3 íŒŒì´í”„ë¼ì¸
  - ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ìƒì„±
  - ì´ë¯¸ì§€ í’ˆì§ˆ ê²€ì¦
  - ë°°ì¹˜ ìƒì„± ì§€ì›

#### CLIP Reward Calculator (`models/clip_reward.py`)

- **ëª©ì **: í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê¸°ë°˜ ë³´ìƒ ê³„ì‚°
- **ê¸°ëŠ¥**:
  - ë‹¨ì¼/ë°°ì¹˜ ë³´ìƒ ê³„ì‚°
  - ë‹¤ì¤‘ ë³´ìƒ í•¨ìˆ˜ (ìœ ì‚¬ë„, í’ˆì§ˆ, ì¼ê´€ì„±)
  - ë³´ìƒ ì •ê·œí™” ë° ìŠ¤ì¼€ì¼ë§

### 2. Training í´ë” - GRPO í•™ìŠµ ì•Œê³ ë¦¬ì¦˜

#### GRPO Trainer (`training/grpo_trainer.py`)

- **ëª©ì **: Group Relative Policy Optimization êµ¬í˜„
- **GRPO vs PPO ì°¨ì´ì **:
  - PPO: ê°œë³„ ìƒ˜í”Œ ê¸°ë°˜ ì–´ë“œë°´í‹°ì§€
  - GRPO: ê·¸ë£¹ ë‚´ ìƒëŒ€ì  ì–´ë“œë°´í‹°ì§€ (ë” ì•ˆì •ì )
- **í•µì‹¬ ê¸°ëŠ¥**:
  - ê·¸ë£¹ ê¸°ë°˜ ì–´ë“œë°´í‹°ì§€ ê³„ì‚°
  - ì°¸ì¡° ëª¨ë¸ ê´€ë¦¬
  - KL ë°œì‚° í˜ë„í‹°
  - í´ë¦¬í•‘ëœ ì„œë¡œê²Œì´íŠ¸ ì†ì‹¤

### 3. Utils í´ë” - ìœ í‹¸ë¦¬í‹°

#### Data Loader (`utils/data_loader.py`)

- **ëª©ì **: í•™ìŠµ/ê²€ì¦ ë°ì´í„° ê´€ë¦¬
- **ê¸°ëŠ¥**:
  - JSONL í˜•ì‹ ë°ì´í„° ë¡œë”©
  - ì¹´í…Œê³ ë¦¬/ë‚œì´ë„ë³„ ë°°ì¹˜ ìƒì„±
  - ê· í˜•ì¡íŒ ë°°ì¹˜ ìƒì„±
  - ê²°ê³¼ ì €ì¥

### 4. Integration í´ë” - í†µí•© ì‹œìŠ¤í…œ

#### Main Trainer (`integration/main_trainer.py`)

- **ëª©ì **: ì „ì²´ ì‹œìŠ¤í…œ í†µí•© ë° ì‹¤í–‰
- **ê¸°ëŠ¥**:
  - ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
  - End-to-End í•™ìŠµ íŒŒì´í”„ë¼ì¸
  - ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
  - ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬

## ì‚¬ìš©ë²• ğŸ› ï¸

### 1. ê¸°ë³¸ ì„¤ì •

```python
# ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì‹œìŠ¤í…œ ì‹¤í–‰
from integration.main_trainer import VLMGRPOSystem

system = VLMGRPOSystem()
system.initialize_components()
system.run_training()
```

### 2. ì»¤ìŠ¤í…€ ì„¤ì •

```python
# ì»¤ìŠ¤í…€ ì„¤ì • íŒŒì¼ ì‚¬ìš©
system = VLMGRPOSystem("config/my_config.json")
```

### 3. ê°œë³„ ì»´í¬ë„ŒíŠ¸ ì‚¬ìš©

```python
# VLMë§Œ ì‚¬ìš©
from models.vlm_wrapper import VLMWrapper

vlm = VLMWrapper()
enhanced = vlm.enhance_prompt("a cat")
print(enhanced)

# SD3ë§Œ ì‚¬ìš©
from models.sd_generator import SD3Generator

generator = SD3Generator()
image = generator.generate_image("beautiful landscape")

# CLIP ë³´ìƒë§Œ ì‚¬ìš©
from models.clip_reward import CLIPRewardCalculator

calculator = CLIPRewardCalculator()
reward = calculator.calculate_reward(image, "beautiful landscape")
```

## ì„¤ì • íŒŒì¼ âš™ï¸

`config/default_config.json`ì—ì„œ ëª¨ë“  ì„¤ì •ì„ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```json
{
  "model_settings": {
    "vlm_model": "microsoft/DialoGPT-medium",
    "sd_model": "runwayml/stable-diffusion-v1-5",
    "clip_model": "openai/clip-vit-base-patch32"
  },
  "training_settings": {
    "learning_rate": 1e-5,
    "group_size": 4,
    "num_iterations": 50
  }
}
```

## ë°ì´í„° í˜•ì‹ ğŸ“Š

### í•™ìŠµ ë°ì´í„° (train_prompts.jsonl)

```json
{"user_prompt": "a cat", "category": "basic", "difficulty": "easy"}
{"user_prompt": "sunset", "category": "basic", "difficulty": "easy"}
{"user_prompt": "abstract art", "category": "creative", "difficulty": "hard"}
```

### ê²€ì¦ ë°ì´í„° (val_prompts.jsonl)

```json
{"user_prompt": "dog", "category": "basic", "difficulty": "easy"}
{"user_prompt": "city skyline", "category": "photography", "difficulty": "medium"}
```

## ì‹¤í—˜ ì¶”ì  ğŸ“ˆ

Wandbë¥¼ í†µí•œ ì‹¤ì‹œê°„ ì‹¤í—˜ ì¶”ì :

- í•™ìŠµ ë©”íŠ¸ë¦­ (loss, reward, KL divergence)
- ê²€ì¦ ê²°ê³¼ (ì„±ê³µë¥ , í’ˆì§ˆ ì ìˆ˜)
- ìƒì„±ëœ ì´ë¯¸ì§€ ìƒ˜í”Œ
- í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¶”ì 

## ì¶œë ¥ ê²°ê³¼ ğŸ“

í•™ìŠµ ì™„ë£Œ í›„ `vlm_grpo_results/` í´ë”ì— ìƒì„±ë˜ëŠ” íŒŒì¼ë“¤:

```
vlm_grpo_results/
â”œâ”€â”€ best_model.pt              # ìµœê³  ì„±ëŠ¥ ëª¨ë¸
â”œâ”€â”€ checkpoint_iter_10.pt      # ì£¼ê¸°ì  ì²´í¬í¬ì¸íŠ¸
â”œâ”€â”€ final_results.json         # ìµœì¢… ê²°ê³¼ ìš”ì•½
â”œâ”€â”€ validation_iter_5.json     # ê²€ì¦ ê²°ê³¼
â””â”€â”€ vlm_grpo_training.log      # í•™ìŠµ ë¡œê·¸
```

## ì„±ëŠ¥ ìµœì í™” ğŸš€

### Apple Silicon (MPS) ì§€ì›

- ëª¨ë“  ëª¨ë¸ì´ Apple Silicon MPSë¥¼ ìë™ ê°ì§€í•˜ê³  í™œìš©
- GPU ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì‚¬ìš©

### ë©”ëª¨ë¦¬ ìµœì í™”

- Attention slicing
- Gradient checkpointing
- Mixed precision training

### ë°°ì¹˜ ì²˜ë¦¬

- íš¨ìœ¨ì ì¸ ë°°ì¹˜ ìƒì„±
- ë³‘ë ¬ ì²˜ë¦¬ ì§€ì›

## ì£¼ìš” ì•Œê³ ë¦¬ì¦˜ ğŸ§ 

### GRPO (Group Relative Policy Optimization)

1. **ê·¸ë£¹ ë°ì´í„° ìˆ˜ì§‘**:

   ```
   í”„ë¡¬í”„íŠ¸ ë°°ì¹˜ â†’ VLM ê°œì„  â†’ SD3 ìƒì„± â†’ CLIP ë³´ìƒ
   ```

2. **ì–´ë“œë°´í‹°ì§€ ê³„ì‚°**:

   ```python
   group_mean = np.mean(rewards)
   advantages = rewards - group_mean  # ìƒëŒ€ì  ì„±ëŠ¥
   ```

3. **ì •ì±… ì—…ë°ì´íŠ¸**:
   ```python
   ratio = Ï€_Î¸ / Ï€_ref  # ì •ì±… ë¹„ìœ¨
   loss = -min(ratio * advantage, clipped_ratio * advantage)
   ```

## ëª¨ë‹ˆí„°ë§ ë° ë””ë²„ê¹… ğŸ”

### ë¡œê¹… ë ˆë²¨

- `DEBUG`: ìƒì„¸í•œ ë””ë²„ê¹… ì •ë³´
- `INFO`: ì¼ë°˜ì ì¸ ì§„í–‰ ìƒí™©
- `WARNING`: ê²½ê³  ë©”ì‹œì§€
- `ERROR`: ì˜¤ë¥˜ ë°œìƒ

### ì£¼ìš” ë©”íŠ¸ë¦­

- **Policy Loss**: ì •ì±… ì†ì‹¤
- **KL Divergence**: ì°¸ì¡° ëª¨ë¸ê³¼ì˜ ì°¨ì´
- **Average Reward**: í‰ê·  ë³´ìƒ
- **Success Rate**: ê²€ì¦ ì„±ê³µë¥ 

## ë¬¸ì œ í•´ê²° ğŸ› ï¸

### ì¼ë°˜ì ì¸ ë¬¸ì œë“¤

1. **ë©”ëª¨ë¦¬ ë¶€ì¡±**:

   - ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
   - `memory_efficient=True` ì„¤ì •

2. **í•™ìŠµ ë¶ˆì•ˆì •**:

   - Learning rate ì¤„ì´ê¸°
   - KL beta ì¡°ì •

3. **ë‚®ì€ ë³´ìƒ**:
   - ë³´ìƒ ê°€ì¤‘ì¹˜ ì¡°ì •
   - í”„ë¡¬í”„íŠ¸ í’ˆì§ˆ í™•ì¸

## í™•ì¥ ê°€ëŠ¥ì„± ğŸ”®

### ìƒˆë¡œìš´ ëª¨ë¸ ì¶”ê°€

- VLM: ë‹¤ë¥¸ ì–¸ì–´ ëª¨ë¸ë¡œ êµì²´ ê°€ëŠ¥
- SD: ë‹¤ë¥¸ diffusion ëª¨ë¸ ì§€ì›
- CLIP: ë‹¤ë¥¸ vision-language ëª¨ë¸ ì‚¬ìš©

### ìƒˆë¡œìš´ ë³´ìƒ í•¨ìˆ˜

- ë¯¸ì  í’ˆì§ˆ í‰ê°€
- ì•ˆì „ì„± ê²€ì‚¬
- ìŠ¤íƒ€ì¼ ì¼ê´€ì„±

### ë‹¤ì¤‘ ëª¨ë‹¬ í™•ì¥

- ë¹„ë””ì˜¤ ìƒì„±
- 3D ëª¨ë¸ ìƒì„±
- ì˜¤ë””ì˜¤ ìƒì„±

## ë¼ì´ì„¼ìŠ¤ ğŸ“„

ì´ í”„ë¡œì íŠ¸ëŠ” ì—°êµ¬ ë° êµìœ¡ ëª©ì ìœ¼ë¡œ ì œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.

## ê¸°ì—¬í•˜ê¸° ğŸ¤

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests and documentation
5. Submit a pull request

---

**Happy Training! ğŸ‰**

ë” ìì„¸í•œ ì •ë³´ë‚˜ ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë©´ ì´ìŠˆë¥¼ ìƒì„±í•´ ì£¼ì„¸ìš”.
