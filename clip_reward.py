from transformers import CLIPProcessor, CLIPModel
import torch
import logging
from aesthetic_predictor_v2_5 import convert_v2_5_from_siglip
from sentence_transformers import SentenceTransformer, util
from typing import List

logger = logging.getLogger(__name__)

class CLIPReward:

    def __init__(self, model_name = "openai/clip-vit-base-patch32", device = "cuda"):
        self.model_name = model_name
        self.device = device

        self._load_model()
        logger.info(f"CLIP reward init")

    def _load_model(self):
        logger.info(f"Loading CLIP model: {self.model_name}")
            
        # CLIP í”„ë¡œì„¸ì„œ ë¡œë“œ
        self.processor = CLIPProcessor.from_pretrained(self.model_name)
        
        # CLIP ëª¨ë¸ ë¡œë“œ
        self.model = CLIPModel.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16
        )

        self.aesthetic_predictor_model, self.aesthetic_predictor_preprocessor = convert_v2_5_from_siglip(
            low_cpu_mem_usage=True,
            trust_remote_code=True,
        )

        self.aesthetic_predictor_model = self.aesthetic_predictor_model.to(torch.float16).to(self.device)

        self.sentence_transformer_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2').to(self.device)
        
        # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        self.model = self.model.to(self.device)
        self.model.eval()  # í‰ê°€ ëª¨ë“œ ì„¤ì •
    
    def calculate_reward(self, user_prompt, enhance_prompt, image, aesthetic_weight_factor = 20.0, clip_pen_threshold = 0.28, semantic_sim_threshold = 0.7):
        inputs = self.processor(
                text=[user_prompt], 
                images=[image], 
                return_tensors="pt", 
                padding=True
            ).to(self.device)
        
        with torch.no_grad():
            outputs = self.model(**inputs)
            
            # ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ì„ë² ë”© ì¶”ì¶œ
            image_embeds = outputs.image_embeds
            text_embeds = outputs.text_embeds

            image_embeds = image_embeds / image_embeds.norm(dim=-1, keepdim=True)
            text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)

            clip_sim = torch.sum(text_embeds * image_embeds, dim=-1).item()
            logger.info(f"clip similiarity: {clip_sim}")

        pixel_values = (
            self.aesthetic_predictor_preprocessor(images=image, return_tensors="pt").pixel_values.to(torch.float16)
            .to(self.device)
        )

        with torch.inference_mode():
            aesthetic_score = self.aesthetic_predictor_model(pixel_values).logits.squeeze().item()
            original_embed = self.sentence_transformer_model.encode(user_prompt, convert_to_tensor=True)
            enhanced_embed = self.sentence_transformer_model.encode(enhance_prompt, convert_to_tensor=True)
            embed_cosine_sim = util.cos_sim(original_embed, enhanced_embed).item()
        logger.info(f"aesthetic score: {aesthetic_score}")

        clip_pen_score = aesthetic_weight_factor * min(clip_sim - clip_pen_threshold, 0)
        semantic_penalty_term = 0.0

        if embed_cosine_sim < semantic_sim_threshold:
            semantic_penalty_term = -10 * (semantic_sim_threshold - embed_cosine_sim)

        reward = aesthetic_score + clip_pen_score + semantic_penalty_term

        return reward
    
    def calculate_batch_rewards(self, user_prompt: str, enhanced_prompts: List[str], images: List, 
                              aesthetic_weight_factor: float = 20.0, clip_pen_threshold: float = 0.28, 
                              semantic_sim_threshold: float = 0.7) -> List[float]:
        """ë°°ì¹˜ ë¦¬ì›Œë“œ ê³„ì‚° - ì—¬ëŸ¬ ì´ë¯¸ì§€ë¥¼ í•œë²ˆì— ì²˜ë¦¬"""
        logger.info(f"ğŸ” ë°°ì¹˜ CLIP ë¦¬ì›Œë“œ ê³„ì‚° ({len(images)}ê°œ)")
        
        batch_rewards = []
        
        try:
            # 1ë‹¨ê³„: ë°°ì¹˜ CLIP ìœ ì‚¬ë„ ê³„ì‚°
            user_prompts = [user_prompt] * len(images)  # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ë³µì œ
            
            inputs = self.processor(
                text=user_prompts,
                images=images,
                return_tensors="pt",
                padding=True
            ).to(self.device)
            
            with torch.no_grad():
                outputs = self.model(**inputs)
                
                # ë°°ì¹˜ ì„ë² ë”© ì •ê·œí™”
                image_embeds = outputs.image_embeds
                text_embeds = outputs.text_embeds
                
                image_embeds = image_embeds / image_embeds.norm(dim=-1, keepdim=True)
                text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)
                
                # ë°°ì¹˜ CLIP ìœ ì‚¬ë„ ê³„ì‚°
                clip_similarities = torch.sum(text_embeds * image_embeds, dim=-1).cpu().tolist()
            
            # 2ë‹¨ê³„: ë°°ì¹˜ ë¯¸ì  ì ìˆ˜ ê³„ì‚°
            aesthetic_scores = []
            for image in images:
                pixel_values = (
                    self.aesthetic_predictor_preprocessor(images=image, return_tensors="pt")
                    .pixel_values.to(torch.float16).to(self.device)
                )
                
                with torch.inference_mode():
                    aesthetic_score = self.aesthetic_predictor_model(pixel_values).logits.squeeze().item()
                    aesthetic_scores.append(aesthetic_score)
            
            # 3ë‹¨ê³„: ë°°ì¹˜ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°
            original_embed = self.sentence_transformer_model.encode(user_prompt, convert_to_tensor=True)
            enhanced_embeds = self.sentence_transformer_model.encode(enhanced_prompts, convert_to_tensor=True)
            semantic_similarities = util.cos_sim(original_embed, enhanced_embeds).squeeze().cpu().tolist()
            
            # ë‹¨ì¼ ê°’ì¸ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            if not isinstance(semantic_similarities, list):
                semantic_similarities = [semantic_similarities]
            
            # 4ë‹¨ê³„: ìµœì¢… ë¦¬ì›Œë“œ ê³„ì‚°
            for i, (clip_sim, aesthetic_score, semantic_sim) in enumerate(
                zip(clip_similarities, aesthetic_scores, semantic_similarities)
            ):
                # CLIP íŒ¨ë„í‹°
                clip_pen_score = aesthetic_weight_factor * min(clip_sim - clip_pen_threshold, 0)
                
                # ì˜ë¯¸ì  íŒ¨ë„í‹°
                semantic_penalty_term = 0.0
                if semantic_sim < semantic_sim_threshold:
                    semantic_penalty_term = -10 * (semantic_sim_threshold - semantic_sim)
                
                # ìµœì¢… ë¦¬ì›Œë“œ
                reward = aesthetic_score + clip_pen_score + semantic_penalty_term
                batch_rewards.append(reward)
                
                logger.info(f"  ì´ë¯¸ì§€ {i+1}: CLIP={clip_sim:.3f}, ë¯¸ì ={aesthetic_score:.3f}, "
                          f"ì˜ë¯¸={semantic_sim:.3f}, ë¦¬ì›Œë“œ={reward:.3f}")
        
        except Exception as e:
            logger.error(f"âŒ ë°°ì¹˜ ë¦¬ì›Œë“œ ê³„ì‚° ì‹¤íŒ¨: {e}")
            # ì—ëŸ¬ ì‹œ ê¸°ë³¸ ë¦¬ì›Œë“œ
            batch_rewards = [0.2] * len(images)
        
        logger.info(f"âœ… ë°°ì¹˜ ë¦¬ì›Œë“œ ì™„ë£Œ: í‰ê·  {sum(batch_rewards)/len(batch_rewards):.3f}")
        return batch_rewards