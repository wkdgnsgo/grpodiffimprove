#!/usr/bin/env python3
"""
GRPO ì•Œê³ ë¦¬ì¦˜ ì •í™•ì„± ê²€ì¦ í…ŒìŠ¤íŠ¸
- ë¡œê·¸ í™•ë¥  ê³„ì‚° ì¼ê´€ì„± ê²€ì‚¬
- Ï€_new/Ï€_old ë¹„ìœ¨ ê³„ì‚° ê²€ì¦
- í´ë¦¬í•‘ ë™ì‘ í™•ì¸
- KL divergence ê³„ì‚° ê²€ì¦
"""

import torch
import torch.nn.functional as F
import logging
from qwen import QWENModel, QWENGRPOConfig

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def test_log_prob_consistency():
    """ë¡œê·¸ í™•ë¥  ê³„ì‚° ì¼ê´€ì„± í…ŒìŠ¤íŠ¸"""
    print("ğŸ” ë¡œê·¸ í™•ë¥  ê³„ì‚° ì¼ê´€ì„± í…ŒìŠ¤íŠ¸")
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    config = QWENGRPOConfig(
        batch_size=1,
        num_rollouts=1,
        max_new_tokens=10
    )
    model = QWENModel(device="cuda", grpo_config=config)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    user_prompt = "cat"
    
    # 1. ìƒì„± ì‹œì ì˜ ë¡œê·¸ í™•ë¥  ê³„ì‚°
    enhanced_prompt, generation_log_prob = model.generate_grpo_enhanced_prompt(user_prompt)
    print(f"ğŸ“ ìƒì„±ëœ í”„ë¡¬í”„íŠ¸: {enhanced_prompt}")
    print(f"ğŸ“Š ìƒì„± ì‹œì  ë¡œê·¸ í™•ë¥ : {generation_log_prob:.6f}")
    
    # 2. í˜„ì¬ ëª¨ë¸ë¡œ ì¬ê³„ì‚°
    current_log_prob = model.calculate_log_prob_for_grpo(user_prompt, enhanced_prompt)
    print(f"ğŸ“Š í˜„ì¬ ëª¨ë¸ ë¡œê·¸ í™•ë¥ : {current_log_prob:.6f}")
    
    # 3. ì°¸ì¡° ëª¨ë¸ë¡œ ê³„ì‚°
    ref_log_prob = model.get_ref_model_log_prob(user_prompt, enhanced_prompt)
    print(f"ğŸ“Š ì°¸ì¡° ëª¨ë¸ ë¡œê·¸ í™•ë¥ : {ref_log_prob:.6f}")
    
    # 4. ì¼ê´€ì„± ê²€ì‚¬
    log_prob_diff = abs(generation_log_prob - current_log_prob)
    print(f"ğŸ” ìƒì„± vs í˜„ì¬ ëª¨ë¸ ì°¨ì´: {log_prob_diff:.6f}")
    
    if log_prob_diff < 0.001:
        print("âœ… ë¡œê·¸ í™•ë¥  ê³„ì‚° ì¼ê´€ì„± PASS")
    else:
        print("âŒ ë¡œê·¸ í™•ë¥  ê³„ì‚° ì¼ê´€ì„± FAIL - ì°¨ì´ê°€ ë„ˆë¬´ í¼")
    
    return {
        'generation_log_prob': generation_log_prob.item(),
        'current_log_prob': current_log_prob.item(),
        'ref_log_prob': ref_log_prob.item(),
        'consistency_check': log_prob_diff < 0.001
    }

def test_grpo_ratio_calculation():
    """GRPO ë¹„ìœ¨ ê³„ì‚° ê²€ì¦"""
    print("\nğŸ” GRPO ë¹„ìœ¨ ê³„ì‚° ê²€ì¦")
    
    # í…ŒìŠ¤íŠ¸ ë¡œê·¸ í™•ë¥  ê°’ë“¤
    old_log_probs = torch.tensor([-5.2, -4.8, -6.1, -5.5])
    current_log_probs = torch.tensor([-5.0, -4.9, -6.0, -5.3])
    
    # ë¡œê·¸ ë¹„ìœ¨ ê³„ì‚°
    log_ratio = current_log_probs - old_log_probs
    print(f"ğŸ“Š ë¡œê·¸ ë¹„ìœ¨: {log_ratio}")
    
    # ë¹„ìœ¨ ê³„ì‚°
    ratio = torch.exp(log_ratio)
    print(f"ğŸ“Š ë¹„ìœ¨ (Ï€_new/Ï€_old): {ratio}")
    
    # í´ë¦¬í•‘ ì ìš©
    clip_ratio = 0.1
    clipped_ratio = torch.clamp(ratio, 1 - clip_ratio, 1 + clip_ratio)
    print(f"ğŸ“Š í´ë¦¬í•‘ëœ ë¹„ìœ¨ (ë²”ìœ„: {1-clip_ratio:.1f}-{1+clip_ratio:.1f}): {clipped_ratio}")
    
    # í´ë¦¬í•‘ íš¨ê³¼ í™•ì¸
    clipping_applied = torch.any(ratio != clipped_ratio)
    print(f"ğŸ” í´ë¦¬í•‘ ì ìš©ë¨: {clipping_applied}")
    
    return {
        'log_ratio': log_ratio.tolist(),
        'ratio': ratio.tolist(),
        'clipped_ratio': clipped_ratio.tolist(),
        'clipping_applied': clipping_applied.item()
    }

def test_policy_loss_calculation():
    """ì •ì±… ì†ì‹¤ ê³„ì‚° ê²€ì¦"""
    print("\nğŸ” ì •ì±… ì†ì‹¤ ê³„ì‚° ê²€ì¦")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    advantages = torch.tensor([0.5, -0.2, 0.8, -0.1])
    ratio = torch.tensor([1.05, 0.95, 1.15, 0.88])
    clip_ratio = 0.1
    
    # í´ë¦¬í•‘ëœ ë¹„ìœ¨
    clipped_ratio = torch.clamp(ratio, 1 - clip_ratio, 1 + clip_ratio)
    
    # ì •ì±… ëª©ì  í•¨ìˆ˜
    policy_obj1 = ratio * advantages
    policy_obj2 = clipped_ratio * advantages
    
    print(f"ğŸ“Š Advantages: {advantages}")
    print(f"ğŸ“Š Ratio: {ratio}")
    print(f"ğŸ“Š Clipped ratio: {clipped_ratio}")
    print(f"ğŸ“Š Policy obj 1 (ratio * adv): {policy_obj1}")
    print(f"ğŸ“Š Policy obj 2 (clipped * adv): {policy_obj2}")
    
    # ì •ì±… ì†ì‹¤ (ìŒìˆ˜ ìµœì†Œê°’)
    policy_loss = -torch.min(policy_obj1, policy_obj2).mean()
    print(f"ğŸ“Š Policy loss: {policy_loss:.6f}")
    
    # í´ë¦¬í•‘ íš¨ê³¼ ë¶„ì„
    clipping_effect = torch.sum(policy_obj1 != policy_obj2)
    print(f"ğŸ” í´ë¦¬í•‘ì´ ì ìš©ëœ ìš”ì†Œ ìˆ˜: {clipping_effect}")
    
    return {
        'policy_loss': policy_loss.item(),
        'clipping_effect': clipping_effect.item(),
        'policy_obj1': policy_obj1.tolist(),
        'policy_obj2': policy_obj2.tolist()
    }

def test_kl_divergence():
    """KL divergence ê³„ì‚° ê²€ì¦"""
    print("\nğŸ” KL divergence ê³„ì‚° ê²€ì¦")
    
    # í…ŒìŠ¤íŠ¸ ë¡œê·¸ í™•ë¥ 
    current_log_probs = torch.tensor([-5.0, -4.9, -6.0, -5.3])
    ref_log_probs = torch.tensor([-5.2, -4.8, -6.1, -5.5])
    
    # KL divergence ê³„ì‚° (current - reference)
    kl_div = (current_log_probs - ref_log_probs).mean()
    print(f"ğŸ“Š Current log probs: {current_log_probs}")
    print(f"ğŸ“Š Reference log probs: {ref_log_probs}")
    print(f"ğŸ“Š KL divergence: {kl_div:.6f}")
    
    # KL í˜ë„í‹°
    kl_coef = 0.02
    kl_penalty = kl_coef * kl_div
    print(f"ğŸ“Š KL penalty (coef={kl_coef}): {kl_penalty:.6f}")
    
    return {
        'kl_div': kl_div.item(),
        'kl_penalty': kl_penalty.item()
    }

def test_full_grpo_update():
    """ì „ì²´ GRPO ì—…ë°ì´íŠ¸ í”„ë¡œì„¸ìŠ¤ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ” ì „ì²´ GRPO ì—…ë°ì´íŠ¸ í”„ë¡œì„¸ìŠ¤ í…ŒìŠ¤íŠ¸")
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    config = QWENGRPOConfig(
        batch_size=2,
        num_rollouts=1,
        max_new_tokens=10,
        clip_ratio=0.1,
        kl_coef=0.02
    )
    model = QWENModel(device="cuda", grpo_config=config)
    
    # ê°€ì§œ ê²½í—˜ ë°ì´í„° ìƒì„±
    experiences = []
    for i in range(2):
        user_prompt = f"test prompt {i}"
        enhanced_prompt, log_prob = model.generate_grpo_enhanced_prompt(user_prompt)
        
        experience = {
            'user_prompt': user_prompt,
            'enhanced_prompt': enhanced_prompt,
            'log_prob': log_prob,
            'reward': 0.5 + i * 0.1,
            'group_advantage': 0.2 + i * 0.1
        }
        experiences.append(experience)
    
    print(f"ğŸ“ ìƒì„±ëœ ê²½í—˜ ìˆ˜: {len(experiences)}")
    
    # GRPO ì—…ë°ì´íŠ¸ ì‹¤í–‰
    metrics = model.update_grpo_policy(experiences)
    
    print("ğŸ“Š ì—…ë°ì´íŠ¸ ê²°ê³¼:")
    for key, value in metrics.items():
        print(f"  {key}: {value:.6f}")
    
    # ê²°ê³¼ ê²€ì¦
    success_checks = []
    success_checks.append(('policy_loss', not torch.isnan(torch.tensor(metrics['policy_loss']))))
    success_checks.append(('kl_div', not torch.isnan(torch.tensor(metrics['kl_div']))))
    success_checks.append(('total_loss', not torch.isnan(torch.tensor(metrics['total_loss']))))
    
    print("\nâœ… ê²€ì¦ ê²°ê³¼:")
    for check_name, passed in success_checks:
        status = "PASS" if passed else "FAIL"
        print(f"  {check_name}: {status}")
    
    return metrics

def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸš€ GRPO ì•Œê³ ë¦¬ì¦˜ ì •í™•ì„± ê²€ì¦ ì‹œì‘")
    
    try:
        # 1. ë¡œê·¸ í™•ë¥  ì¼ê´€ì„± í…ŒìŠ¤íŠ¸
        log_prob_results = test_log_prob_consistency()
        
        # 2. ë¹„ìœ¨ ê³„ì‚° í…ŒìŠ¤íŠ¸
        ratio_results = test_grpo_ratio_calculation()
        
        # 3. ì •ì±… ì†ì‹¤ ê³„ì‚° í…ŒìŠ¤íŠ¸
        policy_loss_results = test_policy_loss_calculation()
        
        # 4. KL divergence í…ŒìŠ¤íŠ¸
        kl_results = test_kl_divergence()
        
        # 5. ì „ì²´ ì—…ë°ì´íŠ¸ í”„ë¡œì„¸ìŠ¤ í…ŒìŠ¤íŠ¸
        full_update_results = test_full_grpo_update()
        
        print("\nğŸ¯ ì „ì²´ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print("ğŸ“Š í•µì‹¬ ì§€í‘œ:")
        print(f"  ë¡œê·¸ í™•ë¥  ì¼ê´€ì„±: {'âœ…' if log_prob_results['consistency_check'] else 'âŒ'}")
        print(f"  í´ë¦¬í•‘ ì ìš©: {'âœ…' if ratio_results['clipping_applied'] else 'âš ï¸'}")
        print(f"  ì •ì±… ì†ì‹¤ ê³„ì‚°: {'âœ…' if not torch.isnan(torch.tensor(policy_loss_results['policy_loss'])) else 'âŒ'}")
        print(f"  KL divergence: {'âœ…' if not torch.isnan(torch.tensor(kl_results['kl_div'])) else 'âŒ'}")
        print(f"  ì „ì²´ ì—…ë°ì´íŠ¸: {'âœ…' if 'error' not in full_update_results else 'âŒ'}")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 