"""
CLIP Reward Calculator
=====================

CLIP ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ê³  ë³´ìƒ ì‹ í˜¸ë¥¼ ìƒì„±í•˜ëŠ” ëª¨ë“ˆì…ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
1. í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê³„ì‚°
2. ë‹¤ì–‘í•œ ë³´ìƒ í•¨ìˆ˜ ì œê³µ
3. ë°°ì¹˜ ì²˜ë¦¬ ì§€ì›
4. ì •ê·œí™” ë° ìŠ¤ì¼€ì¼ë§

Author: AI Assistant
Date: 2025-01-22
"""

import torch
import torch.nn.functional as F
from transformers import CLIPProcessor, CLIPModel
from PIL import Image
from typing import List, Dict, Optional, Union, Tuple
import logging
import numpy as np

logger = logging.getLogger(__name__)

class CLIPRewardCalculator:
    """
    CLIPì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê¸°ë°˜ ë³´ìƒì„ ê³„ì‚°í•˜ëŠ” í´ë˜ìŠ¤
    
    ì´ í´ë˜ìŠ¤ëŠ” GRPO í•™ìŠµì—ì„œ í•µì‹¬ì ì¸ ì—­í• ì„ í•©ë‹ˆë‹¤:
    1. ìƒì„±ëœ ì´ë¯¸ì§€ì™€ í”„ë¡¬í”„íŠ¸ ê°„ì˜ ìœ ì‚¬ë„ ì¸¡ì •
    2. ìœ ì‚¬ë„ë¥¼ ë³´ìƒ ì‹ í˜¸ë¡œ ë³€í™˜
    3. í•™ìŠµ ì•ˆì •ì„±ì„ ìœ„í•œ ì •ê·œí™”
    
    Attributes:
        model_name (str): ì‚¬ìš©í•  CLIP ëª¨ë¸ ì´ë¦„
        processor: CLIP í”„ë¡œì„¸ì„œ ê°ì²´
        model: CLIP ëª¨ë¸ ê°ì²´
        device: ì—°ì‚° ë””ë°”ì´ìŠ¤
        reward_config (dict): ë³´ìƒ ê³„ì‚° ì„¤ì •
    """
    
    def __init__(self,
                 model_name: str = "openai/clip-vit-base-patch32",
                 device: str = "auto",
                 reward_scale: float = 1.0,
                 reward_offset: float = 0.0,
                 temperature: float = 1.0,
                 reward_weights: Optional[Dict[str, float]] = None):
        """
        CLIP Reward Calculator ì´ˆê¸°í™”
        
        Args:
            model_name (str): ì‚¬ìš©í•  CLIP ëª¨ë¸ ì´ë¦„
            device (str): ë””ë°”ì´ìŠ¤ ì„¤ì • ("auto", "mps", "cuda", "cpu")
            reward_scale (float): ë³´ìƒ ìŠ¤ì¼€ì¼ë§ íŒ©í„°
            reward_offset (float): ë³´ìƒ ì˜¤í”„ì…‹
            temperature (float): ì†Œí”„íŠ¸ë§¥ìŠ¤ ì˜¨ë„ (ìœ ì‚¬ë„ ì¡°ì ˆ)
            reward_weights (Dict[str, float], optional): ë‹¤ì¤‘ ë³´ìƒ ê°€ì¤‘ì¹˜
        """
        self.model_name = model_name
        self.reward_scale = reward_scale
        self.reward_offset = reward_offset
        self.temperature = temperature
        
        # ë‹¤ì¤‘ ë³´ìƒ ê°€ì¤‘ì¹˜ ì„¤ì •
        if reward_weights is None:
            self.reward_weights = {
                'clip_similarity': 0.6,
                'image_quality': 0.3,
                'semantic_consistency': 0.1
            }
        else:
            self.reward_weights = reward_weights
            
        logger.info(f"ğŸ¯ Multi-reward weights: {self.reward_weights}")
        
        # ë””ë°”ì´ìŠ¤ ìë™ ì„ íƒ
        if device == "auto":
            if torch.backends.mps.is_available():
                self.device = torch.device("mps")
                logger.info("ğŸ Using Apple Silicon MPS for CLIP")
            elif torch.cuda.is_available():
                self.device = torch.device("cuda")
                logger.info("ğŸš€ Using CUDA GPU for CLIP")
            else:
                self.device = torch.device("cpu")
                logger.info("ğŸ’» Using CPU for CLIP")
        else:
            self.device = torch.device(device)
        
        # ë³´ìƒ ê³„ì‚° ì„¤ì •
        self.reward_config = {
            'scale': self.reward_scale,
            'offset': self.reward_offset,
            'temperature': self.temperature,
            'normalize': True,      # ë³´ìƒ ì •ê·œí™” ì—¬ë¶€
            'clip_range': (-1, 1),  # ë³´ìƒ í´ë¦¬í•‘ ë²”ìœ„
        }
        
        # ëª¨ë¸ ë¡œë“œ
        self._load_model()
    
    def _load_model(self):
        """
        CLIP ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œë¥¼ ë¡œë“œí•˜ëŠ” ë‚´ë¶€ ë©”ì„œë“œ
        
        ì´ ë©”ì„œë“œëŠ”:
        1. CLIP í”„ë¡œì„¸ì„œ ë¡œë“œ
        2. CLIP ëª¨ë¸ ë¡œë“œ ë° ë””ë°”ì´ìŠ¤ ì´ë™
        3. í‰ê°€ ëª¨ë“œ ì„¤ì •
        4. ì—ëŸ¬ ì²˜ë¦¬
        """
        try:
            logger.info(f"ğŸ“¥ Loading CLIP model: {self.model_name}")
            
            # CLIP í”„ë¡œì„¸ì„œ ë¡œë“œ
            self.processor = CLIPProcessor.from_pretrained(self.model_name)
            
            # CLIP ëª¨ë¸ ë¡œë“œ
            self.model = CLIPModel.from_pretrained(
                self.model_name,
                torch_dtype=torch.float16 if self.device.type in ['cuda', 'mps'] else torch.float32
            )
            
            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            self.model = self.model.to(self.device)
            self.model.eval()  # í‰ê°€ ëª¨ë“œ ì„¤ì •
            
            logger.info(f"âœ… CLIP model loaded successfully on {self.device}")
            
        except Exception as e:
            logger.error(f"âŒ Failed to load CLIP model: {e}")
            raise RuntimeError(f"CLIP model loading failed: {e}")
    
    def calculate_reward(self, 
                        image: Image.Image, 
                        text: str) -> float:
        """
        ë‹¨ì¼ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìŒì— ëŒ€í•œ ë³´ìƒ ê³„ì‚°
        
        ì´ ë©”ì„œë“œëŠ”:
        1. ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ CLIPìœ¼ë¡œ ì¸ì½”ë”©
        2. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        3. ë³´ìƒ í•¨ìˆ˜ ì ìš©
        4. ì •ê·œí™” ë° ìŠ¤ì¼€ì¼ë§
        
        Args:
            image (PIL.Image.Image): í‰ê°€í•  ì´ë¯¸ì§€
            text (str): ë¹„êµí•  í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
            
        Returns:
            float: ê³„ì‚°ëœ ë³´ìƒ ê°’ (-1.0 ~ 1.0)
            
        Example:
            reward = calculator.calculate_reward(generated_image, "a cute cat")
            # reward: 0.85 (ë†’ì€ ìœ ì‚¬ë„)
        """
        try:
            # ì…ë ¥ ì „ì²˜ë¦¬
            inputs = self.processor(
                text=[text], 
                images=[image], 
                return_tensors="pt", 
                padding=True
            )
            
            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # CLIPìœ¼ë¡œ íŠ¹ì„± ì¶”ì¶œ
            with torch.no_grad():
                outputs = self.model(**inputs)
                
                # ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ì„ë² ë”© ì¶”ì¶œ
                image_embeds = outputs.image_embeds
                text_embeds = outputs.text_embeds
                
                # ì„ë² ë”© ì •ê·œí™”
                image_embeds = F.normalize(image_embeds, p=2, dim=-1)
                text_embeds = F.normalize(text_embeds, p=2, dim=-1)
                
                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                similarity = torch.cosine_similarity(image_embeds, text_embeds, dim=-1)
                
                # ì˜¨ë„ ìŠ¤ì¼€ì¼ë§ ì ìš©
                similarity = similarity / self.temperature
                
                # ë³´ìƒ ë³€í™˜
                reward = self._transform_similarity_to_reward(similarity.item())
                
                logger.debug(f"ğŸ’° Reward calculated: {reward:.4f} for text: '{text[:30]}...'")
                
                return reward
                
        except Exception as e:
            logger.warning(f"âš ï¸ Reward calculation failed: {e}")
            # ì‹¤íŒ¨ ì‹œ ì¤‘ë¦½ ë³´ìƒ ë°˜í™˜
            return 0.0
    
    def calculate_rewards_batch(self, 
                               images: List[Image.Image], 
                               texts: List[str]) -> List[float]:
        """
        ì—¬ëŸ¬ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìŒì— ëŒ€í•œ ë°°ì¹˜ ë³´ìƒ ê³„ì‚°
        
        Args:
            images (List[PIL.Image.Image]): í‰ê°€í•  ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸
            texts (List[str]): ë¹„êµí•  í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            List[float]: ê³„ì‚°ëœ ë³´ìƒ ê°’ ë¦¬ìŠ¤íŠ¸
        """
        if len(images) != len(texts):
            raise ValueError("Images and texts must have the same length")
        
        rewards = []
        
        try:
            # ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì…ë ¥ ì¤€ë¹„
            inputs = self.processor(
                text=texts, 
                images=images, 
                return_tensors="pt", 
                padding=True
            )
            
            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # CLIPìœ¼ë¡œ ë°°ì¹˜ ì²˜ë¦¬
            with torch.no_grad():
                outputs = self.model(**inputs)
                
                # ì„ë² ë”© ì¶”ì¶œ ë° ì •ê·œí™”
                image_embeds = F.normalize(outputs.image_embeds, p=2, dim=-1)
                text_embeds = F.normalize(outputs.text_embeds, p=2, dim=-1)
                
                # ë°°ì¹˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                similarities = torch.cosine_similarity(image_embeds, text_embeds, dim=-1)
                
                # ì˜¨ë„ ìŠ¤ì¼€ì¼ë§
                similarities = similarities / self.temperature
                
                # ë³´ìƒ ë³€í™˜
                for similarity in similarities:
                    reward = self._transform_similarity_to_reward(similarity.item())
                    rewards.append(reward)
                
                logger.debug(f"ğŸ’° Batch rewards calculated: {len(rewards)} items")
                
        except Exception as e:
            logger.warning(f"âš ï¸ Batch reward calculation failed: {e}")
            # ì‹¤íŒ¨ ì‹œ ì¤‘ë¦½ ë³´ìƒë“¤ ë°˜í™˜
            rewards = [0.0] * len(images)
        
        return rewards
    
    def _transform_similarity_to_reward(self, similarity: float) -> float:
        """
        CLIP ìœ ì‚¬ë„ë¥¼ ë³´ìƒ ì‹ í˜¸ë¡œ ë³€í™˜í•˜ëŠ” ë‚´ë¶€ ë©”ì„œë“œ
        
        ì´ ë©”ì„œë“œëŠ”:
        1. ìœ ì‚¬ë„ë¥¼ ë³´ìƒ ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
        2. ì˜¤í”„ì…‹ ì ìš©
        3. ì •ê·œí™” (ì„ íƒì )
        4. í´ë¦¬í•‘
        
        Args:
            similarity (float): CLIP ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (-1 ~ 1)
            
        Returns:
            float: ë³€í™˜ëœ ë³´ìƒ ê°’
        """
        # ê¸°ë³¸ ë³´ìƒ ë³€í™˜: ìœ ì‚¬ë„ * ìŠ¤ì¼€ì¼ + ì˜¤í”„ì…‹
        reward = similarity * self.reward_config['scale'] + self.reward_config['offset']
        
        # ì •ê·œí™” ì ìš© (ì„ íƒì )
        if self.reward_config['normalize']:
            # ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ë¡œ ë¶€ë“œëŸ¬ìš´ ì •ê·œí™”
            reward = torch.sigmoid(torch.tensor(reward)).item()
            # -1 ~ 1 ë²”ìœ„ë¡œ ë³€í™˜
            reward = 2 * reward - 1
        
        # í´ë¦¬í•‘ ì ìš©
        clip_min, clip_max = self.reward_config['clip_range']
        reward = max(clip_min, min(clip_max, reward))
        
        return reward
    
    def calculate_quality_reward(self, image: Image.Image) -> float:
        """
        ì´ë¯¸ì§€ í’ˆì§ˆ ê¸°ë°˜ ë³´ìƒ ê³„ì‚°
        
        ì´ ë©”ì„œë“œëŠ” í…ìŠ¤íŠ¸ ì—†ì´ ì´ë¯¸ì§€ ìì²´ì˜ í’ˆì§ˆì„ í‰ê°€í•©ë‹ˆë‹¤.
        ì¼ë°˜ì ì¸ ê³ í’ˆì§ˆ ì´ë¯¸ì§€ íŠ¹ì„±ê³¼ì˜ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤.
        
        Args:
            image (PIL.Image.Image): í‰ê°€í•  ì´ë¯¸ì§€
            
        Returns:
            float: í’ˆì§ˆ ê¸°ë°˜ ë³´ìƒ ê°’
        """
        quality_prompts = [
            "high quality image",
            "professional photography",
            "detailed and sharp image",
            "well-composed photograph"
        ]
        
        # ê° í’ˆì§ˆ í”„ë¡¬í”„íŠ¸ì™€ì˜ ìœ ì‚¬ë„ ê³„ì‚°
        quality_scores = []
        for prompt in quality_prompts:
            score = self.calculate_reward(image, prompt)
            quality_scores.append(score)
        
        # í‰ê·  í’ˆì§ˆ ì ìˆ˜ ë°˜í™˜
        quality_reward = np.mean(quality_scores)
        
        logger.debug(f"ğŸ¨ Quality reward: {quality_reward:.4f}")
        return quality_reward
    
    def calculate_semantic_consistency(self, 
                                     image: Image.Image, 
                                     original_prompt: str, 
                                     enhanced_prompt: str) -> float:
        """
        ì›ë³¸ í”„ë¡¬í”„íŠ¸ì™€ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ê°„ì˜ ì˜ë¯¸ì  ì¼ê´€ì„± í‰ê°€
        
        Args:
            image (PIL.Image.Image): ìƒì„±ëœ ì´ë¯¸ì§€
            original_prompt (str): ì›ë³¸ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
            enhanced_prompt (str): VLMì´ ê°œì„ í•œ í”„ë¡¬í”„íŠ¸
            
        Returns:
            float: ì˜ë¯¸ì  ì¼ê´€ì„± ì ìˆ˜
        """
        # ê° í”„ë¡¬í”„íŠ¸ì™€ ì´ë¯¸ì§€ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚°
        original_reward = self.calculate_reward(image, original_prompt)
        enhanced_reward = self.calculate_reward(image, enhanced_prompt)
        
        # ì¼ê´€ì„± ì ìˆ˜: ë‘ ë³´ìƒì˜ ìµœì†Œê°’ (ë‘˜ ë‹¤ ë†’ì•„ì•¼ ì¼ê´€ì„± ìˆìŒ)
        consistency_score = min(original_reward, enhanced_reward)
        
        logger.debug(f"ğŸ”— Semantic consistency: {consistency_score:.4f}")
        return consistency_score
    
    def get_model_info(self) -> Dict:
        """
        ëª¨ë¸ ì •ë³´ ë°˜í™˜ (ë””ë²„ê¹… ë° ë¡œê¹…ìš©)
        
        Returns:
            Dict: ëª¨ë¸ ê´€ë ¨ ì •ë³´
        """
        return {
            "model_name": self.model_name,
            "device": str(self.device),
            "reward_config": self.reward_config,
            "parameters": sum(p.numel() for p in self.model.parameters()),
        }
    
    def update_reward_config(self, **kwargs):
        """
        ë³´ìƒ ê³„ì‚° ì„¤ì • ì—…ë°ì´íŠ¸
        
        Args:
            **kwargs: ì—…ë°ì´íŠ¸í•  ì„¤ì •ë“¤
        """
        self.reward_config.update(kwargs)
        logger.info(f"ğŸ”§ Reward config updated: {kwargs}")


class MultiRewardCalculator:
    """
    ì—¬ëŸ¬ ë³´ìƒ í•¨ìˆ˜ë¥¼ ì¡°í•©í•˜ì—¬ ì¢…í•©ì ì¸ ë³´ìƒì„ ê³„ì‚°í•˜ëŠ” í´ë˜ìŠ¤
    
    ì´ í´ë˜ìŠ¤ëŠ”:
    1. CLIP ìœ ì‚¬ë„ ë³´ìƒ
    2. ì´ë¯¸ì§€ í’ˆì§ˆ ë³´ìƒ  
    3. ì˜ë¯¸ì  ì¼ê´€ì„± ë³´ìƒ
    4. ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìµœì¢… ë³´ìƒ ê³„ì‚°
    """
    
    def __init__(self,
                 clip_calculator: CLIPRewardCalculator,
                 weights: Optional[Dict[str, float]] = None):
        """
        Multi Reward Calculator ì´ˆê¸°í™”
        
        Args:
            clip_calculator (CLIPRewardCalculator): CLIP ë³´ìƒ ê³„ì‚°ê¸°
            weights (Dict[str, float], optional): ê° ë³´ìƒì˜ ê°€ì¤‘ì¹˜
        """
        self.clip_calculator = clip_calculator
        
        # ê¸°ë³¸ ê°€ì¤‘ì¹˜ ì„¤ì •
        self.weights = weights or {
            'clip_similarity': 0.6,    # CLIP ìœ ì‚¬ë„ (ì£¼ìš”)
            'image_quality': 0.3,      # ì´ë¯¸ì§€ í’ˆì§ˆ
            'semantic_consistency': 0.1 # ì˜ë¯¸ì  ì¼ê´€ì„±
        }
        
        # ê°€ì¤‘ì¹˜ ì •ê·œí™”
        total_weight = sum(self.weights.values())
        self.weights = {k: v/total_weight for k, v in self.weights.items()}
        
        logger.info(f"ğŸ¯ Multi-reward weights: {self.weights}")
    
    def calculate_comprehensive_reward(self,
                                     image: Image.Image,
                                     original_prompt: str,
                                     enhanced_prompt: str) -> Dict[str, float]:
        """
        ì¢…í•©ì ì¸ ë³´ìƒ ê³„ì‚°
        
        Args:
            image (PIL.Image.Image): ìƒì„±ëœ ì´ë¯¸ì§€
            original_prompt (str): ì›ë³¸ í”„ë¡¬í”„íŠ¸
            enhanced_prompt (str): ê°œì„ ëœ í”„ë¡¬í”„íŠ¸
            
        Returns:
            Dict[str, float]: ê° ë³´ìƒê³¼ ìµœì¢… ì¢…í•© ë³´ìƒ
        """
        rewards = {}
        
        # 1. CLIP ìœ ì‚¬ë„ ë³´ìƒ (ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ê¸°ì¤€)
        rewards['clip_similarity'] = self.clip_calculator.calculate_reward(
            image, enhanced_prompt
        )
        
        # 2. ì´ë¯¸ì§€ í’ˆì§ˆ ë³´ìƒ
        rewards['image_quality'] = self.clip_calculator.calculate_quality_reward(image)
        
        # 3. ì˜ë¯¸ì  ì¼ê´€ì„± ë³´ìƒ
        rewards['semantic_consistency'] = self.clip_calculator.calculate_semantic_consistency(
            image, original_prompt, enhanced_prompt
        )
        
        # 4. ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìµœì¢… ë³´ìƒ ê³„ì‚°
        final_reward = sum(
            rewards[key] * self.weights[key] 
            for key in rewards.keys() 
            if key in self.weights
        )
        
        rewards['final_reward'] = final_reward
        
        logger.debug(f"ğŸ† Comprehensive rewards: {rewards}")
        return rewards


if __name__ == "__main__":
    # CLIP Reward Calculator í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ğŸ§ª CLIP Reward Calculator Test")
    print("=" * 40)
    
    try:
        # CLIP ë³´ìƒ ê³„ì‚°ê¸° ì´ˆê¸°í™”
        calculator = CLIPRewardCalculator(
            model_name="openai/clip-vit-base-patch32",
            device="auto",
            reward_scale=1.0
        )
        
        print("âœ… CLIP Reward Calculator initialized successfully")
        print(f"ğŸ“Š Model info: {calculator.get_model_info()}")
        
        # í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±
        test_image = Image.new('RGB', (224, 224), color='red')
        
        # í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë“¤
        test_cases = [
            ("red color", "ë†’ì€ ìœ ì‚¬ë„ ì˜ˆìƒ"),
            ("blue sky", "ë‚®ì€ ìœ ì‚¬ë„ ì˜ˆìƒ"),
            ("colorful image", "ì¤‘ê°„ ìœ ì‚¬ë„ ì˜ˆìƒ")
        ]
        
        print("\nğŸ”„ Testing reward calculation:")
        for prompt, description in test_cases:
            reward = calculator.calculate_reward(test_image, prompt)
            print(f"  '{prompt}' â†’ {reward:.4f} ({description})")
        
        # í’ˆì§ˆ ë³´ìƒ í…ŒìŠ¤íŠ¸
        quality_reward = calculator.calculate_quality_reward(test_image)
        print(f"\nğŸ¨ Quality reward: {quality_reward:.4f}")
        
        # Multi-reward í…ŒìŠ¤íŠ¸
        multi_calculator = MultiRewardCalculator(calculator)
        comprehensive_rewards = multi_calculator.calculate_comprehensive_reward(
            test_image, "red", "bright red color"
        )
        print(f"\nğŸ† Comprehensive rewards: {comprehensive_rewards}")
        
        print("\nâœ… All tests passed!")
        
    except Exception as e:
        print(f"âŒ Test failed: {e}")
    
    print("\nUsage:")
    print("from models.clip_reward import CLIPRewardCalculator")
    print("calculator = CLIPRewardCalculator()")
    print("reward = calculator.calculate_reward(image, 'a cat')") 