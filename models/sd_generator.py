"""
Stable Diffusion 3 Generator
============================

Stable Diffusion 3ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¡œë¶€í„° ê³ í’ˆì§ˆ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ëª¨ë“ˆì…ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
1. í…ìŠ¤íŠ¸-íˆ¬-ì´ë¯¸ì§€ ìƒì„± (Text-to-Image)
2. ì´ë¯¸ì§€ í’ˆì§ˆ ìµœì í™”
3. ë°°ì¹˜ ìƒì„± ì§€ì›
4. ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ìƒì„±

Author: AI Assistant
Date: 2025-01-22
"""

import torch
from diffusers import StableDiffusion3Pipeline, DiffusionPipeline
from PIL import Image
from typing import List, Dict, Optional, Union, Tuple
import logging
import os
import numpy as np

logger = logging.getLogger(__name__)

class SD3Generator:
    """
    Stable Diffusion 3ë¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ìƒì„±ê¸° í´ë˜ìŠ¤
    
    ì´ í´ë˜ìŠ¤ëŠ” í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ ë°›ì•„ì„œ ê³ í’ˆì§ˆì˜ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    GRPO í•™ìŠµì—ì„œëŠ” ìƒì„±ëœ ì´ë¯¸ì§€ê°€ CLIP ë³´ìƒ ê³„ì‚°ì— ì‚¬ìš©ë©ë‹ˆë‹¤.
    
    Attributes:
        model_name (str): ì‚¬ìš©í•  SD3 ëª¨ë¸ ì´ë¦„
        pipeline: Diffusion íŒŒì´í”„ë¼ì¸ ê°ì²´
        device: ì—°ì‚° ë””ë°”ì´ìŠ¤
        generation_config (dict): ì´ë¯¸ì§€ ìƒì„± ì„¤ì •
    """
    
    def __init__(self,
                 model_name: str = "stabilityai/stable-diffusion-3-medium",
                 device: str = "auto",
                 height: int = 1024,
                 width: int = 1024,
                 num_inference_steps: int = 28,
                 guidance_scale: float = 7.0):
        """
        SD3 Generator ì´ˆê¸°í™”
        
        Args:
            model_name (str): ì‚¬ìš©í•  SD3 ëª¨ë¸ ì´ë¦„
            device (str): ë””ë°”ì´ìŠ¤ ì„¤ì • ("auto", "mps", "cuda", "cpu")
            height (int): ìƒì„±í•  ì´ë¯¸ì§€ ë†’ì´ (SD3ëŠ” 1024x1024 ê¶Œì¥)
            width (int): ìƒì„±í•  ì´ë¯¸ì§€ ë„ˆë¹„
            num_inference_steps (int): ì¶”ë¡  ìŠ¤í… ìˆ˜ (SD3ëŠ” 28ìŠ¤í… ê¶Œì¥)
            guidance_scale (float): ê°€ì´ë˜ìŠ¤ ìŠ¤ì¼€ì¼ (SD3ëŠ” 7.0 ê¶Œì¥)
        """
        self.model_name = model_name
        self.height = height
        self.width = width
        self.num_inference_steps = num_inference_steps
        self.guidance_scale = guidance_scale
        
        # ë””ë°”ì´ìŠ¤ ìë™ ì„ íƒ
        if device == "auto":
            if torch.backends.mps.is_available():
                self.device = "mps"
                logger.info("ğŸ Using Apple Silicon MPS for SD3")
            elif torch.cuda.is_available():
                self.device = "cuda"
                logger.info("ğŸš€ Using CUDA GPU for SD3")
            else:
                self.device = "cpu"
                logger.info("ğŸ’» Using CPU for SD3 (slow)")
        else:
            self.device = device
        
        # SD3 íŠ¹í™” ì´ë¯¸ì§€ ìƒì„± ì„¤ì •
        self.generation_config = {
            'height': self.height,
            'width': self.width,
            'num_inference_steps': self.num_inference_steps,
            'guidance_scale': self.guidance_scale,
            'num_images_per_prompt': 1,
            'generator': None,  # ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìœ„í•´ ì‹œë“œ ì„¤ì • ê°€ëŠ¥
            'max_sequence_length': 256,  # SD3 íŠ¹í™” ì„¤ì •
        }
        
        # íŒŒì´í”„ë¼ì¸ ë¡œë“œ
        self._load_pipeline()
    
    def _load_pipeline(self):
        """
        Stable Diffusion 3 íŒŒì´í”„ë¼ì¸ì„ ë¡œë“œí•˜ëŠ” ë‚´ë¶€ ë©”ì„œë“œ
        
        ì´ ë©”ì„œë“œëŠ”:
        1. SD3 íŒŒì´í”„ë¼ì¸ ë¡œë“œ
        2. ë””ë°”ì´ìŠ¤ ì„¤ì •
        3. ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •
        4. ì•ˆì „ ì²´í¬ ë¹„í™œì„±í™” (ì—°êµ¬ìš©)
        """
        try:
            logger.info(f"ğŸ“¥ Loading SD3 pipeline: {self.model_name}")
            
            # SD3 ëª¨ë¸ì¸ì§€ í™•ì¸í•˜ì—¬ ì ì ˆí•œ ë¡œë”© ë°©ì‹ ì„ íƒ
            if "stable-diffusion-3" in self.model_name.lower():
                # SD3 ì „ìš© ë¡œë”© ë°©ì‹
                self.pipeline = StableDiffusion3Pipeline.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.float16 if self.device in ['cuda', 'mps'] else torch.float32,
                    text_encoder_3=None,  # T5 í…ìŠ¤íŠ¸ ì¸ì½”ë” ë¹„í™œì„±í™” (ë©”ëª¨ë¦¬ ì ˆì•½)
                    transformer=None,  # í•„ìš”ì‹œ ì»¤ìŠ¤í…€ íŠ¸ëœìŠ¤í¬ë¨¸ ì‚¬ìš©
                )
            else:
                # ì¼ë°˜ SD ëª¨ë¸ ë¡œë”©
                self.pipeline = DiffusionPipeline.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.float16 if self.device in ['cuda', 'mps'] else torch.float32,
                    use_safetensors=True,
                    variant="fp16" if self.device in ['cuda', 'mps'] else None
                )
            
            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            self.pipeline = self.pipeline.to(self.device)
            
            # ë©”ëª¨ë¦¬ ìµœì í™” (í•„ìš”í•œ ê²½ìš°)
            if self.device == "mps":
                # Apple Silicon ìµœì í™”
                self.pipeline.enable_attention_slicing()
            elif self.device == "cuda":
                # CUDA ìµœì í™”
                if hasattr(self.pipeline, 'enable_memory_efficient_attention'):
                    self.pipeline.enable_memory_efficient_attention()
                self.pipeline.enable_attention_slicing()
            
            # ì•ˆì „ ì²´í¬ ë¹„í™œì„±í™” (ì—°êµ¬ ëª©ì )
            if hasattr(self.pipeline, 'safety_checker'):
                self.pipeline.safety_checker = None
            if hasattr(self.pipeline, 'requires_safety_checker'):
                self.pipeline.requires_safety_checker = False
            
            logger.info(f"âœ… SD3 pipeline loaded successfully on {self.device}")
            
        except Exception as e:
            logger.error(f"âŒ Failed to load SD3 pipeline: {e}")
            logger.info("ğŸ”„ Trying alternative loading method...")
            
            # ëŒ€ì•ˆ ë¡œë”© ë°©ì‹
            try:
                self.pipeline = DiffusionPipeline.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.float16 if self.device in ['cuda', 'mps'] else torch.float32,
                    use_safetensors=True,
                )
                self.pipeline = self.pipeline.to(self.device)
                logger.info(f"âœ… SD3 pipeline loaded with alternative method on {self.device}")
                
            except Exception as e2:
                logger.error(f"âŒ Alternative loading also failed: {e2}")
                raise RuntimeError(f"SD3 pipeline loading failed: {e} | Alternative: {e2}")
    
    def generate_image(self, 
                      prompt: str, 
                      negative_prompt: Optional[str] = None,
                      seed: Optional[int] = None) -> Image.Image:
        """
        ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ë¡œë¶€í„° ì´ë¯¸ì§€ ìƒì„±
        
        ì´ ë©”ì„œë“œëŠ” í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ ë°›ì•„ì„œ:
        1. í”„ë¡¬í”„íŠ¸ ì „ì²˜ë¦¬
        2. SD3ë¡œ ì´ë¯¸ì§€ ìƒì„±
        3. í›„ì²˜ë¦¬ ë° í’ˆì§ˆ í™•ì¸
        4. PIL Image ë°˜í™˜
        
        Args:
            prompt (str): ì´ë¯¸ì§€ ìƒì„±ìš© í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
            negative_prompt (str, optional): ì›í•˜ì§€ ì•ŠëŠ” ìš”ì†Œ ì§€ì •
            seed (int, optional): ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìœ„í•œ ì‹œë“œ
            
        Returns:
            PIL.Image.Image: ìƒì„±ëœ ì´ë¯¸ì§€
            
        Example:
            image = generator.generate_image(
                "a fluffy orange cat sitting on a windowsill, professional photography"
            )
        """
        try:
            # ì‹œë“œ ì„¤ì • (ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼)
            generator = None
            if seed is not None:
                generator = torch.Generator(device=self.device).manual_seed(seed)
                self.generation_config['generator'] = generator
            
            # ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸ ê¸°ë³¸ê°’ ì„¤ì •
            if negative_prompt is None:
                negative_prompt = "blurry, low quality, distorted, deformed, ugly, bad anatomy"
            
            logger.debug(f"ğŸ¨ Generating image for prompt: '{prompt[:50]}...'")
            
            # ì´ë¯¸ì§€ ìƒì„±
            with torch.no_grad():
                result = self.pipeline(
                    prompt=prompt,
                    negative_prompt=negative_prompt,
                    **self.generation_config
                )
            
            # ìƒì„±ëœ ì´ë¯¸ì§€ ì¶”ì¶œ
            generated_image = result.images[0]
            
            # ì´ë¯¸ì§€ í’ˆì§ˆ ê²€ì¦
            if self._validate_image(generated_image):
                logger.debug("âœ… Image generated successfully")
                return generated_image
            else:
                logger.warning("âš ï¸ Generated image quality check failed")
                return generated_image  # ì‹¤íŒ¨í•´ë„ ì´ë¯¸ì§€ëŠ” ë°˜í™˜
                
        except Exception as e:
            logger.error(f"âŒ Image generation failed: {e}")
            # ì‹¤íŒ¨ ì‹œ ë¹ˆ ì´ë¯¸ì§€ ë°˜í™˜
            return self._create_fallback_image()
    
    def generate_images_batch(self, 
                             prompts: List[str],
                             negative_prompts: Optional[List[str]] = None,
                             seeds: Optional[List[int]] = None) -> List[Image.Image]:
        """
        ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ë¡œë¶€í„° ë°°ì¹˜ ì´ë¯¸ì§€ ìƒì„±
        
        Args:
            prompts (List[str]): ì´ë¯¸ì§€ ìƒì„±ìš© í”„ë¡¬í”„íŠ¸ ë¦¬ìŠ¤íŠ¸
            negative_prompts (List[str], optional): ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸ ë¦¬ìŠ¤íŠ¸
            seeds (List[int], optional): ì‹œë“œ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            List[PIL.Image.Image]: ìƒì„±ëœ ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸
        """
        generated_images = []
        
        # ê¸°ë³¸ê°’ ì„¤ì •
        if negative_prompts is None:
            negative_prompts = [None for _ in range(len(prompts))]
        if seeds is None:
            seeds = [None for _ in range(len(prompts))]
        
        # ê° í”„ë¡¬í”„íŠ¸ì— ëŒ€í•´ ì´ë¯¸ì§€ ìƒì„±
        for i, prompt in enumerate(prompts):
            neg_prompt = negative_prompts[i] if i < len(negative_prompts) else None
            seed = seeds[i] if i < len(seeds) else None
            
            image = self.generate_image(prompt, neg_prompt, seed)
            generated_images.append(image)
            
            logger.debug(f"ğŸ“¸ Generated image {i+1}/{len(prompts)}")
        
        return generated_images
    
    def _validate_image(self, image: Image.Image) -> bool:
        """
        ìƒì„±ëœ ì´ë¯¸ì§€ì˜ í’ˆì§ˆì„ ê²€ì¦í•˜ëŠ” ë‚´ë¶€ ë©”ì„œë“œ
        
        Args:
            image (PIL.Image.Image): ê²€ì¦í•  ì´ë¯¸ì§€
            
        Returns:
            bool: ì´ë¯¸ì§€ê°€ ìœ íš¨í•œì§€ ì—¬ë¶€
        """
        try:
            # ê¸°ë³¸ ê²€ì¦: í¬ê¸° í™•ì¸
            if image.size[0] < 100 or image.size[1] < 100:
                return False
            
            # ì´ë¯¸ì§€ ë°ì´í„° ê²€ì¦
            img_array = np.array(image)
            
            # ì™„ì „íˆ ê²€ì€ìƒ‰ì´ê±°ë‚˜ í°ìƒ‰ì¸ ì´ë¯¸ì§€ ì œì™¸
            if np.all(img_array == 0) or np.all(img_array == 255):
                return False
            
            # ë¶„ì‚°ì´ ë„ˆë¬´ ë‚®ì€ ì´ë¯¸ì§€ ì œì™¸ (ë…¸ì´ì¦ˆë§Œ ìˆëŠ” ê²½ìš°)
            if np.var(img_array) < 100:
                return False
            
            return True
            
        except Exception as e:
            logger.warning(f"âš ï¸ Image validation failed: {e}")
            return False
    
    def _create_fallback_image(self) -> Image.Image:
        """
        ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•  ê¸°ë³¸ ì´ë¯¸ì§€ ìƒì„±
        
        Returns:
            PIL.Image.Image: ê¸°ë³¸ ì´ë¯¸ì§€
        """
        # ë‹¨ìƒ‰ ì´ë¯¸ì§€ ìƒì„± (ì—°êµ¬ìš©)
        fallback_image = Image.new(
            'RGB', 
            (self.width, self.height), 
            color=(128, 128, 128)  # íšŒìƒ‰
        )
        
        logger.info("ğŸ”„ Using fallback image")
        return fallback_image
    
    def save_image(self, 
                   image: Image.Image, 
                   save_path: str,
                   quality: int = 95) -> bool:
        """
        ì´ë¯¸ì§€ë¥¼ íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            image (PIL.Image.Image): ì €ì¥í•  ì´ë¯¸ì§€
            save_path (str): ì €ì¥ ê²½ë¡œ
            quality (int): JPEG í’ˆì§ˆ (1-100)
            
        Returns:
            bool: ì €ì¥ ì„±ê³µ ì—¬ë¶€
        """
        try:
            # ë””ë ‰í† ë¦¬ ìƒì„±
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            
            # ì´ë¯¸ì§€ ì €ì¥
            if save_path.lower().endswith('.jpg') or save_path.lower().endswith('.jpeg'):
                image.save(save_path, 'JPEG', quality=quality)
            else:
                image.save(save_path)
            
            logger.debug(f"ğŸ’¾ Image saved: {save_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Failed to save image: {e}")
            return False
    
    def get_pipeline_info(self) -> Dict:
        """
        íŒŒì´í”„ë¼ì¸ ì •ë³´ ë°˜í™˜ (ë””ë²„ê¹… ë° ë¡œê¹…ìš©)
        
        Returns:
            Dict: íŒŒì´í”„ë¼ì¸ ê´€ë ¨ ì •ë³´
        """
        return {
            "model_name": self.model_name,
            "device": self.device,
            "generation_config": self.generation_config,
            "memory_usage": torch.cuda.memory_allocated() if torch.cuda.is_available() else 0
        }
    
    def update_generation_config(self, **kwargs):
        """
        ì´ë¯¸ì§€ ìƒì„± ì„¤ì • ì—…ë°ì´íŠ¸
        
        Args:
            **kwargs: ì—…ë°ì´íŠ¸í•  ì„¤ì •ë“¤
        """
        self.generation_config.update(kwargs)
        logger.info(f"ğŸ”§ Generation config updated: {kwargs}")
    
    def clear_cache(self):
        """
        GPU ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬ (ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ì‚¬ìš©)
        """
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            logger.info("ğŸ§¹ CUDA cache cleared")
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            # MPS ìºì‹œ ì •ë¦¬ (PyTorch 2.0+)
            try:
                torch.mps.empty_cache()
                logger.info("ğŸ§¹ MPS cache cleared")
            except:
                pass


if __name__ == "__main__":
    # SD3 Generator í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ğŸ§ª SD3 Generator Test")
    print("=" * 30)
    
    try:
        # SD3 ìƒì„±ê¸° ì´ˆê¸°í™”
        generator = SD3Generator(
            model_name="stabilityai/stable-diffusion-3-medium",
            device="auto",
            height=1024,  # SD3 ê¶Œì¥ í¬ê¸°
            width=1024,
            num_inference_steps=28  # SD3 ê¶Œì¥ ìŠ¤í… ìˆ˜
        )
        
        print("âœ… SD3 Generator initialized successfully")
        print(f"ğŸ“Š Pipeline info: {generator.get_pipeline_info()}")
        
        # í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë“¤
        test_prompts = [
            "a cute cat sitting on a windowsill",
            "beautiful sunset over mountains",
            "professional portrait of a woman"
        ]
        
        print("\nğŸ¨ Testing image generation:")
        for i, prompt in enumerate(test_prompts):
            print(f"  Generating image {i+1}: '{prompt}'")
            image = generator.generate_image(prompt)
            
            # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ì €ì¥
            save_path = f"test_output_{i+1}.jpg"
            success = generator.save_image(image, save_path)
            if success:
                print(f"    âœ… Saved: {save_path}")
            else:
                print(f"    âŒ Save failed")
        
        print("\nâœ… All tests passed!")
        
    except Exception as e:
        print(f"âŒ Test failed: {e}")
    
    print("\nUsage:")
    print("from models.sd_generator import SD3Generator")
    print("generator = SD3Generator()")
    print("image = generator.generate_image('a beautiful landscape')") 