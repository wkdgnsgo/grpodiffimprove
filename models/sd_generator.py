"""
Stable Diffusion 3 Generator
============================

Stable Diffusion 3ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¡œë¶€í„° ê³ í’ˆì§ˆ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ëª¨ë“ˆì…ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
1. í…ìŠ¤íŠ¸-íˆ¬-ì´ë¯¸ì§€ ìƒì„± (Text-to-Image)
2. ì´ë¯¸ì§€ í’ˆì§ˆ ìµœì í™”
3. ë°°ì¹˜ ìƒì„± ì§€ì›
4. ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ìƒì„±

Author: AI Assistant
Date: 2025-01-22
"""

import torch
from diffusers import StableDiffusion3Pipeline, DiffusionPipeline
from PIL import Image
from typing import List, Dict, Optional, Union, Tuple
import logging
import os
import numpy as np
import json

logger = logging.getLogger(__name__)

class SD3Generator:
    """
    Stable Diffusion 3ë¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ìƒì„±ê¸° í´ë˜ìŠ¤
    
    ì´ í´ë˜ìŠ¤ëŠ” í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ ë°›ì•„ì„œ ê³ í’ˆì§ˆì˜ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    GRPO í•™ìŠµì—ì„œëŠ” ìƒì„±ëœ ì´ë¯¸ì§€ê°€ CLIP ë³´ìƒ ê³„ì‚°ì— ì‚¬ìš©ë©ë‹ˆë‹¤.
    
    Attributes:
        model_name (str): ì‚¬ìš©í•  SD3 ëª¨ë¸ ì´ë¦„ (configì—ì„œ ì½ì–´ì˜´)
        pipeline: Diffusion íŒŒì´í”„ë¼ì¸ ê°ì²´
        device: ì—°ì‚° ë””ë°”ì´ìŠ¤
        generation_config (dict): ì´ë¯¸ì§€ ìƒì„± ì„¤ì •
    """
    
    def __init__(self,
                 config_path: str = "config/default_config.json",
                 device: str = "auto",
                 height: int = 1024,
                 width: int = 1024,
                 num_inference_steps: int = 28,
                 guidance_scale: float = 7.0):
        """
        SD3 Generator ì´ˆê¸°í™”
        
        Args:
            config_path (str): ì„¤ì • íŒŒì¼ ê²½ë¡œ (ëª¨ë¸ ì´ë¦„ì„ ì—¬ê¸°ì„œ ì½ì–´ì˜´)
            device (str): ë””ë°”ì´ìŠ¤ ì„¤ì • ("auto", "mps", "cuda", "cpu")
            height (int): ìƒì„±í•  ì´ë¯¸ì§€ ë†’ì´ (SD3ëŠ” 1024x1024 ê¶Œì¥)
            width (int): ìƒì„±í•  ì´ë¯¸ì§€ ë„ˆë¹„
            num_inference_steps (int): ì¶”ë¡  ìŠ¤í… ìˆ˜ (SD3ëŠ” 28ìŠ¤í… ê¶Œì¥)
            guidance_scale (float): ê°€ì´ë˜ìŠ¤ ìŠ¤ì¼€ì¼ (SD3ëŠ” 7.0 ê¶Œì¥)
        """
        
        # ì„¤ì • íŒŒì¼ì—ì„œ ëª¨ë¸ ì´ë¦„ ì½ê¸°
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            self.model_name = config['model_settings']['sd_model']
            logger.info(f"ğŸ“„ Loaded SD model name from config: {self.model_name}")
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to load config: {e}, using default model")
            self.model_name = "stabilityai/stable-diffusion-3-medium"
        
        self.height = height
        self.width = width
        self.num_inference_steps = num_inference_steps
        self.guidance_scale = guidance_scale
        
        # ë””ë°”ì´ìŠ¤ ìë™ ì„ íƒ
        if device == "auto":
            if torch.backends.mps.is_available():
                self.device = "mps"
                logger.info("ğŸ Using Apple Silicon MPS for SD3")
            elif torch.cuda.is_available():
                self.device = "cuda"
                logger.info("ğŸš€ Using CUDA GPU for SD3")
            else:
                self.device = "cpu"
                logger.info("ğŸ’» Using CPU for SD3 (slow)")
        else:
            self.device = device
        
        # SD3 íŠ¹í™” ì´ë¯¸ì§€ ìƒì„± ì„¤ì •
        self.generation_config = {
            'height': self.height,
            'width': self.width,
            'num_inference_steps': self.num_inference_steps,
            'guidance_scale': self.guidance_scale,
            'num_images_per_prompt': 1,
            'generator': None,  # ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìœ„í•´ ì‹œë“œ ì„¤ì • ê°€ëŠ¥
            'max_sequence_length': 256,  # SD3 íŠ¹í™” ì„¤ì •
        }
        
        # íŒŒì´í”„ë¼ì¸ ë¡œë“œ
        self._load_pipeline()
    
    def _load_pipeline(self):
        """
        Stable Diffusion 3 íŒŒì´í”„ë¼ì¸ì„ ë¡œë“œí•˜ëŠ” ë‚´ë¶€ ë©”ì„œë“œ
        
        ì´ ë©”ì„œë“œëŠ”:
        1. SD3 íŒŒì´í”„ë¼ì¸ ë¡œë“œ
        2. ë””ë°”ì´ìŠ¤ ì„¤ì •
        3. ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •
        4. ì•ˆì „ ì²´í¬ ë¹„í™œì„±í™” (ì—°êµ¬ìš©)
        """
        try:
            logger.info(f"ğŸ“¥ Loading SD3 pipeline: {self.model_name}")
            
            # SD3 ëª¨ë¸ì¸ì§€ í™•ì¸í•˜ì—¬ ì ì ˆí•œ ë¡œë”© ë°©ì‹ ì„ íƒ
            if "stable-diffusion-3" in self.model_name.lower():
                # SD3 ì „ìš© ë¡œë”© ë°©ì‹ (diffusers í˜¸í™˜ ë²„ì „)
                try:
                    self.pipeline = StableDiffusion3Pipeline.from_pretrained(
                        self.model_name,
                        torch_dtype=torch.float16 if self.device in ['cuda', 'mps'] else torch.float32,
                        use_safetensors=True,
                        variant="fp16" if self.device in ['cuda', 'mps'] else None
                    )
                    logger.info("âœ… SD3 loaded with StableDiffusion3Pipeline")
                except Exception as e:
                    logger.warning(f"âš ï¸ SD3 specific loading failed: {e}, trying generic method")
                    # SD3 ì „ìš© ë¡œë”© ì‹¤íŒ¨ ì‹œ ì¼ë°˜ ë°©ì‹ìœ¼ë¡œ ì‹œë„
                    self.pipeline = DiffusionPipeline.from_pretrained(
                        self.model_name,
                        torch_dtype=torch.float16 if self.device in ['cuda', 'mps'] else torch.float32,
                        use_safetensors=True,
                        variant="fp16" if self.device in ['cuda', 'mps'] else None
                    )
                    logger.info("âœ… SD3 loaded with DiffusionPipeline")
            else:
                # ì¼ë°˜ SD ëª¨ë¸ ë¡œë”©
                self.pipeline = DiffusionPipeline.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.float16 if self.device in ['cuda', 'mps'] else torch.float32,
                    use_safetensors=True,
                    variant="fp16" if self.device in ['cuda', 'mps'] else None
                )
                logger.info("âœ… SD model loaded with DiffusionPipeline")
            
            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            self.pipeline = self.pipeline.to(self.device)
            
            # ë©”ëª¨ë¦¬ ìµœì í™” (í•„ìš”í•œ ê²½ìš°)
            if self.device == "mps":
                # Apple Silicon ìµœì í™”
                self.pipeline.enable_attention_slicing()
            elif self.device == "cuda":
                # CUDA ìµœì í™”
                if hasattr(self.pipeline, 'enable_memory_efficient_attention'):
                    self.pipeline.enable_memory_efficient_attention()
                self.pipeline.enable_attention_slicing()
            
            # ì•ˆì „ ì²´í¬ ë¹„í™œì„±í™” (ì—°êµ¬ ëª©ì )
            if hasattr(self.pipeline, 'safety_checker'):
                self.pipeline.safety_checker = None
            if hasattr(self.pipeline, 'requires_safety_checker'):
                self.pipeline.requires_safety_checker = False
            
            logger.info(f"âœ… SD3 pipeline loaded successfully on {self.device}")
            
        except Exception as e:
            logger.error(f"âŒ Failed to load SD3 pipeline: {e}")
            logger.info("ğŸ”„ Trying alternative loading method...")
            
            # ëŒ€ì•ˆ ë¡œë”© ë°©ì‹
            try:
                # ê°€ì¥ ê¸°ë³¸ì ì¸ ë°©ì‹ìœ¼ë¡œ ë¡œë”© ì‹œë„
                self.pipeline = DiffusionPipeline.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.float32,  # float32ë¡œ ê°•ì œ ì„¤ì • (í˜¸í™˜ì„±)
                    use_safetensors=False,  # safetensors ë¹„í™œì„±í™”
                )
                self.pipeline = self.pipeline.to(self.device)
                
                # ì•ˆì „ ì²´í¬ ë¹„í™œì„±í™”
                if hasattr(self.pipeline, 'safety_checker'):
                    self.pipeline.safety_checker = None
                if hasattr(self.pipeline, 'requires_safety_checker'):
                    self.pipeline.requires_safety_checker = False
                    
                logger.info(f"âœ… SD3 pipeline loaded with alternative method on {self.device}")
                
            except Exception as e2:
                logger.error(f"âŒ Alternative loading also failed: {e2}")
                logger.warning("ğŸ”„ Using fallback SD model...")
                
                # ìµœì¢… ëŒ€ì•ˆ: ë” ì•ˆì •ì ì¸ SD ëª¨ë¸ ì‚¬ìš©
                try:
                    fallback_model = "runwayml/stable-diffusion-v1-5"
                    logger.info(f"ğŸ“¥ Loading fallback model: {fallback_model}")
                    
                    self.pipeline = DiffusionPipeline.from_pretrained(
                        fallback_model,
                        torch_dtype=torch.float16 if self.device in ['cuda', 'mps'] else torch.float32,
                        use_safetensors=True,
                        variant="fp16" if self.device in ['cuda', 'mps'] else None
                    )
                    self.pipeline = self.pipeline.to(self.device)
                    
                    # ì•ˆì „ ì²´í¬ ë¹„í™œì„±í™”
                    if hasattr(self.pipeline, 'safety_checker'):
                        self.pipeline.safety_checker = None
                    if hasattr(self.pipeline, 'requires_safety_checker'):
                        self.pipeline.requires_safety_checker = False
                    
                    # ì„¤ì •ì„ SD1.5ì— ë§ê²Œ ì¡°ì •
                    self.height = 512
                    self.width = 512
                    self.num_inference_steps = 20
                    self.guidance_scale = 7.5
                    
                    self.generation_config.update({
                        'height': self.height,
                        'width': self.width,
                        'num_inference_steps': self.num_inference_steps,
                        'guidance_scale': self.guidance_scale,
                    })
                    
                    logger.info(f"âœ… Fallback SD model loaded successfully on {self.device}")
                    
                except Exception as e3:
                    logger.error(f"âŒ All loading methods failed: {e} | {e2} | {e3}")
                    raise RuntimeError(f"SD pipeline loading failed completely: {e3}")
    
    def generate_image(self, prompt: str, **kwargs) -> Optional[Image.Image]:
        """
        í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¡œë¶€í„° ì´ë¯¸ì§€ ìƒì„±
        
        Args:
            prompt (str): ì´ë¯¸ì§€ ìƒì„±ìš© í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
            **kwargs: ì¶”ê°€ ìƒì„± íŒŒë¼ë¯¸í„°
            
        Returns:
            Optional[Image.Image]: ìƒì„±ëœ ì´ë¯¸ì§€ (ì‹¤íŒ¨ ì‹œ None)
        """
        if not prompt or not prompt.strip():
            logger.warning("âš ï¸ Empty prompt provided for image generation")
            return self._create_fallback_image()
        
        try:
            # ì…ë ¥ ê²€ì¦ ë° ì •ì œ
            prompt = prompt.strip()
            if len(prompt) > 500:  # í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ì œí•œ
                prompt = prompt[:500]
                logger.warning("âš ï¸ Prompt truncated to 500 characters")
            
            # ì•ˆì „í•œ ìƒì„± íŒŒë¼ë¯¸í„° ì„¤ì •
            safe_params = {
                'height': kwargs.get('height', self.height),
                'width': kwargs.get('width', self.width),
                'num_inference_steps': min(50, max(10, kwargs.get('num_inference_steps', self.num_inference_steps))),
                'guidance_scale': max(1.0, min(20.0, kwargs.get('guidance_scale', self.guidance_scale))),
                'negative_prompt': kwargs.get('negative_prompt', "blurry, low quality, distorted"),
                'generator': None,  # ì•ˆì •ì„±ì„ ìœ„í•´ ì‹œë“œ ê³ ì • í•´ì œ
                'output_type': "pil",
                'return_dict': True
            }
            
            logger.debug(f"ğŸ¨ Generating image with prompt: '{prompt[:50]}...'")
            logger.debug(f"ğŸ“Š Generation params: {safe_params}")
            
            # CUDA ë©”ëª¨ë¦¬ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # ì´ë¯¸ì§€ ìƒì„± (ì•ˆì „í•œ ë°©ì‹ìœ¼ë¡œ)
            try:
                with torch.no_grad():
                    # ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ìƒì„±
                    if hasattr(self.pipeline, 'enable_model_cpu_offload'):
                        self.pipeline.enable_model_cpu_offload()
                    
                    result = self.pipeline(
                        prompt=prompt,
                        **safe_params
                    )
                    
                    # ê²°ê³¼ ê²€ì¦
                    if result is None or not hasattr(result, 'images') or len(result.images) == 0:
                        logger.warning("âš ï¸ Empty generation result, using fallback")
                        return self._create_fallback_image()
                    
                    image = result.images[0]
                    
                    # ì´ë¯¸ì§€ ê²€ì¦
                    if image is None or not hasattr(image, 'size'):
                        logger.warning("âš ï¸ Invalid image generated, using fallback")
                        return self._create_fallback_image()
                    
                    logger.debug(f"âœ… Image generated: {image.size}")
                    return image
                    
            except RuntimeError as e:
                if "CUDA" in str(e) or "device-side assert" in str(e) or "out of memory" in str(e):
                    logger.error(f"âŒ CUDA error during image generation: {e}")
                    # CUDA ìºì‹œ ì •ë¦¬
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        torch.cuda.synchronize()
                    return self._create_fallback_image()
                else:
                    raise e
                    
        except Exception as e:
            logger.error(f"âŒ Image generation failed: {e}")
            return self._create_fallback_image()
    
    def generate_images_batch(self, 
                             prompts: List[str],
                             negative_prompts: Optional[List[str]] = None,
                             seeds: Optional[List[int]] = None) -> List[Image.Image]:
        """
        ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ë¡œë¶€í„° ë°°ì¹˜ ì´ë¯¸ì§€ ìƒì„±
        
        Args:
            prompts (List[str]): ì´ë¯¸ì§€ ìƒì„±ìš© í”„ë¡¬í”„íŠ¸ ë¦¬ìŠ¤íŠ¸
            negative_prompts (List[str], optional): ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸ ë¦¬ìŠ¤íŠ¸
            seeds (List[int], optional): ì‹œë“œ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            List[PIL.Image.Image]: ìƒì„±ëœ ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸
        """
        generated_images = []
        
        # ê¸°ë³¸ê°’ ì„¤ì •
        if negative_prompts is None:
            negative_prompts = [None for _ in range(len(prompts))]
        if seeds is None:
            seeds = [None for _ in range(len(prompts))]
        
        # ê° í”„ë¡¬í”„íŠ¸ì— ëŒ€í•´ ì´ë¯¸ì§€ ìƒì„±
        for i, prompt in enumerate(prompts):
            neg_prompt = negative_prompts[i] if i < len(negative_prompts) else None
            seed = seeds[i] if i < len(seeds) else None
            
            image = self.generate_image(prompt, neg_prompt, seed)
            generated_images.append(image)
            
            logger.debug(f"ğŸ“¸ Generated image {i+1}/{len(prompts)}")
        
        return generated_images
    
    def _validate_image(self, image: Image.Image) -> bool:
        """
        ìƒì„±ëœ ì´ë¯¸ì§€ì˜ í’ˆì§ˆì„ ê²€ì¦í•˜ëŠ” ë‚´ë¶€ ë©”ì„œë“œ
        
        Args:
            image (PIL.Image.Image): ê²€ì¦í•  ì´ë¯¸ì§€
            
        Returns:
            bool: ì´ë¯¸ì§€ê°€ ìœ íš¨í•œì§€ ì—¬ë¶€
        """
        try:
            # ê¸°ë³¸ ê²€ì¦: í¬ê¸° í™•ì¸
            if image.size[0] < 100 or image.size[1] < 100:
                return False
            
            # ì´ë¯¸ì§€ ë°ì´í„° ê²€ì¦
            img_array = np.array(image)
            
            # ì™„ì „íˆ ê²€ì€ìƒ‰ì´ê±°ë‚˜ í°ìƒ‰ì¸ ì´ë¯¸ì§€ ì œì™¸
            if np.all(img_array == 0) or np.all(img_array == 255):
                return False
            
            # ë¶„ì‚°ì´ ë„ˆë¬´ ë‚®ì€ ì´ë¯¸ì§€ ì œì™¸ (ë…¸ì´ì¦ˆë§Œ ìˆëŠ” ê²½ìš°)
            if np.var(img_array) < 100:
                return False
            
            return True
            
        except Exception as e:
            logger.warning(f"âš ï¸ Image validation failed: {e}")
            return False
    
    def _create_fallback_image(self) -> Image.Image:
        """
        ìƒì„± ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•  ëŒ€ì²´ ì´ë¯¸ì§€ ìƒì„±
        
        Returns:
            Image.Image: ëŒ€ì²´ ì´ë¯¸ì§€
        """
        try:
            # ë‹¨ìƒ‰ ì´ë¯¸ì§€ ìƒì„± (RGB)
            fallback_image = Image.new('RGB', (self.width, self.height), color=(128, 128, 128))
            logger.info("ğŸ”„ Using fallback image")
            return fallback_image
        except Exception as e:
            logger.error(f"âŒ Failed to create fallback image: {e}")
            # ìµœì†Œí•œì˜ ì´ë¯¸ì§€ë¼ë„ ìƒì„±
            return Image.new('RGB', (512, 512), color=(64, 64, 64))
    
    def save_image(self, 
                   image: Image.Image, 
                   save_path: str,
                   quality: int = 95) -> bool:
        """
        ì´ë¯¸ì§€ë¥¼ íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            image (PIL.Image.Image): ì €ì¥í•  ì´ë¯¸ì§€
            save_path (str): ì €ì¥ ê²½ë¡œ
            quality (int): JPEG í’ˆì§ˆ (1-100)
            
        Returns:
            bool: ì €ì¥ ì„±ê³µ ì—¬ë¶€
        """
        try:
            # ë””ë ‰í† ë¦¬ ìƒì„±
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            
            # ì´ë¯¸ì§€ ì €ì¥
            if save_path.lower().endswith('.jpg') or save_path.lower().endswith('.jpeg'):
                image.save(save_path, 'JPEG', quality=quality)
            else:
                image.save(save_path)
            
            logger.debug(f"ğŸ’¾ Image saved: {save_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Failed to save image: {e}")
            return False
    
    def get_pipeline_info(self) -> Dict:
        """
        íŒŒì´í”„ë¼ì¸ ì •ë³´ ë°˜í™˜ (ë””ë²„ê¹… ë° ë¡œê¹…ìš©)
        
        Returns:
            Dict: íŒŒì´í”„ë¼ì¸ ê´€ë ¨ ì •ë³´
        """
        return {
            "model_name": self.model_name,
            "device": self.device,
            "generation_config": self.generation_config,
            "memory_usage": torch.cuda.memory_allocated() if torch.cuda.is_available() else 0
        }
    
    def update_generation_config(self, **kwargs):
        """
        ì´ë¯¸ì§€ ìƒì„± ì„¤ì • ì—…ë°ì´íŠ¸
        
        Args:
            **kwargs: ì—…ë°ì´íŠ¸í•  ì„¤ì •ë“¤
        """
        self.generation_config.update(kwargs)
        logger.info(f"ğŸ”§ Generation config updated: {kwargs}")
    
    def clear_cache(self):
        """
        GPU ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬ (ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ì‚¬ìš©)
        """
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            logger.info("ğŸ§¹ CUDA cache cleared")
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            # MPS ìºì‹œ ì •ë¦¬ (PyTorch 2.0+)
            try:
                torch.mps.empty_cache()
                logger.info("ğŸ§¹ MPS cache cleared")
            except:
                pass


if __name__ == "__main__":
    # SD3 Generator í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ğŸ§ª SD3 Generator Test")
    print("=" * 30)
    
    try:
        # SD3 ìƒì„±ê¸° ì´ˆê¸°í™”
        generator = SD3Generator(
            config_path="config/default_config.json",
            device="auto",
            height=1024,  # SD3 ê¶Œì¥ í¬ê¸°
            width=1024,
            num_inference_steps=28  # SD3 ê¶Œì¥ ìŠ¤í… ìˆ˜
        )
        
        print("âœ… SD3 Generator initialized successfully")
        print(f"ğŸ“Š Pipeline info: {generator.get_pipeline_info()}")
        
        # í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë“¤
        test_prompts = [
            "a cute cat sitting on a windowsill",
            "beautiful sunset over mountains",
            "professional portrait of a woman"
        ]
        
        print("\nğŸ¨ Testing image generation:")
        for i, prompt in enumerate(test_prompts):
            print(f"  Generating image {i+1}: '{prompt}'")
            image = generator.generate_image(prompt)
            
            # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ì €ì¥
            save_path = f"test_output_{i+1}.jpg"
            success = generator.save_image(image, save_path)
            if success:
                print(f"    âœ… Saved: {save_path}")
            else:
                print(f"    âŒ Save failed")
        
        print("\nâœ… All tests passed!")
        
    except Exception as e:
        print(f"âŒ Test failed: {e}")
    
    print("\nUsage:")
    print("from models.sd_generator import SD3Generator")
    print("generator = SD3Generator()")
    print("image = generator.generate_image('a beautiful landscape')") 