"""
VLM (Vision Language Model) Wrapper
===================================

Qwen2.5-VLì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„¸í•˜ê³  í’ˆì§ˆ ë†’ì€ í”„ë¡¬í”„íŠ¸ë¡œ ê°œì„ í•˜ëŠ” ëª¨ë“ˆì…ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
1. í”„ë¡¬í”„íŠ¸ ê°œì„  (Prompt Enhancement)
2. í…ìŠ¤íŠ¸ ìƒì„± íŒŒë¼ë¯¸í„° ê´€ë¦¬
3. ë””ë°”ì´ìŠ¤ ìµœì í™” (MPS/CUDA/CPU)
4. ë°°ì¹˜ ì²˜ë¦¬ ì§€ì›

Author: AI Assistant
Date: 2025-01-22
"""

import torch
import torch.nn as nn
from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor
from typing import List, Dict, Optional, Union
import logging
import json

logger = logging.getLogger(__name__)

class VLMWrapper(nn.Module):
    """
    Qwen2.5-VLì„ ë˜í•‘í•˜ì—¬ í”„ë¡¬í”„íŠ¸ ê°œì„  ê¸°ëŠ¥ì„ ì œê³µí•˜ëŠ” í´ë˜ìŠ¤
    
    ì´ í´ë˜ìŠ¤ëŠ” ì‚¬ìš©ìì˜ ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ (ì˜ˆ: "a cat")ë¥¼ ë°›ì•„ì„œ
    ë” ìƒì„¸í•˜ê³  êµ¬ì²´ì ì¸ í”„ë¡¬í”„íŠ¸ (ì˜ˆ: "a fluffy orange tabby cat sitting gracefully...")ë¡œ 
    ë³€í™˜í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
    
    Attributes:
        model_name (str): ì‚¬ìš©í•  Qwen2.5-VL ëª¨ë¸ ì´ë¦„ (configì—ì„œ ì½ì–´ì˜´)
        tokenizer: í† í¬ë‚˜ì´ì € ê°ì²´
        processor: í”„ë¡œì„¸ì„œ ê°ì²´
        model: Qwen2.5-VL ëª¨ë¸ ê°ì²´
        device: ì—°ì‚° ë””ë°”ì´ìŠ¤ (MPS/CUDA/CPU)
        generation_config (dict): í…ìŠ¤íŠ¸ ìƒì„± ì„¤ì •
    """
    
    def __init__(self,
                 config_path: str = "config/default_config.json",
                 device: str = "auto",
                 max_new_tokens: int = 100,
                 temperature: float = 0.7,
                 top_p: float = 0.9,
                 do_sample: bool = True):
        """
        VLM Wrapper ì´ˆê¸°í™”
        
        Args:
            config_path (str): ì„¤ì • íŒŒì¼ ê²½ë¡œ (ëª¨ë¸ ì´ë¦„ì„ ì—¬ê¸°ì„œ ì½ì–´ì˜´)
            device (str): ë””ë°”ì´ìŠ¤ ì„¤ì • ("auto", "mps", "cuda", "cpu")
            max_new_tokens (int): ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜
            temperature (float): ìƒì„± ì˜¨ë„ (ë‹¤ì–‘ì„± vs ì¼ê´€ì„±)
            top_p (float): ëˆ„ì  í™•ë¥  ì„ê³„ê°’
            do_sample (bool): ìƒ˜í”Œë§ ì—¬ë¶€
        """
        super().__init__()
        
        # ì„¤ì • íŒŒì¼ì—ì„œ ëª¨ë¸ ì´ë¦„ ì½ê¸°
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            self.model_name = config['model_settings']['vlm_model']
            logger.info(f"ğŸ“„ Loaded model name from config: {self.model_name}")
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to load config: {e}, using default model")
            self.model_name = "Qwen/Qwen2.5-VL-7B-Instruct"
        
        # ë””ë°”ì´ìŠ¤ ìë™ ì„ íƒ
        if device == "auto":
            if torch.backends.mps.is_available():
                self.device = torch.device("mps")
                logger.info("ğŸ Using Apple Silicon MPS for Qwen2.5-VL")
            elif torch.cuda.is_available():
                self.device = torch.device("cuda")
                logger.info("ğŸš€ Using CUDA GPU for Qwen2.5-VL")
            else:
                self.device = torch.device("cpu")
                logger.info("ğŸ’» Using CPU for Qwen2.5-VL")
        else:
            self.device = torch.device(device)
        
        # í…ìŠ¤íŠ¸ ìƒì„± ì„¤ì •
        self.generation_config = {
            'max_new_tokens': max_new_tokens,
            'temperature': temperature,
            'top_p': top_p,
            'do_sample': do_sample,
            'pad_token_id': None,  # ëª¨ë¸ ë¡œë“œ í›„ ì„¤ì •
            'eos_token_id': None,  # ëª¨ë¸ ë¡œë“œ í›„ ì„¤ì •
        }
        
        # ëª¨ë¸ ë¡œë“œ
        self._load_model()
    
    def _load_model(self):
        """
        Qwen2.5-VL ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•˜ëŠ” ë‚´ë¶€ ë©”ì„œë“œ
        
        ì´ ë©”ì„œë“œëŠ”:
        1. í† í¬ë‚˜ì´ì € ë¡œë“œ
        2. í”„ë¡œì„¸ì„œ ë¡œë“œ
        3. Qwen2.5-VL ëª¨ë¸ ë¡œë“œ
        4. ë””ë°”ì´ìŠ¤ ì„¤ì •
        5. í† í° ID ì„¤ì •
        """
        try:
            logger.info(f"ğŸ“¥ Loading Qwen2.5-VL model: {self.model_name}")
            
            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True,
                padding_side="left"
            )
            
            # íŒ¨ë”© í† í° ì„¤ì • (ì¤‘ìš”!)
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
                logger.info("ğŸ”§ Set pad_token to eos_token")
            
            # í”„ë¡œì„¸ì„œ ë¡œë“œ
            self.processor = AutoProcessor.from_pretrained(
                self.model_name,
                trust_remote_code=True
            )
            
            # ëª¨ë¸ ë¡œë“œ
            self.model = Qwen2VLForConditionalGeneration.from_pretrained(
                self.model_name,
                trust_remote_code=True,
                torch_dtype=torch.float16 if self.device.type in ['cuda', 'mps'] else torch.float32,
                device_map="auto" if self.device.type == 'cuda' else None
            )
            
            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™ (device_mapì´ ì—†ëŠ” ê²½ìš°)
            if self.device.type != 'cuda':
                self.model = self.model.to(self.device)
            
            # í‰ê°€ ëª¨ë“œ ì„¤ì •
            self.model.eval()
            
            # í† í° ID ì„¤ì •
            self.generation_config['pad_token_id'] = self.tokenizer.pad_token_id
            self.generation_config['eos_token_id'] = self.tokenizer.eos_token_id
            
            logger.info(f"âœ… Qwen2.5-VL model loaded successfully on {self.device}")
            
        except Exception as e:
            logger.error(f"âŒ Failed to load Qwen2.5-VL model: {e}")
            logger.info("ğŸ”„ Trying fallback loading method...")
            
            # ëŒ€ì•ˆ ë¡œë”© ë°©ì‹ (ì¼ë°˜ AutoTokenizer ì‚¬ìš©)
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(
                    "Qwen/Qwen2.5-7B-Instruct",
                    trust_remote_code=True
                )
                
                # íŒ¨ë”© í† í° ì„¤ì • (ì¤‘ìš”!)
                if self.tokenizer.pad_token is None:
                    self.tokenizer.pad_token = self.tokenizer.eos_token
                    self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
                    logger.info("ğŸ”§ Set pad_token to eos_token for fallback model")
                
                # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸ë¡œ ëŒ€ì²´
                from transformers import AutoModelForCausalLM
                self.model = AutoModelForCausalLM.from_pretrained(
                    "Qwen/Qwen2.5-7B-Instruct",
                    trust_remote_code=True,
                    torch_dtype=torch.float16 if self.device.type in ['cuda', 'mps'] else torch.float32
                )
                
                self.model = self.model.to(self.device)
                self.model.eval()
                
                self.generation_config['pad_token_id'] = self.tokenizer.pad_token_id
                self.generation_config['eos_token_id'] = self.tokenizer.eos_token_id
                
                # í”„ë¡œì„¸ì„œë¥¼ Noneìœ¼ë¡œ ì„¤ì • (fallbackì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
                self.processor = None
                
                logger.info(f"âœ… Fallback model loaded successfully on {self.device}")
                
            except Exception as e2:
                logger.error(f"âŒ Fallback loading also failed: {e2}")
                raise RuntimeError(f"Qwen2.5-VL model loading failed: {e} | Fallback: {e2}")
    
    def enhance_prompt(self, user_prompt: str) -> str:
        """
        ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ë¥¼ ê°œì„ í•˜ì—¬ ë” ìƒì„¸í•œ í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜
        
        ì´ ë©”ì„œë“œëŠ” ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ë°›ì•„ì„œ:
        1. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì ìš©
        2. Qwen2.5-VLë¡œ í…ìŠ¤íŠ¸ ìƒì„±
        3. í›„ì²˜ë¦¬ ë° ì •ì œ
        4. ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ë°˜í™˜
        
        Args:
            user_prompt (str): ì‚¬ìš©ìê°€ ì…ë ¥í•œ ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸
            
        Returns:
            str: ê°œì„ ëœ ìƒì„¸ í”„ë¡¬í”„íŠ¸
            
        Example:
            Input: "a cat"
            Output: "a fluffy orange tabby cat sitting gracefully on a windowsill, 
                    soft natural lighting, professional pet photography, detailed fur texture"
        """
        try:
            # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì ìš©
            enhanced_prompt_template = self._create_enhancement_template(user_prompt)
        
            if self.processor is not None:
                # Qwen2.5-VL ìŠ¤íƒ€ì¼ ë©”ì‹œì§€ í˜•ì‹
                messages = [
                    {
                        "role": "system", 
                        "content": "You are an expert in creating detailed, artistic image generation prompts. Transform simple prompts into rich, descriptive ones."
                    },
                    {
                        "role": "user", 
                        "content": enhanced_prompt_template
                    }
                ]
                
                # ì±„íŒ… í…œí”Œë¦¿ ì ìš©
                text = self.processor.apply_chat_template(
                    messages, 
                    tokenize=False, 
                    add_generation_prompt=True
                )
                
                # í† í¬ë‚˜ì´ì§•
                inputs = self.processor(
                    text=[text],
                    images=None,  # í…ìŠ¤íŠ¸ë§Œ ì²˜ë¦¬
                    padding=True,
                    return_tensors="pt"
                ).to(self.device)
            else:
                # ëŒ€ì•ˆ ë°©ì‹ (fallback ëª¨ë¸ìš©)
                text = f"Human: {enhanced_prompt_template}\n\nAssistant:"
                inputs = self.tokenizer(
                    text, 
                    return_tensors="pt", 
                    padding=True, 
                    truncation=True,
                    max_length=512
                ).to(self.device)
        
            # í…ìŠ¤íŠ¸ ìƒì„±
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    **self.generation_config
                )
        
            # ë””ì½”ë”© ë° í›„ì²˜ë¦¬
            output_ids = outputs[0][inputs.input_ids.shape[1]:]
            if self.processor is not None:
                generated_text = self.processor.decode(
                    output_ids, 
                    skip_special_tokens=True
                )
            else:
                generated_text = self.tokenizer.decode(
                    output_ids, 
                    skip_special_tokens=True
                )
        
            # ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ì¶”ì¶œ ë° ì •ì œ
            enhanced_prompt = self._extract_enhanced_prompt(
                generated_text, 
                enhanced_prompt_template
            )
            
            logger.debug(f"ğŸ“ Original: {user_prompt}")
            logger.debug(f"âœ¨ Enhanced: {enhanced_prompt}")
            
            return enhanced_prompt
            
        except Exception as e:
            logger.warning(f"âš ï¸ Prompt enhancement failed: {e}")
            # ì‹¤íŒ¨ ì‹œ ì›ë³¸ í”„ë¡¬í”„íŠ¸ì— ê¸°ë³¸ ê°œì„  ì ìš©
            return self._fallback_enhancement(user_prompt)
    
    def enhance_prompts_batch(self, user_prompts: List[str]) -> List[str]:
        """
        ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•˜ì—¬ íš¨ìœ¨ì„± í–¥ìƒ
        
        Args:
            user_prompts (List[str]): ê°œì„ í•  í”„ë¡¬í”„íŠ¸ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            List[str]: ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        enhanced_prompts = []
        
        for prompt in user_prompts:
            enhanced = self.enhance_prompt(prompt)
            enhanced_prompts.append(enhanced)
        
        return enhanced_prompts
    
    def _create_enhancement_template(self, user_prompt: str) -> str:
        """
        í”„ë¡¬í”„íŠ¸ ê°œì„ ì„ ìœ„í•œ í…œí”Œë¦¿ ìƒì„±
        
        ì´ ë©”ì„œë“œëŠ” Qwen2.5-VLì´ ë” ë‚˜ì€ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•˜ë„ë¡ ìœ ë„í•˜ëŠ”
        í…œí”Œë¦¿ì„ ë§Œë“­ë‹ˆë‹¤.
        
        Args:
            user_prompt (str): ì›ë³¸ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
            
        Returns:
            str: ê°œì„ ì„ ìœ„í•œ í…œí”Œë¦¿ í”„ë¡¬í”„íŠ¸
        """
        templates = [
            f"Enhance this image prompt to be more detailed and artistic: '{user_prompt}' -> Enhanced:",
            f"Make this prompt more descriptive and visually rich: '{user_prompt}' -> Improved:",
            f"Transform this simple prompt into a detailed description: '{user_prompt}' -> Detailed:",
        ]
            
        # ëœë¤í•˜ê²Œ í…œí”Œë¦¿ ì„ íƒ (ë‹¤ì–‘ì„± ì¦ê°€)
        import random
        return random.choice(templates)
    
    def _extract_enhanced_prompt(self, generated_text: str, template: str) -> str:
        """
        ìƒì„±ëœ í…ìŠ¤íŠ¸ì—ì„œ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ë§Œ ì¶”ì¶œ
        
        Args:
            generated_text (str): Qwen2.5-VLì´ ìƒì„±í•œ ì „ì²´ í…ìŠ¤íŠ¸
            template (str): ì‚¬ìš©ëœ í…œí”Œë¦¿
            
        Returns:
            str: ì¶”ì¶œëœ ê°œì„  í”„ë¡¬í”„íŠ¸
        """
        try:
            # í…œí”Œë¦¿ ì´í›„ ë¶€ë¶„ ì¶”ì¶œ
            if "->" in template:
                split_marker = template.split("->")[-1].strip()
                if split_marker in generated_text:
                    enhanced_part = generated_text.split(split_marker, 1)[-1]
                else:
                    enhanced_part = generated_text.replace(template, "")
            else:
                enhanced_part = generated_text.replace(template, "")
            
            # ì •ì œ ë° ì •ë¦¬
            enhanced_part = enhanced_part.strip()
            
            # ë¶ˆí•„ìš”í•œ ë¶€ë¶„ ì œê±°
            enhanced_part = enhanced_part.split('\n')[0]  # ì²« ë²ˆì§¸ ì¤„ë§Œ
            enhanced_part = enhanced_part.split('.')[0]   # ì²« ë²ˆì§¸ ë¬¸ì¥ë§Œ
            
            # ìµœì†Œ ê¸¸ì´ í™•ì¸
            if len(enhanced_part) < 10:
                raise ValueError("Enhanced prompt too short")
            
            return enhanced_part.strip()
            
        except Exception as e:
            logger.warning(f"âš ï¸ Enhanced prompt extraction failed: {e}")
            return generated_text.strip()[:200]  # ì²˜ìŒ 200ìë§Œ ì‚¬ìš©
    
    def _fallback_enhancement(self, user_prompt: str) -> str:
        """
        Qwen2.5-VL ê°œì„  ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•  ê¸°ë³¸ ê°œì„  ë°©ë²•
        
        ì´ ë©”ì„œë“œëŠ” Qwen2.5-VLì´ ì‹¤íŒ¨í–ˆì„ ë•Œ ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ
        ê¸°ë³¸ì ì¸ í”„ë¡¬í”„íŠ¸ ê°œì„ ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            user_prompt (str): ì›ë³¸ í”„ë¡¬í”„íŠ¸
            
        Returns:
            str: ê¸°ë³¸ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸
        """
        # ê¸°ë³¸ í’ˆì§ˆ í–¥ìƒ í‚¤ì›Œë“œ ì¶”ê°€
        quality_keywords = [
            "high quality", "detailed", "professional", 
            "well-lit", "sharp focus", "artistic"
        ]
        
        # ëœë¤í•˜ê²Œ 2-3ê°œ í‚¤ì›Œë“œ ì„ íƒ
        import random
        selected_keywords = random.sample(quality_keywords, k=min(3, len(quality_keywords)))
        
        enhanced = f"{user_prompt}, {', '.join(selected_keywords)}"
        
        logger.info(f"ğŸ”„ Using fallback enhancement: {enhanced}")
        return enhanced
    
    def get_model_info(self) -> Dict:
        """
        ëª¨ë¸ ì •ë³´ ë°˜í™˜ (ë””ë²„ê¹… ë° ë¡œê¹…ìš©)
        
        Returns:
            Dict: ëª¨ë¸ ê´€ë ¨ ì •ë³´
        """
        return {
            "model_name": self.model_name,
            "device": str(self.device),
            "parameters": sum(p.numel() for p in self.model.parameters()),
            "generation_config": self.generation_config,
            "vocab_size": self.tokenizer.vocab_size if hasattr(self.tokenizer, 'vocab_size') else None
        }
    
    def update_generation_config(self, **kwargs):
        """
        í…ìŠ¤íŠ¸ ìƒì„± ì„¤ì • ì—…ë°ì´íŠ¸
        
        Args:
            **kwargs: ì—…ë°ì´íŠ¸í•  ì„¤ì •ë“¤
        """
        self.generation_config.update(kwargs)
        logger.info(f"ğŸ”§ Generation config updated: {kwargs}")


# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    # VLM Wrapper í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ğŸ§ª VLM Wrapper Test")
    print("=" * 30)
    
    try:
        # VLM ë˜í¼ ì´ˆê¸°í™”
        vlm = VLMWrapper(
            config_path="config/default_config.json",
            device="auto",
            max_new_tokens=100,
            temperature=0.7
        )
        
        print("âœ… VLM Wrapper initialized successfully")
        print(f"ğŸ“Š Model info: {vlm.get_model_info()}")
        
        # í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë“¤
        test_prompts = [
            "a cat",
            "sunset",
            "beautiful woman",
            "mountain landscape"
        ]
        
        print("\nğŸ”„ Testing prompt enhancement:")
        for prompt in test_prompts:
            enhanced = vlm.enhance_prompt(prompt)
            print(f"  '{prompt}' â†’ '{enhanced}'")
        
        print("\nâœ… All tests passed!")
        
    except Exception as e:
        print(f"âŒ Test failed: {e}")
    
    print("\nUsage:")
    print("from models.vlm_wrapper import VLMWrapper")
    print("vlm = VLMWrapper()")
    print("enhanced = vlm.enhance_prompt('a cat')") 