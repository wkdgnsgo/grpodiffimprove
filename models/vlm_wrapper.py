"""
VLM (Vision Language Model) Wrapper
===================================

Qwen2.5-VLì„ ì‹¤ì œë¡œ ë¡œë“œí•˜ì—¬ GRPO í•™ìŠµì„ ìœ„í•œ ì •ì±… ë„¤íŠ¸ì›Œí¬ë¡œ ì‚¬ìš©í•˜ëŠ” ëª¨ë“ˆì…ë‹ˆë‹¤.
LoRAë¥¼ ì‚¬ìš©í•˜ì—¬ íš¨ìœ¨ì ì¸ fine-tuningì„ ì§€ì›í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
1. ì‹¤ì œ Qwen2.5-VL ëª¨ë¸ ë¡œë“œ ë° ê´€ë¦¬
2. LoRA ì–´ëŒ‘í„°ë¥¼ í†µí•œ íš¨ìœ¨ì  í•™ìŠµ
3. GRPO í•™ìŠµì„ ìœ„í•œ ì •ì±… ë„¤íŠ¸ì›Œí¬ ì¸í„°í˜ì´ìŠ¤
4. í† í°ë³„ ìˆœì°¨ ìƒì„± ë° í™•ë¥  ê³„ì‚°
5. í…ìŠ¤íŠ¸ ìƒì„± íŒŒë¼ë¯¸í„° ê´€ë¦¬
6. í† í° ê¸¸ì´ ì œí•œ (CLIP 77 í† í° ì œí•œ)

Author: AI Assistant
Date: 2025-01-22
"""

import torch
import torch.nn as nn
from transformers import (
    Qwen2VLForConditionalGeneration, 
    AutoProcessor, 
    AutoTokenizer,
    CLIPTokenizer
)
from torch.distributions import Categorical
from typing import List, Dict, Optional, Union
import logging
import json

# LoRA ê´€ë ¨ imports
try:
    from peft import LoraConfig, get_peft_model, TaskType
    PEFT_AVAILABLE = True
except ImportError:
    PEFT_AVAILABLE = False
    logging.warning("âš ï¸ PEFT library not available. LoRA training will be disabled.")

logger = logging.getLogger(__name__)

class VLMWrapper(nn.Module):
    """
    Qwen2.5-VLì„ GRPO í•™ìŠµì„ ìœ„í•œ ì •ì±… ë„¤íŠ¸ì›Œí¬ë¡œ ë˜í•‘í•˜ëŠ” í´ë˜ìŠ¤
    
    ì´ í´ë˜ìŠ¤ëŠ” ì‹¤ì œ Qwen2.5-VL ëª¨ë¸ì„ ë¡œë“œí•˜ê³  GRPO í•™ìŠµì— í•„ìš”í•œ
    ì •ì±… ë„¤íŠ¸ì›Œí¬ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤. LoRAë¥¼ ì‚¬ìš©í•˜ì—¬ íš¨ìœ¨ì ì¸ í•™ìŠµì„ ì§€ì›í•©ë‹ˆë‹¤.
    
    ì£¼ìš” ë©”ì„œë“œ:
    - forward(): í† í° ì‹œí€€ìŠ¤ì— ëŒ€í•œ ë‹¤ìŒ í† í° ë¶„í¬ ê³„ì‚°
    - generate_sequence(): í† í°ë³„ ìˆœì°¨ ìƒì„±
    - get_log_prob(): íŠ¹ì • í† í°ì˜ ë¡œê·¸ í™•ë¥  ê³„ì‚°
    
    Attributes:
        model_name (str): ì‚¬ìš©í•  Qwen2.5-VL ëª¨ë¸ ì´ë¦„
        model: Qwen2.5-VL ëª¨ë¸ ê°ì²´ (LoRA ì–´ëŒ‘í„° í¬í•¨ ê°€ëŠ¥)
        tokenizer: í† í¬ë‚˜ì´ì € ê°ì²´
        processor: í”„ë¡œì„¸ì„œ ê°ì²´
        clip_tokenizer: CLIP í† í¬ë‚˜ì´ì € (í† í° ê¸¸ì´ ì²´í¬ìš©)
        device: ì—°ì‚° ë””ë°”ì´ìŠ¤ (MPS/CUDA/CPU)
        max_token_length (int): ìµœëŒ€ í† í° ê¸¸ì´ (ê¸°ë³¸ê°’: 77)
        use_lora (bool): LoRA ì‚¬ìš© ì—¬ë¶€
    """
    
    def __init__(self,
                 config_path: str = "config/default_config.json",
                 device: str = "auto",
                 max_token_length: int = 77):
        """
        VLM Wrapper ì´ˆê¸°í™” (ì‹¤ì œ Qwen2.5-VL ëª¨ë¸ ë¡œë“œ with LoRA)
        
        Args:
            config_path (str): ì„¤ì • íŒŒì¼ ê²½ë¡œ
            device (str): ë””ë°”ì´ìŠ¤ ì„¤ì •
            max_token_length (int): ìµœëŒ€ í† í° ê¸¸ì´ (CLIP ì œí•œ)
        """
        super().__init__()
        
        # ì„¤ì • íŒŒì¼ì—ì„œ ëª¨ë¸ ì´ë¦„ ë° LoRA ì„¤ì • ì½ê¸°
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            self.model_name = config['model_settings']['vlm_model']
            self.vlm_training_config = config['model_settings'].get('vlm_training', {})
            logger.info(f"ğŸ“„ VLM model name from config: {self.model_name}")
            logger.info(f"ğŸ”§ VLM training config: {self.vlm_training_config}")
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to load config: {e}, using default")
            self.model_name = "Qwen/Qwen2.5-VL-7B-Instruct"
            self.vlm_training_config = {}
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        if device == "auto":
            if torch.cuda.is_available():
                self.device = torch.device("cuda")
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                self.device = torch.device("mps")
            else:
                self.device = torch.device("cpu")
        else:
            self.device = torch.device(device)
        
        logger.info(f"ğŸ–¥ï¸ Using device: {self.device}")
        
        # í† í° ê¸¸ì´ ì œí•œ ì„¤ì •
        self.max_token_length = max_token_length
        
        # LoRA ì„¤ì •
        self.use_lora = self.vlm_training_config.get('use_lora', False) and PEFT_AVAILABLE
        if self.use_lora and not PEFT_AVAILABLE:
            logger.warning("âš ï¸ LoRA requested but PEFT not available. Falling back to full fine-tuning.")
            self.use_lora = False
        
        # ëª¨ë¸ ë¡œë“œ
        self._load_model()
        
        # CLIP í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” (í† í° ê¸¸ì´ ì²´í¬ìš©)
        try:
            self.clip_tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")
            logger.info("âœ… CLIP tokenizer loaded for token length validation")
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to load CLIP tokenizer: {e}, using fallback")
            self.clip_tokenizer = None
        
        logger.info(f"âœ… VLM Wrapper initialized with {self.model_name} (max_tokens: {max_token_length})")
    
    def _load_config(self):
        """Config íŒŒì¼ ë¡œë“œ"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to load config: {e}")
            return {}
        if self.use_lora:
            logger.info(f"ğŸ¯ LoRA enabled for efficient training")
    
    def _load_model(self):
        """
        ì‹¤ì œ Qwen2.5-VL ëª¨ë¸ ë¡œë“œ with LoRA support
        """
        try:
            logger.info(f"ğŸ“¥ Loading Qwen2.5-VL model: {self.model_name}")
            
            # ì–‘ìí™” ì„¤ì •
            load_in_8bit = self.vlm_training_config.get('load_in_8bit', False)
            load_in_4bit = self.vlm_training_config.get('load_in_4bit', False)
            
            model_kwargs = {
                "torch_dtype": torch.float16 if self.device.type == "cuda" else torch.float32,
                "device_map": "auto" if self.device.type == "cuda" else None,
                "trust_remote_code": True,  # Qwen ëª¨ë¸ì— í•„ìš”
                "attn_implementation": "eager",  # ì•ˆì •ì„±ì„ ìœ„í•´ eager attention ì‚¬ìš©
            }
            
            if load_in_8bit:
                model_kwargs["load_in_8bit"] = True
                logger.info("ğŸ”§ Loading model in 8-bit mode")
            elif load_in_4bit:
                model_kwargs["load_in_4bit"] = True
                logger.info("ğŸ”§ Loading model in 4-bit mode")
            
            # Visual encoder ì´ˆê¸°í™” ê²½ê³  ì–µì œë¥¼ ìœ„í•œ ì„¤ì •
            import warnings
            with warnings.catch_warnings():
                warnings.filterwarnings("ignore", message="Some weights of.*were not initialized")
                
                # ëª¨ë¸ ë¡œë“œ
                self.model = Qwen2VLForConditionalGeneration.from_pretrained(
                    self.model_name,
                    **model_kwargs
                )
            
            # ëª¨ë¸ ì´ˆê¸°í™” ìƒíƒœ ìƒì„¸ ë¡œê¹…
            self._log_model_initialization_status()
            
            # CPUë¡œ ì´ë™ (í•„ìš”í•œ ê²½ìš°)
            if self.device.type == "cpu" and not (load_in_8bit or load_in_4bit):
                self.model = self.model.to(self.device)
            
            # LoRA ì–´ëŒ‘í„° ì¶”ê°€
            if self.use_lora:
                self._setup_lora()
            
            # í† í¬ë‚˜ì´ì € ë° í”„ë¡œì„¸ì„œ ë¡œë“œ
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.processor = AutoProcessor.from_pretrained(self.model_name)
            
            # íŒ¨ë”© í† í° ì„¤ì •
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ì„¤ì •
            self.model.train()
            
            logger.info(f"âœ… Model loaded successfully")
            logger.info(f"ğŸ“Š Model parameters: {sum(p.numel() for p in self.model.parameters()):,}")
            if self.use_lora:
                trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
                logger.info(f"ğŸ¯ Trainable parameters (LoRA): {trainable_params:,}")
            logger.info(f"ğŸ“ Vocab size: {len(self.tokenizer)}")
            
        except Exception as e:
            logger.error(f"âŒ Failed to load Qwen2.5-VL model: {e}")
            logger.info("ğŸ”„ Attempting fallback to Qwen2.5-7B-Instruct...")
            
            try:
                # Fallback to text-only model
                fallback_model = "Qwen/Qwen2.5-7B-Instruct"
                self.model = Qwen2VLForConditionalGeneration.from_pretrained(
                    fallback_model,
                    **model_kwargs
                )
                
                if self.device.type == "cpu" and not (load_in_8bit or load_in_4bit):
                    self.model = self.model.to(self.device)
                
                # LoRA ì–´ëŒ‘í„° ì¶”ê°€ (fallback modelì—ë„)
                if self.use_lora:
                    self._setup_lora()
                
                self.tokenizer = AutoTokenizer.from_pretrained(fallback_model)
                self.processor = AutoProcessor.from_pretrained(fallback_model)
                
                if self.tokenizer.pad_token is None:
                    self.tokenizer.pad_token = self.tokenizer.eos_token
                
                self.model.train()
                self.model_name = fallback_model
                
                logger.info(f"âœ… Fallback model loaded: {fallback_model}")
                
            except Exception as e2:
                logger.error(f"âŒ Fallback model loading also failed: {e2}")
                raise RuntimeError(f"Failed to load both primary and fallback models: {e}, {e2}")
    
    def _log_model_initialization_status(self):
        """
        ëª¨ë¸ ì´ˆê¸°í™” ìƒíƒœë¥¼ ì‚¬ìš©ìì—ê²Œ ëª…í™•íˆ ì•Œë ¤ì£¼ëŠ” ë©”ì„œë“œ
        """
        try:
            # ì „ì²´ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
            total_params = sum(p.numel() for p in self.model.parameters())
            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
            
            # Visual encoder ê´€ë ¨ íŒŒë¼ë¯¸í„° í™•ì¸
            visual_params = [name for name, _ in self.model.named_parameters() if 'visual' in name]
            text_params = [name for name, _ in self.model.named_parameters() if 'visual' not in name]
            
            logger.info("ğŸ“Š Model Initialization Status:")
            logger.info(f"  Total parameters: {total_params:,}")
            logger.info(f"  Trainable parameters: {trainable_params:,}")
            logger.info(f"  Visual encoder parameters: {len(visual_params):,}")
            logger.info(f"  Text model parameters: {len(text_params):,}")
            
            if visual_params:
                logger.info("")
                logger.info("ğŸ–¼ï¸ Visual Encoder Information:")
                logger.info("  âœ… Visual encoder successfully loaded")
                logger.info("  â„¹ï¸ Some visual weights may show 'newly initialized' warnings")
                logger.info("  â„¹ï¸ This is NORMAL for Qwen2.5-VL models and does not affect performance")
                logger.info("  â„¹ï¸ The model will learn appropriate visual representations during training")
                logger.info("")
                logger.info("ğŸ¯ Training Recommendation:")
                logger.info("  - Use LoRA for efficient training")
                logger.info("  - Start with lower learning rates for visual components")
                logger.info("  - Monitor visual-text alignment during training")
            else:
                logger.info("ğŸ“ Text-only model configuration detected")
            
        except Exception as e:
            logger.warning(f"âš ï¸ Could not analyze model initialization: {e}")

    def _initialize_visual_encoder_if_needed(self):
        """
        Visual encoder ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” ê°œì„ 
        
        Qwen2.5-VL ëª¨ë¸ì—ì„œ visual encoder ë¶€ë¶„ì´ ìƒˆë¡œ ì´ˆê¸°í™”ë  ë•Œ
        ë” ì ì ˆí•œ ì´ˆê¸°í™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        """
        try:
            logger.info("ğŸ”§ Checking visual encoder initialization...")
            
            # Visual encoder ëª¨ë“ˆ ì°¾ê¸°
            visual_modules = []
            for name, module in self.model.named_modules():
                if 'visual' in name and hasattr(module, 'weight'):
                    visual_modules.append((name, module))
            
            if not visual_modules:
                logger.info("â„¹ï¸ No visual encoder modules found to initialize")
                return
            
            # ì´ˆê¸°í™”ê°€ í•„ìš”í•œ ëª¨ë“ˆë“¤ì— ëŒ€í•´ ê°œì„ ëœ ì´ˆê¸°í™” ì ìš©
            initialized_count = 0
            for name, module in visual_modules:
                if hasattr(module, 'weight') and module.weight is not None:
                    # Xavier/Glorot ì´ˆê¸°í™” ì ìš©
                    if len(module.weight.shape) >= 2:
                        torch.nn.init.xavier_uniform_(module.weight)
                        initialized_count += 1
                    
                    # bias ì´ˆê¸°í™”
                    if hasattr(module, 'bias') and module.bias is not None:
                        torch.nn.init.zeros_(module.bias)
            
            if initialized_count > 0:
                logger.info(f"âœ… Improved initialization applied to {initialized_count} visual encoder modules")
            else:
                logger.info("â„¹ï¸ Visual encoder modules already properly initialized")
                
        except Exception as e:
            logger.warning(f"âš ï¸ Visual encoder initialization failed: {e}")
            logger.info("â„¹ï¸ Continuing with default initialization")

    def _setup_lora(self):
        """
        LoRA ì–´ëŒ‘í„° ì„¤ì •
        """
        if not PEFT_AVAILABLE:
            logger.warning("âš ï¸ PEFT not available, skipping LoRA setup")
            return
        
        try:
            logger.info("ğŸ¯ Setting up LoRA adapter...")
            
            # LoRA ì„¤ì •
            lora_config = LoraConfig(
                task_type=TaskType.CAUSAL_LM,
                r=self.vlm_training_config.get('lora_rank', 16),
                lora_alpha=self.vlm_training_config.get('lora_alpha', 32),
                lora_dropout=self.vlm_training_config.get('lora_dropout', 0.1),
                target_modules=self.vlm_training_config.get('lora_target_modules', ["q_proj", "v_proj", "k_proj", "o_proj"]),
                bias="none",
                inference_mode=False,  # í•™ìŠµ ëª¨ë“œ
            )
            
            # LoRA ì–´ëŒ‘í„° ì ìš©
            self.model = get_peft_model(self.model, lora_config)
            
            # í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë§Œ í™œì„±í™”
            self.model.print_trainable_parameters()
            
            logger.info("âœ… LoRA adapter setup complete")
            
        except Exception as e:
            logger.error(f"âŒ Failed to setup LoRA: {e}")
            logger.info("ğŸ”„ Falling back to full fine-tuning")
            self.use_lora = False

    def forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> Categorical:
        """
        ì •ì±… ë„¤íŠ¸ì›Œí¬ forward pass - ë‹¤ìŒ í† í° ë¶„í¬ ê³„ì‚°
        
        Args:
            input_ids (torch.Tensor): ì…ë ¥ í† í° ì‹œí€€ìŠ¤ [batch_size, seq_len]
            attention_mask (torch.Tensor, optional): ì–´í…ì…˜ ë§ˆìŠ¤í¬
            
        Returns:
            Categorical: ë‹¤ìŒ í† í°ì— ëŒ€í•œ í™•ë¥  ë¶„í¬
        """
        # ì…ë ¥ ê²€ì¦
        if input_ids.dim() == 1:
            input_ids = input_ids.unsqueeze(0)
        
        # ì–´í…ì…˜ ë§ˆìŠ¤í¬ ìƒì„± (í•„ìš”í•œ ê²½ìš°)
        if attention_mask is None:
            attention_mask = torch.ones_like(input_ids)
        
        # ëª¨ë¸ forward pass
        with torch.cuda.amp.autocast(enabled=self.device.type == "cuda"):
            outputs = self.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                return_dict=True
            )
        
        # ë§ˆì§€ë§‰ í† í°ì˜ ë¡œì§“ ì¶”ì¶œ
        logits = outputs.logits[:, -1, :]  # [batch_size, vocab_size]
        
        # í™•ë¥  ë¶„í¬ ìƒì„±
        return Categorical(logits=logits)
    
    def generate_sequence(self, prompt: str, max_new_tokens: int = None) -> Dict[str, any]:
        """
        í† í°ë³„ ìˆœì°¨ ìƒì„± (GRPO í•™ìŠµìš©)
        
        Args:
            prompt (str): ì…ë ¥ í”„ë¡¬í”„íŠ¸
            max_new_tokens (int): ìµœëŒ€ ìƒì„± í† í° ìˆ˜
            
        Returns:
            Dict: ìƒì„± ê²°ê³¼ (í† í°, ë¡œê·¸ í™•ë¥ , ìƒíƒœ ë“±)
        """
        if max_new_tokens is None:
            # Configì—ì„œ í† í° ì„¤ì •ì„ ë¡œë“œ
            try:
                config = self._load_config()
                max_new_tokens = config.get('token_settings', {}).get('max_new_tokens', 20)
            except:
                max_new_tokens = 20
        
        # í”„ë¡¬í”„íŠ¸ í† í¬ë‚˜ì´ì§•
        inputs = self.tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)
        input_ids = inputs["input_ids"].to(self.device)
        attention_mask = inputs["attention_mask"].to(self.device)
        
        # ìƒì„± ë°ì´í„° ì €ì¥
        states = []
        actions = []
        log_probs = []
        
        current_ids = input_ids.clone()
        current_mask = attention_mask.clone()
        
        self.model.eval()
        
        with torch.no_grad():
            for step in range(max_new_tokens):
                # í˜„ì¬ ìƒíƒœ ì €ì¥
                states.append(current_ids.clone())
                
                # ë‹¤ìŒ í† í° ë¶„í¬ ê³„ì‚°
                policy_dist = self.forward(current_ids, current_mask)
                
                # í† í° ìƒ˜í”Œë§
                next_token = policy_dist.sample()
                log_prob = policy_dist.log_prob(next_token)
                
                # ë°ì´í„° ì €ì¥
                actions.append(next_token)
                log_probs.append(log_prob)
                
                # ì‹œí€€ìŠ¤ ì—…ë°ì´íŠ¸
                next_token_expanded = next_token.unsqueeze(-1)  # [batch_size, 1]
                current_ids = torch.cat([current_ids, next_token_expanded], dim=-1)
                
                # ì–´í…ì…˜ ë§ˆìŠ¤í¬ ì—…ë°ì´íŠ¸
                new_mask = torch.ones((current_mask.size(0), 1), device=self.device)
                current_mask = torch.cat([current_mask, new_mask], dim=-1)
                
                # EOS í† í°ì´ë©´ ì¤‘ë‹¨
                if next_token.item() == self.tokenizer.eos_token_id:
                    break
        
        # ìƒì„±ëœ í…ìŠ¤íŠ¸ ë””ì½”ë”©
        generated_text = self.tokenizer.decode(current_ids.squeeze(), skip_special_tokens=True)
        
        return {
            'generated_text': generated_text,
            'generated_ids': current_ids,
            'states': states,
            'actions': actions,
            'log_probs': log_probs
        }
    
    def get_log_prob(self, input_ids: torch.Tensor, target_token: torch.Tensor, 
                     attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        íŠ¹ì • í† í°ì˜ ë¡œê·¸ í™•ë¥  ê³„ì‚°
        
        Args:
            input_ids (torch.Tensor): ì…ë ¥ ì‹œí€€ìŠ¤
            target_token (torch.Tensor): íƒ€ê²Ÿ í† í°
            attention_mask (torch.Tensor, optional): ì–´í…ì…˜ ë§ˆìŠ¤í¬
            
        Returns:
            torch.Tensor: ë¡œê·¸ í™•ë¥ 
        """
        policy_dist = self.forward(input_ids, attention_mask)
        return policy_dist.log_prob(target_token)

    def _count_tokens(self, text: str) -> int:
        """
        í…ìŠ¤íŠ¸ì˜ í† í° ê°œìˆ˜ë¥¼ ê³„ì‚°
        
        Args:
            text (str): í† í° ê°œìˆ˜ë¥¼ ê³„ì‚°í•  í…ìŠ¤íŠ¸
            
        Returns:
            int: í† í° ê°œìˆ˜
        """
        if not text:
            return 0
            
        try:
            if self.clip_tokenizer:
                # CLIP tokenizer ì‚¬ìš©
                tokens = self.clip_tokenizer.encode(text, add_special_tokens=True)
                return len(tokens)
            else:
                # Fallback: Qwen tokenizer ì‚¬ìš©
                tokens = self.tokenizer.encode(text, add_special_tokens=True)
                return len(tokens)
        except Exception as e:
            logger.warning(f"âš ï¸ Token counting failed: {e}, using fallback")
            return len(text.split()) + len(text) // 20

    def _truncate_to_token_limit(self, text: str, max_tokens: int) -> str:
        """
        í…ìŠ¤íŠ¸ë¥¼ í† í° ì œí•œì— ë§ê²Œ ì˜ë¼ëƒ„
        
        Args:
            text (str): ì˜ë¼ë‚¼ í…ìŠ¤íŠ¸
            max_tokens (int): ìµœëŒ€ í† í° ìˆ˜
            
        Returns:
            str: ì˜ë¼ë‚¸ í…ìŠ¤íŠ¸
        """
        if not text:
            return text
            
        current_tokens = self._count_tokens(text)
        if current_tokens <= max_tokens:
            return text
        
        # í† í° ë‹¨ìœ„ë¡œ ì˜ë¼ë‚´ê¸°
        words = text.split()
        truncated_text = ""
        
        for word in words:
            test_text = truncated_text + (" " if truncated_text else "") + word
            if self._count_tokens(test_text) > max_tokens:
                break
            truncated_text = test_text
        
        if not truncated_text:  # ë‹¨ì–´ í•˜ë‚˜ë„ ëª» ë„£ì€ ê²½ìš°
            # ë¬¸ì ë‹¨ìœ„ë¡œ ì˜ë¼ë‚´ê¸°
            for i in range(len(text)):
                test_text = text[:i+1]
                if self._count_tokens(test_text) > max_tokens:
                    truncated_text = text[:i] if i > 0 else text[:1]
                    break
            else:
                truncated_text = text
        
        logger.debug(f"ğŸ”„ Truncated text from {current_tokens} to {self._count_tokens(truncated_text)} tokens")
        return truncated_text

    def enhance_prompt(self, user_prompt: str, use_model_generation: bool = True) -> str:
        """
        ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ë¥¼ ê°œì„  (ëª¨ë¸ ìƒì„± ë˜ëŠ” ê·œì¹™ ê¸°ë°˜)
        
        Args:
            user_prompt (str): ê°œì„ í•  ì›ë³¸ í”„ë¡¬í”„íŠ¸
            use_model_generation (bool): ëª¨ë¸ ìƒì„± ì‚¬ìš© ì—¬ë¶€
            
        Returns:
            str: ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ (77í† í° ì´í•˜)
        """
        if not user_prompt or not user_prompt.strip():
            logger.warning("âš ï¸ Empty prompt provided")
            return "a beautiful image"
        
        try:
            user_prompt = user_prompt.strip()
            
            # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ì˜ í† í° ìˆ˜ í™•ì¸
            user_prompt_tokens = self._count_tokens(user_prompt)
            logger.debug(f"ğŸ“Š User prompt tokens: {user_prompt_tokens}/{self.max_token_length}")
            
            if user_prompt_tokens >= self.max_token_length:
                # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ê°€ ì´ë¯¸ ì œí•œì„ ì´ˆê³¼í•˜ëŠ” ê²½ìš° ì˜ë¼ë‚´ê¸°
                user_prompt = self._truncate_to_token_limit(user_prompt, self.max_token_length - 5)
                logger.warning(f"âš ï¸ User prompt truncated to fit token limit: {user_prompt}")
                return user_prompt
            
            if use_model_generation and hasattr(self, 'model') and self.model is not None:
                # ëª¨ë¸ì„ ì‚¬ìš©í•œ í”„ë¡¬í”„íŠ¸ ê°œì„ 
                enhanced_prompt = self._enhance_with_model(user_prompt)
            else:
                # ê·œì¹™ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ê°œì„ 
                available_tokens = self.max_token_length - user_prompt_tokens
                enhanced_prompt = self._enhance_with_placeholders(user_prompt, available_tokens)
            
            # ìµœì¢… í† í° ê¸¸ì´ ê²€ì¦
            final_tokens = self._count_tokens(enhanced_prompt)
            if final_tokens > self.max_token_length:
                enhanced_prompt = self._truncate_to_token_limit(enhanced_prompt, self.max_token_length)
                logger.warning(f"âš ï¸ Enhanced prompt truncated to fit token limit")
            
            logger.debug(f"âœ… Enhanced: '{user_prompt}' â†’ '{enhanced_prompt}' ({final_tokens} tokens)")
            return enhanced_prompt
            
        except Exception as e:
            logger.warning(f"âš ï¸ Prompt enhancement failed: {e}")
            fallback = self._fallback_enhancement(user_prompt)
            return self._truncate_to_token_limit(fallback, self.max_token_length)

    def _enhance_with_model(self, user_prompt: str) -> str:
        """
        ëª¨ë¸ì„ ì‚¬ìš©í•œ í”„ë¡¬í”„íŠ¸ ê°œì„ 
        
        Args:
            user_prompt (str): ì›ë³¸ í”„ë¡¬í”„íŠ¸
            
        Returns:
            str: ê°œì„ ëœ í”„ë¡¬í”„íŠ¸
        """
        try:
            # í”„ë¡¬í”„íŠ¸ ê°œì„ ì„ ìœ„í•œ í…œí”Œë¦¿
            enhancement_prompt = f"Enhance this image prompt to be more detailed and artistic: '{user_prompt}' -> Enhanced:"
            
            # í† í°í™”
            inputs = self.tokenizer(enhancement_prompt, return_tensors="pt", padding=True, truncation=True)
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # ìƒì„±
            self.model.eval()
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=30,
                    temperature=0.8,
                    do_sample=True,
                    top_p=0.9,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            # ë””ì½”ë”©
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # ê°œì„ ëœ ë¶€ë¶„ ì¶”ì¶œ
            if "Enhanced:" in generated_text:
                enhanced_part = generated_text.split("Enhanced:")[-1].strip()
            else:
                enhanced_part = generated_text.replace(enhancement_prompt, "").strip()
            
            # ìµœì†Œ ê¸¸ì´ í™•ì¸
            if len(enhanced_part) < 10:
                raise ValueError("Enhanced prompt too short")
            
            return enhanced_part[:200]  # ìµœëŒ€ 200ì ì œí•œ
            
        except Exception as e:
            logger.warning(f"âš ï¸ Model-based enhancement failed: {e}, using fallback")
            return self._enhance_with_placeholders(user_prompt)

    def _enhance_with_placeholders(self, user_prompt: str, available_tokens: int = None) -> str:
        """
        í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ì‚¬ìš©í•œ ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ ê°œì„  (í† í° ì œí•œ ê³ ë ¤)
        
        Args:
            user_prompt (str): ì›ë³¸ í”„ë¡¬í”„íŠ¸
            available_tokens (int): ê°œì„ ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í† í° ìˆ˜
            
        Returns:
            str: ê°œì„ ëœ í”„ë¡¬í”„íŠ¸
        """
        # ê¸°ë³¸ í’ˆì§ˆ í–¥ìƒ í‚¤ì›Œë“œë“¤
        quality_keywords = [
            "high quality", "detailed", "professional", "sharp focus",
            "well-lit", "artistic", "masterpiece", "8k resolution"
        ]
        
        style_keywords = [
            "photorealistic", "cinematic lighting", "depth of field",
            "vivid colors", "perfect composition", "award-winning"
        ]
        
        # ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        enhanced_parts = [user_prompt]
        
        # í’ˆì§ˆ í‚¤ì›Œë“œ ì¶”ê°€ (í† í° ì œí•œ ê³ ë ¤)
        import random
        all_keywords = quality_keywords + style_keywords
        random.shuffle(all_keywords)
        
        for keyword in all_keywords[:4]:  # ìµœëŒ€ 4ê°œ í‚¤ì›Œë“œ ì‹œë„
            test_prompt = ", ".join(enhanced_parts + [keyword])
            if available_tokens is None or self._count_tokens(test_prompt) <= self.max_token_length:
                enhanced_parts.append(keyword)
            else:
                break
        
        # ìµœì¢… í”„ë¡¬í”„íŠ¸ ì¡°í•©
        enhanced_prompt = ", ".join(enhanced_parts)
        
        return enhanced_prompt

    def _fallback_enhancement(self, user_prompt: str) -> str:
        """
        ê°œì„  ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•  ê¸°ë³¸ ê°œì„  ë°©ë²• (í† í° ì œí•œ ê³ ë ¤)
        
        Args:
            user_prompt (str): ì›ë³¸ í”„ë¡¬í”„íŠ¸
            
        Returns:
            str: ê¸°ë³¸ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ (77í† í° ì´í•˜)
        """
        # ê¸°ë³¸ í’ˆì§ˆ í–¥ìƒ í‚¤ì›Œë“œ ì¶”ê°€
        quality_keywords = [
            "high quality", "detailed", "professional", 
            "well-lit", "sharp focus", "artistic"
        ]
        
        # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ì˜ í† í° ìˆ˜ í™•ì¸
        user_tokens = self._count_tokens(user_prompt)
        available_tokens = self.max_token_length - user_tokens
        
        if available_tokens <= 5:  # ì—¬ìœ  í† í°ì´ ë„ˆë¬´ ì ìœ¼ë©´ ì›ë³¸ë§Œ ë°˜í™˜
            return self._truncate_to_token_limit(user_prompt, self.max_token_length)
        
        # í† í° ì œí•œì„ ê³ ë ¤í•˜ì—¬ í‚¤ì›Œë“œ ì¶”ê°€
        enhanced_parts = [user_prompt]
        
        import random
        random.shuffle(quality_keywords)
        
        for keyword in quality_keywords:
            test_prompt = ", ".join(enhanced_parts + [keyword])
            if self._count_tokens(test_prompt) <= self.max_token_length:
                enhanced_parts.append(keyword)
            else:
                break
        
        enhanced = ", ".join(enhanced_parts)
        
        logger.info(f"ğŸ”„ Using fallback enhancement: {enhanced} ({self._count_tokens(enhanced)} tokens)")
        return enhanced
    
    def get_model_info(self) -> Dict:
        """
        ëª¨ë¸ ì •ë³´ ë°˜í™˜ (ë””ë²„ê¹… ë° ë¡œê¹…ìš©)
        
        Returns:
            Dict: ëª¨ë¸ ê´€ë ¨ ì •ë³´
        """
        if hasattr(self, 'model') and self.model is not None:
            return {
                "model_name": self.model_name,
                "device": str(self.device),
                "parameters": sum(p.numel() for p in self.model.parameters()),
                "vocab_size": len(self.tokenizer) if hasattr(self, 'tokenizer') else None,
                "max_token_length": self.max_token_length
            }
        else:
            return {
                "model_name": self.model_name,
                "device": str(self.device),
                "parameters": 0,
                "vocab_size": None,
                "max_token_length": self.max_token_length,
                "status": "not_loaded"
            }


# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    # VLM Wrapper í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ğŸ§ª VLM Wrapper Test")
    print("=" * 30)
    
    try:
        # VLM ë˜í¼ ì´ˆê¸°í™”
        vlm = VLMWrapper(
            config_path="config/default_config.json",
            device="auto",
            max_token_length=77
        )
        
        print("âœ… VLM Wrapper initialized successfully")
        print(f"ğŸ“Š Model info: {vlm.get_model_info()}")
        
        # í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë“¤
        test_prompts = [
            "a cat",
            "sunset",
            "beautiful woman",
            "mountain landscape"
        ]
        
        print("\nğŸ”„ Testing prompt enhancement:")
        for prompt in test_prompts:
            enhanced = vlm.enhance_prompt(prompt)
            print(f"  '{prompt}' â†’ '{enhanced}'")
        
        print("\nâœ… All tests passed!")
        
    except Exception as e:
        print(f"âŒ Test failed: {e}")
    
    print("\nUsage:")
    print("from models.vlm_wrapper import VLMWrapper")
    print("vlm = VLMWrapper()")
    print("enhanced = vlm.enhance_prompt('a cat')")

    # ì°¸ì¡° ê¸€ê³¼ ì¼ì¹˜í•˜ëŠ” êµ¬í˜„:
    for step in range(max_new_tokens):
        # state_t = user_prompt + ì§€ê¸ˆê¹Œì§€_ìƒì„±ëœ_í† í°ë“¤
        current_state = current_sequence.clone()
        
        # action_t = ë‹¤ìŒ_í† í°_ì„ íƒ  
        policy_dist = policy_network(current_sequence)
        next_token = policy_dist.sample()
        
        # ì‹œí€€ìŠ¤ ì—…ë°ì´íŠ¸
        current_sequence = torch.cat([current_sequence, next_token])
        
        if next_token == EOS: break

    # í™˜ê²½ ì‹¤í–‰: ë™ê²°ëœ íŒŒì´í”„ë¼ì¸
    generated_text = tokenizer.decode(current_sequence)
    image = sd_generator.generate_image(generated_text)  # ë™ê²°ëœ SD3
    reward = clip_reward.calculate_reward(image, text)   # ë™ê²°ëœ CLIP 