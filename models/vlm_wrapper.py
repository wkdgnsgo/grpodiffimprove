"""
VLM (Vision Language Model) Wrapper
===================================

Qwen2.5-VLì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„¸í•˜ê³  í’ˆì§ˆ ë†’ì€ í”„ë¡¬í”„íŠ¸ë¡œ ê°œì„ í•˜ëŠ” ëª¨ë“ˆì…ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
1. í”„ë¡¬í”„íŠ¸ ê°œì„  (Prompt Enhancement)
2. í…ìŠ¤íŠ¸ ìƒì„± íŒŒë¼ë¯¸í„° ê´€ë¦¬
3. ë””ë°”ì´ìŠ¤ ìµœì í™” (MPS/CUDA/CPU)
4. ë°°ì¹˜ ì²˜ë¦¬ ì§€ì›
5. í† í° ê¸¸ì´ ì œí•œ (CLIP 77 í† í° ì œí•œ)

Author: AI Assistant
Date: 2025-01-22
"""

import torch
import torch.nn as nn
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoProcessor, CLIPTokenizer
from typing import List, Dict, Optional, Union
import logging
import json

logger = logging.getLogger(__name__)

class VLMWrapper(nn.Module):
    """
    Qwen2.5-VLì„ ë˜í•‘í•˜ì—¬ í”„ë¡¬í”„íŠ¸ ê°œì„  ê¸°ëŠ¥ì„ ì œê³µí•˜ëŠ” í´ë˜ìŠ¤
    
    ì´ í´ë˜ìŠ¤ëŠ” ì‚¬ìš©ìì˜ ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ (ì˜ˆ: "a cat")ë¥¼ ë°›ì•„ì„œ
    ë” ìƒì„¸í•˜ê³  êµ¬ì²´ì ì¸ í”„ë¡¬í”„íŠ¸ (ì˜ˆ: "a fluffy orange tabby cat sitting gracefully...")ë¡œ 
    ë³€í™˜í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
    
    77í† í° ì œí•œì„ ì¤€ìˆ˜í•˜ì—¬ CLIP text encoderì™€ í˜¸í™˜ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤.
    
    Attributes:
        model_name (str): ì‚¬ìš©í•  Qwen2.5-VL ëª¨ë¸ ì´ë¦„ (configì—ì„œ ì½ì–´ì˜´)
        tokenizer: í† í¬ë‚˜ì´ì € ê°ì²´
        processor: í”„ë¡œì„¸ì„œ ê°ì²´
        model: Qwen2.5-VL ëª¨ë¸ ê°ì²´
        clip_tokenizer: CLIP í† í¬ë‚˜ì´ì € (í† í° ê¸¸ì´ ì²´í¬ìš©)
        device: ì—°ì‚° ë””ë°”ì´ìŠ¤ (MPS/CUDA/CPU)
        generation_config (dict): í…ìŠ¤íŠ¸ ìƒì„± ì„¤ì •
        max_token_length (int): ìµœëŒ€ í† í° ê¸¸ì´ (ê¸°ë³¸ê°’: 77)
    """
    
    def __init__(self,
                 config_path: str = "config/default_config.json",
                 device: str = "auto",
                 max_new_tokens: int = 100,
                 temperature: float = 0.7,
                 top_p: float = 0.9,
                 do_sample: bool = True,
                 max_token_length: int = 77):
        """
        VLM Wrapper ì´ˆê¸°í™” (ê°„ë‹¨í•œ í”Œë ˆì´ìŠ¤í™€ë” ë°©ì‹)
        
        Args:
            config_path (str): ì„¤ì • íŒŒì¼ ê²½ë¡œ
            device (str): ë””ë°”ì´ìŠ¤ ì„¤ì • (ì‚¬ìš©ë˜ì§€ ì•ŠìŒ)
            max_new_tokens (int): ìµœëŒ€ í† í° ìˆ˜ (ì‚¬ìš©ë˜ì§€ ì•ŠìŒ)
            temperature (float): ìƒì„± ì˜¨ë„ (ì‚¬ìš©ë˜ì§€ ì•ŠìŒ)
            top_p (float): ëˆ„ì  í™•ë¥  ì„ê³„ê°’ (ì‚¬ìš©ë˜ì§€ ì•ŠìŒ)
            do_sample (bool): ìƒ˜í”Œë§ ì—¬ë¶€ (ì‚¬ìš©ë˜ì§€ ì•ŠìŒ)
            max_token_length (int): ìµœëŒ€ í† í° ê¸¸ì´ (CLIP ì œí•œ)
        """
        super().__init__()
        
        # ì„¤ì • íŒŒì¼ì—ì„œ ëª¨ë¸ ì´ë¦„ ì½ê¸° (ì°¸ê³ ìš©)
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            self.model_name = config['model_settings']['vlm_model']
            logger.info(f"ğŸ“„ VLM model name from config: {self.model_name}")
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to load config: {e}, using default")
            self.model_name = "placeholder-vlm"
        
        # ê°„ë‹¨í•œ í”Œë ˆì´ìŠ¤í™€ë” ë°©ì‹ì´ë¯€ë¡œ ì‹¤ì œ ëª¨ë¸ ë¡œë“œ ì—†ìŒ
        self.device = "cpu"  # í”Œë ˆì´ìŠ¤í™€ë” ë°©ì‹ì—ì„œëŠ” ë””ë°”ì´ìŠ¤ ë¶ˆí•„ìš”
        self.model = None
        self.tokenizer = None
        self.processor = None
        
        # CLIP í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” (í† í° ê¸¸ì´ ì²´í¬ìš©)
        try:
            self.clip_tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")
            logger.info("âœ… CLIP tokenizer loaded for token length validation")
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to load CLIP tokenizer: {e}, using fallback")
            self.clip_tokenizer = None
        
        # í† í° ê¸¸ì´ ì œí•œ ì„¤ì •
        self.max_token_length = max_token_length
        
        # ìƒì„± ì„¤ì • (ì‚¬ìš©ë˜ì§€ ì•Šì§€ë§Œ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€)
        self.generation_config = {
            'max_new_tokens': max_new_tokens,
            'temperature': temperature,
            'top_p': top_p,
            'do_sample': do_sample
        }
        
        logger.info(f"âœ… VLM Wrapper initialized with placeholder-based enhancement (max_tokens: {max_token_length})")
    
    def _count_tokens(self, text: str) -> int:
        """
        í…ìŠ¤íŠ¸ì˜ í† í° ê°œìˆ˜ë¥¼ ê³„ì‚°
        
        Args:
            text (str): í† í° ê°œìˆ˜ë¥¼ ê³„ì‚°í•  í…ìŠ¤íŠ¸
            
        Returns:
            int: í† í° ê°œìˆ˜
        """
        if not text:
            return 0
            
        try:
            if self.clip_tokenizer:
                # CLIP tokenizer ì‚¬ìš©
                tokens = self.clip_tokenizer.encode(text, add_special_tokens=True)
                return len(tokens)
            else:
                # Fallback: ëŒ€ëµì ì¸ í† í° ê°œìˆ˜ ì¶”ì • (ì˜ì–´ ê¸°ì¤€ í‰ê·  4ìë‹¹ 1í† í°)
                return len(text.split()) + len(text) // 20
        except Exception as e:
            logger.warning(f"âš ï¸ Token counting failed: {e}, using fallback")
            return len(text.split()) + len(text) // 20

    def _truncate_to_token_limit(self, text: str, max_tokens: int) -> str:
        """
        í…ìŠ¤íŠ¸ë¥¼ í† í° ì œí•œì— ë§ê²Œ ì˜ë¼ëƒ„
        
        Args:
            text (str): ì˜ë¼ë‚¼ í…ìŠ¤íŠ¸
            max_tokens (int): ìµœëŒ€ í† í° ìˆ˜
            
        Returns:
            str: ì˜ë¼ë‚¸ í…ìŠ¤íŠ¸
        """
        if not text:
            return text
            
        current_tokens = self._count_tokens(text)
        if current_tokens <= max_tokens:
            return text
        
        # í† í° ë‹¨ìœ„ë¡œ ì˜ë¼ë‚´ê¸°
        words = text.split()
        truncated_text = ""
        
        for word in words:
            test_text = truncated_text + (" " if truncated_text else "") + word
            if self._count_tokens(test_text) > max_tokens:
                break
            truncated_text = test_text
        
        if not truncated_text:  # ë‹¨ì–´ í•˜ë‚˜ë„ ëª» ë„£ì€ ê²½ìš°
            # ë¬¸ì ë‹¨ìœ„ë¡œ ì˜ë¼ë‚´ê¸°
            for i in range(len(text)):
                test_text = text[:i+1]
                if self._count_tokens(test_text) > max_tokens:
                    truncated_text = text[:i] if i > 0 else text[:1]
                    break
            else:
                truncated_text = text
        
        logger.debug(f"ğŸ”„ Truncated text from {current_tokens} to {self._count_tokens(truncated_text)} tokens")
        return truncated_text

    def _load_model(self):
        """
        ëª¨ë¸ ë¡œë“œ ë©”ì„œë“œ (í”Œë ˆì´ìŠ¤í™€ë” ë°©ì‹ì—ì„œëŠ” ì‚¬ìš©ë˜ì§€ ì•ŠìŒ)
        """
        logger.info("ğŸ“ Using placeholder-based enhancement, no model loading required")
        pass
    
    def enhance_prompt(self, user_prompt: str) -> str:
        """
        ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ë¥¼ ê°„ë‹¨í•œ í”Œë ˆì´ìŠ¤í™€ë” ë°©ì‹ìœ¼ë¡œ ê°œì„ í•˜ë˜, 77í† í° ì œí•œì„ ì¤€ìˆ˜
        
        Args:
            user_prompt (str): ê°œì„ í•  ì›ë³¸ í”„ë¡¬í”„íŠ¸
            
        Returns:
            str: ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ (77í† í° ì´í•˜)
        """
        if not user_prompt or not user_prompt.strip():
            logger.warning("âš ï¸ Empty prompt provided, using fallback")
            return self._fallback_enhancement("")
        
        try:
            # ì…ë ¥ ê²€ì¦ ë° ì •ì œ
            user_prompt = user_prompt.strip()
            if len(user_prompt) > 200:  # ë„ˆë¬´ ê¸´ í”„ë¡¬í”„íŠ¸ ì œí•œ
                user_prompt = user_prompt[:200]
                logger.warning("âš ï¸ Prompt truncated to 200 characters")
            
            # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ì˜ í† í° ìˆ˜ í™•ì¸
            user_prompt_tokens = self._count_tokens(user_prompt)
            logger.debug(f"ğŸ“Š User prompt tokens: {user_prompt_tokens}/{self.max_token_length}")
            
            if user_prompt_tokens >= self.max_token_length:
                # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ê°€ ì´ë¯¸ ì œí•œì„ ì´ˆê³¼í•˜ëŠ” ê²½ìš° ì˜ë¼ë‚´ê¸°
                user_prompt = self._truncate_to_token_limit(user_prompt, self.max_token_length - 5)
                logger.warning(f"âš ï¸ User prompt truncated to fit token limit: {user_prompt}")
                return user_prompt
            
            # ê°œì„ í•  ìˆ˜ ìˆëŠ” í† í° ìˆ˜ ê³„ì‚°
            available_tokens = self.max_token_length - user_prompt_tokens
            logger.debug(f"ğŸ“Š Available tokens for enhancement: {available_tokens}")
            
            # ê°„ë‹¨í•œ í”Œë ˆì´ìŠ¤í™€ë” ê¸°ë°˜ ê°œì„ 
            enhanced_prompt = self._enhance_with_placeholders(user_prompt, available_tokens)
            
            # ìµœì¢… í† í° ê¸¸ì´ ê²€ì¦
            final_tokens = self._count_tokens(enhanced_prompt)
            if final_tokens > self.max_token_length:
                enhanced_prompt = self._truncate_to_token_limit(enhanced_prompt, self.max_token_length)
                logger.warning(f"âš ï¸ Enhanced prompt truncated to fit token limit")
            
            logger.debug(f"âœ… Enhanced: '{user_prompt}' â†’ '{enhanced_prompt}' ({final_tokens} tokens)")
            return enhanced_prompt
            
        except Exception as e:
            logger.warning(f"âš ï¸ Prompt enhancement failed: {e}")
            fallback = self._fallback_enhancement(user_prompt)
            return self._truncate_to_token_limit(fallback, self.max_token_length)
    
    def _enhance_with_placeholders(self, user_prompt: str, available_tokens: int = None) -> str:
        """
        í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ì‚¬ìš©í•œ ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ ê°œì„  (í† í° ì œí•œ ê³ ë ¤)
        
        Args:
            user_prompt (str): ì›ë³¸ í”„ë¡¬í”„íŠ¸
            available_tokens (int): ê°œì„ ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í† í° ìˆ˜
            
        Returns:
            str: ê°œì„ ëœ í”„ë¡¬í”„íŠ¸
        """
        # ê¸°ë³¸ í’ˆì§ˆ í–¥ìƒ í‚¤ì›Œë“œë“¤
        quality_keywords = [
            "high quality", "detailed", "professional", "sharp focus",
            "well-lit", "artistic", "masterpiece", "8k resolution"
        ]
        
        style_keywords = [
            "photorealistic", "cinematic lighting", "depth of field",
            "vivid colors", "perfect composition", "award-winning"
        ]
        
        # ì¹´í…Œê³ ë¦¬ë³„ íŠ¹í™” í‚¤ì›Œë“œ
        category_keywords = {
            "person": ["portrait", "beautiful", "elegant", "expressive"],
            "woman": ["graceful", "stunning", "sophisticated", "charming"],
            "man": ["handsome", "distinguished", "confident", "strong"],
            "cat": ["fluffy", "adorable", "cute", "playful"],
            "dog": ["loyal", "friendly", "energetic", "beautiful"],
            "landscape": ["scenic", "breathtaking", "panoramic", "majestic"],
            "mountain": ["towering", "snow-capped", "dramatic", "rugged"],
            "ocean": ["crystal clear", "turquoise", "serene", "vast"],
            "forest": ["lush", "dense", "mystical", "green"],
            "city": ["urban", "modern", "bustling", "architectural"],
            "building": ["impressive", "grand", "structural", "geometric"],
            "flower": ["blooming", "colorful", "delicate", "fragrant"],
            "food": ["delicious", "appetizing", "gourmet", "fresh"],
            "car": ["sleek", "powerful", "luxury", "sporty"],
            "abstract": ["creative", "unique", "innovative", "contemporary"]
        }
        
        # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ì—ì„œ í‚¤ì›Œë“œ ê°ì§€
        user_lower = user_prompt.lower()
        detected_category = None
        
        for category, keywords in category_keywords.items():
            if category in user_lower:
                detected_category = category
                break
        
        # ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        enhanced_parts = [user_prompt]
        
        # ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ì¶”ê°€
        if detected_category:
            category_words = category_keywords[detected_category]
            for word in category_words[:2]:  # ìƒìœ„ 2ê°œë§Œ ì‚¬ìš©
                test_prompt = ", ".join(enhanced_parts + [word])
                if available_tokens is None or self._count_tokens(test_prompt) <= self.max_token_length:
                    enhanced_parts.append(word)
                else:
                    break
        
        # í’ˆì§ˆ í‚¤ì›Œë“œ ì¶”ê°€ (í† í° ì œí•œ ê³ ë ¤)
        import random
        all_keywords = quality_keywords + style_keywords
        random.shuffle(all_keywords)
        
        for keyword in all_keywords[:4]:  # ìµœëŒ€ 4ê°œ í‚¤ì›Œë“œ ì‹œë„
            test_prompt = ", ".join(enhanced_parts + [keyword])
            if available_tokens is None or self._count_tokens(test_prompt) <= self.max_token_length:
                enhanced_parts.append(keyword)
            else:
                break
        
        # ìµœì¢… í”„ë¡¬í”„íŠ¸ ì¡°í•©
        enhanced_prompt = ", ".join(enhanced_parts)
        
        return enhanced_prompt
    
    def enhance_prompts_batch(self, user_prompts: List[str]) -> List[str]:
        """
        ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•˜ì—¬ íš¨ìœ¨ì„± í–¥ìƒ
        
        Args:
            user_prompts (List[str]): ê°œì„ í•  í”„ë¡¬í”„íŠ¸ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            List[str]: ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        enhanced_prompts = []
        
        for prompt in user_prompts:
            enhanced = self.enhance_prompt(prompt)
            enhanced_prompts.append(enhanced)
        
        return enhanced_prompts
    
    def _create_enhancement_template(self, user_prompt: str) -> str:
        """
        í”„ë¡¬í”„íŠ¸ ê°œì„ ì„ ìœ„í•œ í…œí”Œë¦¿ ìƒì„±
        
        ì´ ë©”ì„œë“œëŠ” Qwen2.5-VLì´ ë” ë‚˜ì€ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•˜ë„ë¡ ìœ ë„í•˜ëŠ”
        í…œí”Œë¦¿ì„ ë§Œë“­ë‹ˆë‹¤.
        
        Args:
            user_prompt (str): ì›ë³¸ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
            
        Returns:
            str: ê°œì„ ì„ ìœ„í•œ í…œí”Œë¦¿ í”„ë¡¬í”„íŠ¸
        """
        templates = [
            f"Enhance this image prompt to be more detailed and artistic: '{user_prompt}' -> Enhanced:",
            f"Make this prompt more descriptive and visually rich: '{user_prompt}' -> Improved:",
            f"Transform this simple prompt into a detailed description: '{user_prompt}' -> Detailed:",
        ]
            
        # ëœë¤í•˜ê²Œ í…œí”Œë¦¿ ì„ íƒ (ë‹¤ì–‘ì„± ì¦ê°€)
        import random
        return random.choice(templates)
    
    def _extract_enhanced_prompt(self, generated_text: str, template: str) -> str:
        """
        ìƒì„±ëœ í…ìŠ¤íŠ¸ì—ì„œ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ë§Œ ì¶”ì¶œ
        
        Args:
            generated_text (str): Qwen2.5-VLì´ ìƒì„±í•œ ì „ì²´ í…ìŠ¤íŠ¸
            template (str): ì‚¬ìš©ëœ í…œí”Œë¦¿
            
        Returns:
            str: ì¶”ì¶œëœ ê°œì„  í”„ë¡¬í”„íŠ¸
        """
        try:
            # í…œí”Œë¦¿ ì´í›„ ë¶€ë¶„ ì¶”ì¶œ
            if "->" in template:
                split_marker = template.split("->")[-1].strip()
                if split_marker in generated_text:
                    enhanced_part = generated_text.split(split_marker, 1)[-1]
                else:
                    enhanced_part = generated_text.replace(template, "")
            else:
                enhanced_part = generated_text.replace(template, "")
            
            # ì •ì œ ë° ì •ë¦¬
            enhanced_part = enhanced_part.strip()
            
            # ë¶ˆí•„ìš”í•œ ë¶€ë¶„ ì œê±°
            enhanced_part = enhanced_part.split('\n')[0]  # ì²« ë²ˆì§¸ ì¤„ë§Œ
            enhanced_part = enhanced_part.split('.')[0]   # ì²« ë²ˆì§¸ ë¬¸ì¥ë§Œ
            
            # ìµœì†Œ ê¸¸ì´ í™•ì¸
            if len(enhanced_part) < 10:
                raise ValueError("Enhanced prompt too short")
            
            return enhanced_part.strip()
            
        except Exception as e:
            logger.warning(f"âš ï¸ Enhanced prompt extraction failed: {e}")
            return generated_text.strip()[:200]  # ì²˜ìŒ 200ìë§Œ ì‚¬ìš©
    
    def _fallback_enhancement(self, user_prompt: str) -> str:
        """
        Qwen2.5-VL ê°œì„  ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•  ê¸°ë³¸ ê°œì„  ë°©ë²• (í† í° ì œí•œ ê³ ë ¤)
        
        ì´ ë©”ì„œë“œëŠ” Qwen2.5-VLì´ ì‹¤íŒ¨í–ˆì„ ë•Œ ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ
        ê¸°ë³¸ì ì¸ í”„ë¡¬í”„íŠ¸ ê°œì„ ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            user_prompt (str): ì›ë³¸ í”„ë¡¬í”„íŠ¸
            
        Returns:
            str: ê¸°ë³¸ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ (77í† í° ì´í•˜)
        """
        # ê¸°ë³¸ í’ˆì§ˆ í–¥ìƒ í‚¤ì›Œë“œ ì¶”ê°€
        quality_keywords = [
            "high quality", "detailed", "professional", 
            "well-lit", "sharp focus", "artistic"
        ]
        
        # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ì˜ í† í° ìˆ˜ í™•ì¸
        user_tokens = self._count_tokens(user_prompt)
        available_tokens = self.max_token_length - user_tokens
        
        if available_tokens <= 5:  # ì—¬ìœ  í† í°ì´ ë„ˆë¬´ ì ìœ¼ë©´ ì›ë³¸ë§Œ ë°˜í™˜
            return self._truncate_to_token_limit(user_prompt, self.max_token_length)
        
        # í† í° ì œí•œì„ ê³ ë ¤í•˜ì—¬ í‚¤ì›Œë“œ ì¶”ê°€
        enhanced_parts = [user_prompt]
        
        import random
        random.shuffle(quality_keywords)
        
        for keyword in quality_keywords:
            test_prompt = ", ".join(enhanced_parts + [keyword])
            if self._count_tokens(test_prompt) <= self.max_token_length:
                enhanced_parts.append(keyword)
            else:
                break
        
        enhanced = ", ".join(enhanced_parts)
        
        logger.info(f"ğŸ”„ Using fallback enhancement: {enhanced} ({self._count_tokens(enhanced)} tokens)")
        return enhanced
    
    def get_model_info(self) -> Dict:
        """
        ëª¨ë¸ ì •ë³´ ë°˜í™˜ (ë””ë²„ê¹… ë° ë¡œê¹…ìš©)
        
        Returns:
            Dict: ëª¨ë¸ ê´€ë ¨ ì •ë³´
        """
        return {
            "model_name": self.model_name,
            "device": str(self.device),
            "parameters": sum(p.numel() for p in self.model.parameters()),
            "generation_config": self.generation_config,
            "vocab_size": self.tokenizer.vocab_size if hasattr(self.tokenizer, 'vocab_size') else None
        }
    
    def update_generation_config(self, **kwargs):
        """
        í…ìŠ¤íŠ¸ ìƒì„± ì„¤ì • ì—…ë°ì´íŠ¸
        
        Args:
            **kwargs: ì—…ë°ì´íŠ¸í•  ì„¤ì •ë“¤
        """
        self.generation_config.update(kwargs)
        logger.info(f"ğŸ”§ Generation config updated: {kwargs}")


# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    # VLM Wrapper í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ğŸ§ª VLM Wrapper Test")
    print("=" * 30)
    
    try:
        # VLM ë˜í¼ ì´ˆê¸°í™”
        vlm = VLMWrapper(
            config_path="config/default_config.json",
            device="auto",
            max_new_tokens=100,
            temperature=0.7
        )
        
        print("âœ… VLM Wrapper initialized successfully")
        print(f"ğŸ“Š Model info: {vlm.get_model_info()}")
        
        # í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë“¤
        test_prompts = [
            "a cat",
            "sunset",
            "beautiful woman",
            "mountain landscape"
        ]
        
        print("\nğŸ”„ Testing prompt enhancement:")
        for prompt in test_prompts:
            enhanced = vlm.enhance_prompt(prompt)
            print(f"  '{prompt}' â†’ '{enhanced}'")
        
        print("\nâœ… All tests passed!")
        
    except Exception as e:
        print(f"âŒ Test failed: {e}")
    
    print("\nUsage:")
    print("from models.vlm_wrapper import VLMWrapper")
    print("vlm = VLMWrapper()")
    print("enhanced = vlm.enhance_prompt('a cat')")

    # ì°¸ì¡° ê¸€ê³¼ ì¼ì¹˜í•˜ëŠ” êµ¬í˜„:
    for step in range(max_new_tokens):
        # state_t = user_prompt + ì§€ê¸ˆê¹Œì§€_ìƒì„±ëœ_í† í°ë“¤
        current_state = current_sequence.clone()
        
        # action_t = ë‹¤ìŒ_í† í°_ì„ íƒ  
        policy_dist = policy_network(current_sequence)
        next_token = policy_dist.sample()
        
        # ì‹œí€€ìŠ¤ ì—…ë°ì´íŠ¸
        current_sequence = torch.cat([current_sequence, next_token])
        
        if next_token == EOS: break

    # í™˜ê²½ ì‹¤í–‰: ë™ê²°ëœ íŒŒì´í”„ë¼ì¸
    generated_text = tokenizer.decode(current_sequence)
    image = sd_generator.generate_image(generated_text)  # ë™ê²°ëœ SD3
    reward = clip_reward.calculate_reward(image, text)   # ë™ê²°ëœ CLIP 