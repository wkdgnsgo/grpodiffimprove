"""
Stable Diffusion 3 Generator for GRPO Training
==============================================

Stable Diffusion 3ë¥¼ ì‚¬ìš©í•˜ì—¬ í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ë¡œë¶€í„° ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ëª¨ë“ˆì…ë‹ˆë‹¤.
GRPO í•™ìŠµì—ì„œ í™˜ê²½ ì—­í• ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
1. Enhanced promptë¡œ ê³ í’ˆì§ˆ ì´ë¯¸ì§€ ìƒì„±
2. ë°°ì¹˜ ìƒì„± ì§€ì›
3. ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ê´€ë¦¬
4. HuggingFace í† í° ìë™ ë¡œê·¸ì¸ ì§€ì›

Author: AI Assistant
Date: 2025-01-22
"""

import torch
from diffusers import StableDiffusion3Pipeline
from PIL import Image
from typing import List, Dict, Optional, Union, Tuple
import logging
import numpy as np

logger = logging.getLogger(__name__)

class SD3Generator:
    """
    Stable Diffusion 3ë¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ìƒì„±ê¸° í´ë˜ìŠ¤
    
    GRPO í•™ìŠµì—ì„œ í™˜ê²½ ì—­í• :
    - State: user_prompt + placeholder + generated_tokens
    - Action: ë‹¤ìŒ í† í° ì„ íƒ
    - Reward: CLIP(user_prompt, generated_image)
    """
    
    def __init__(self,
                 model_name: str = "stabilityai/stable-diffusion-3-medium-diffusers",
                 device: str = "auto",
                 height: int = 1024,
                 width: int = 1024,
                 num_inference_steps: int = 28,
                 guidance_scale: float = 7.0):
        """
        SD3 Generator ì´ˆê¸°í™”
        
        Args:
            model_name (str): SD3 ëª¨ë¸ ì´ë¦„
            device (str): ë””ë°”ì´ìŠ¤ ì„¤ì •
            height (int): ìƒì„±í•  ì´ë¯¸ì§€ ë†’ì´
            width (int): ìƒì„±í•  ì´ë¯¸ì§€ ë„ˆë¹„
            num_inference_steps (int): ì¶”ë¡  ìŠ¤í… ìˆ˜
            guidance_scale (float): ê°€ì´ë˜ìŠ¤ ìŠ¤ì¼€ì¼
        """
        self.model_name = model_name
        self.height = height
        self.width = width
        self.num_inference_steps = num_inference_steps
        self.guidance_scale = guidance_scale
        
        # ë””ë°”ì´ìŠ¤ ìë™ ì„ íƒ (Multi-GPU ì§€ì›)
        if device == "auto":
            try:
                import sys
                import os
                sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
                from gpu_config import get_device_for_model
                device_str = get_device_for_model('sd3')
                self.device = device_str
                logger.info(f"ğŸš€ Using assigned GPU for SD3: {device_str}")
            except ImportError:
                if torch.backends.mps.is_available():
                    self.device = "mps"
                    logger.info("ğŸ Using Apple Silicon MPS for SD3")
                elif torch.cuda.is_available():
                    self.device = "cuda"
                    logger.info("ğŸš€ Using CUDA GPU for SD3")
                else:
                    self.device = "cpu"
                    logger.info("ğŸ’» Using CPU for SD3 (slow)")
        else:
            self.device = device
        
        # íŒŒì´í”„ë¼ì¸ ë¡œë“œ
        self._load_pipeline()
        
        logger.info(f"âœ… SD3 Generator initialized: {self.model_name}")
    
    def _load_pipeline(self):
        """SD3 íŒŒì´í”„ë¼ì¸ ë¡œë“œ"""
        try:
            logger.info(f"ğŸ“¥ Loading SD3 pipeline: {self.model_name}")
            
            self.pipeline = StableDiffusion3Pipeline.from_pretrained(
                self.model_name,
                torch_dtype=torch.float16 
            )
            
            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            self.pipeline = self.pipeline.to(self.device)
            
            # ë©”ëª¨ë¦¬ ìµœì í™”
            if self.device == "mps":
                self.pipeline.enable_attention_slicing()
            elif self.device == "cuda":
                self.pipeline.enable_attention_slicing()
                self.pipeline.enable_model_cpu_offload()
            
            # ì•ˆì „ ì²´í¬ ë¹„í™œì„±í™” (ì—°êµ¬ìš©)
            if hasattr(self.pipeline, 'safety_checker'):
                self.pipeline.safety_checker = None
            if hasattr(self.pipeline, 'requires_safety_checker'):
                self.pipeline.requires_safety_checker = False
            
            # Progress bar ë¹„í™œì„±í™”
            self.pipeline.set_progress_bar_config(disable=True)
            
            logger.info(f"âœ… SD3 pipeline loaded successfully on {self.device}")
            
        except Exception as e:
            logger.error(f"âŒ Failed to load SD3 pipeline: {e}")
            raise RuntimeError(f"SD3 pipeline loading failed: {e}")
    
    def generate_image(self, prompt: str, seed: Optional[int] = None) -> Optional[Image.Image]:
        """
        í”„ë¡¬í”„íŠ¸ë¡œ ì´ë¯¸ì§€ ìƒì„±
        
        Args:
            prompt (str): ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸
            seed (int, optional): ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìœ„í•œ ì‹œë“œ
            
        Returns:
            PIL.Image.Image: ìƒì„±ëœ ì´ë¯¸ì§€
        """
        try:
            # ì‹œë“œ ì„¤ì •
            generator = None
            if seed is not None:
                generator = torch.Generator(device=self.device).manual_seed(seed)
            
            # ì´ë¯¸ì§€ ìƒì„±
            with torch.no_grad():
                result = self.pipeline(
                    prompt=prompt,
                    height=self.height,
                    width=self.width,
                    num_inference_steps=self.num_inference_steps,
                    guidance_scale=self.guidance_scale,
                    generator=generator,
                    num_images_per_prompt=1
                )
            
            image = result.images[0]
            
            # ì´ë¯¸ì§€ ê²€ì¦
            if self._validate_image(image):
                logger.debug(f"ğŸ–¼ï¸ Generated image for: '{prompt[:50]}...'")
                return image
            else:
                logger.warning(f"âš ï¸ Generated invalid image for: '{prompt[:50]}...'")
                return self._create_fallback_image()
                
        except Exception as e:
            logger.error(f"âŒ Image generation failed: {e}")
            return self._create_fallback_image()
    
    def generate_images_batch(self, 
                             prompts: List[str],
                             seeds: Optional[List[int]] = None) -> List[Image.Image]:
        """
        ë°°ì¹˜ë¡œ ì—¬ëŸ¬ ì´ë¯¸ì§€ ìƒì„±
        
        Args:
            prompts (List[str]): í”„ë¡¬í”„íŠ¸ ë¦¬ìŠ¤íŠ¸
            seeds (List[int], optional): ì‹œë“œ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            List[Image.Image]: ìƒì„±ëœ ì´ë¯¸ì§€ë“¤
        """
        images = []
        
        for i, prompt in enumerate(prompts):
            seed = seeds[i] if seeds and i < len(seeds) else None
            image = self.generate_image(prompt, seed)
            images.append(image)
        
        logger.info(f"ğŸ“Š Generated {len(images)} images in batch")
        return images
    
    def _validate_image(self, image: Image.Image) -> bool:
        """ìƒì„±ëœ ì´ë¯¸ì§€ ê²€ì¦"""
        try:
            if image is None:
                return False
            
            # í¬ê¸° ê²€ì¦
            if image.size[0] < 64 or image.size[1] < 64:
                return False
            
            # ì±„ë„ ê²€ì¦
            if len(image.getbands()) < 3:
                return False
            
            # í”½ì…€ ê°’ ê²€ì¦ (ëª¨ë“  í”½ì…€ì´ ë™ì¼í•œ ìƒ‰ì´ ì•„ë‹Œì§€)
            img_array = np.array(image)
            if np.std(img_array) < 1.0:  # ë„ˆë¬´ ë‹¨ì¡°ë¡œìš´ ì´ë¯¸ì§€
                return False
            
            return True
            
        except Exception:
            return False
    
    def _create_fallback_image(self) -> Image.Image:
        """ì‹¤íŒ¨ ì‹œ í´ë°± ì´ë¯¸ì§€ ìƒì„±"""
        # ë‹¨ìˆœí•œ ê·¸ë¼ë””ì–¸íŠ¸ ì´ë¯¸ì§€ ìƒì„±
        width, height = self.width, self.height
        image = Image.new('RGB', (width, height))
        
        pixels = []
        for y in range(height):
            for x in range(width):
                # ê°„ë‹¨í•œ ê·¸ë¼ë””ì–¸íŠ¸
                r = int(255 * x / width)
                g = int(255 * y / height)
                b = 128
                pixels.append((r, g, b))
        
        image.putdata(pixels)
        return image
    
    def get_model_info(self) -> Dict:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            'model_name': self.model_name,
            'device': self.device,
            'image_size': f"{self.width}x{self.height}",
            'inference_steps': self.num_inference_steps,
            'guidance_scale': self.guidance_scale
        } 