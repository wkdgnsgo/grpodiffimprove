"""
QWEN 7B Wrapper for Prompt Enhancement
=====================================

QWEN 7B ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ User promptë¥¼ í–¥ìƒëœ promptë¡œ ë³€í™˜í•˜ëŠ” ëª¨ë“ˆì…ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
1. User prompt + placeholder êµ¬ì¡°ë¡œ ì…ë ¥ êµ¬ì„±
2. QWEN 7Bë¡œ í–¥ìƒëœ prompt ìƒì„±
3. ìƒì„±ëœ prompt ê²€ì¦ ë° í›„ì²˜ë¦¬

Author: AI Assistant
Date: 2025-01-22
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig
from typing import List, Dict, Optional, Union
import logging
import re
import json

logger = logging.getLogger(__name__)

class QwenWrapper:
    """
    QWEN VL ëª¨ë¸ì„ ì‚¬ìš©í•œ í”„ë¡¬í”„íŠ¸ í–¥ìƒ í´ë˜ìŠ¤
    
    ì´ í´ë˜ìŠ¤ëŠ”:
    1. User promptë¥¼ ë°›ì•„ì„œ placeholderì™€ í•¨ê»˜ êµ¬ì„±
    2. QWEN VLë¡œ í–¥ìƒëœ prompt ìƒì„±
    3. ìƒì„±ëœ ê²°ê³¼ ê²€ì¦ ë° ì •ì œ
    """
    
    def __init__(self, 
                 model_name: str = "Qwen/Qwen2-VL-7B-Instruct",
                 device: str = "auto",
                 max_new_tokens: int = 100,
                 temperature: float = 0.7):
        """
        QWEN Wrapper ì´ˆê¸°í™”
        
        Args:
            model_name (str): QWEN ëª¨ë¸ ì´ë¦„
            device (str): ë””ë°”ì´ìŠ¤ ì„¤ì •
            max_new_tokens (int): ìµœëŒ€ ìƒì„± í† í° ìˆ˜
            temperature (float): ìƒì„± ì˜¨ë„
        """
        self.model_name = model_name
        self.max_new_tokens = max_new_tokens
        self.temperature = temperature
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì • (Multi-GPU ì§€ì›)
        if device == "auto":
            try:
                from gpu_config import get_device_for_model
                device_str = get_device_for_model('qwen')
                self.device = torch.device(device_str)
                logger.info(f"ğŸš€ Using assigned GPU for QWEN: {device_str}")
            except ImportError:
                if torch.cuda.is_available():
                    self.device = torch.device("cuda")
                    logger.info("ğŸš€ Using CUDA GPU")
                elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                    self.device = torch.device("mps")
                    logger.info("ğŸ Using Apple Silicon MPS")
                else:
                    self.device = torch.device("cpu")
                    logger.info("ğŸ’» Using CPU")
        else:
            self.device = torch.device(device)
        
        # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
        self._load_model()
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •
        self._setup_prompt_template()
        
        logger.info(f"âœ… QWEN Wrapper initialized: {self.model_name}")
    
    def _load_model(self):
        """QWEN ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ"""
        try:
            logger.info(f"ğŸ“¥ Loading QWEN VL model: {self.model_name}")
            
            # Distributed training í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ìë™ distributed ë°©ì§€)
            import os
            if 'RANK' not in os.environ:
                os.environ['RANK'] = '0'
            if 'WORLD_SIZE' not in os.environ:
                os.environ['WORLD_SIZE'] = '1'
            if 'LOCAL_RANK' not in os.environ:
                os.environ['LOCAL_RANK'] = '0'
            if 'MASTER_ADDR' not in os.environ:
                os.environ['MASTER_ADDR'] = 'localhost'
            if 'MASTER_PORT' not in os.environ:
                os.environ['MASTER_PORT'] = '12355'
            
            # VL ëª¨ë¸ì„ ìœ„í•œ ì„í¬íŠ¸
            from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
            
            # í”„ë¡œì„¸ì„œ ë¡œë“œ (VL ëª¨ë¸ì€ processor ì‚¬ìš©)
            self.processor = AutoProcessor.from_pretrained(
                self.model_name,
                trust_remote_code=True
            )
            self.tokenizer = self.processor.tokenizer
            
            # VL ëª¨ë¸ ë¡œë“œ (distributed ëª¨ë“œ ë°©ì§€)
            model_kwargs = {
                'torch_dtype': torch.float16 if self.device.type in ['cuda', 'mps'] else torch.float32,
                'trust_remote_code': True,
                'low_cpu_mem_usage': True
            }
            
            # device_mapì„ íŠ¹ì • GPUë¡œ ê³ ì • (auto ì‚¬ìš© ì•ˆí•¨)
            if self.device.type == "cuda":
                model_kwargs['device_map'] = {
                    '': self.device  # ì „ì²´ ëª¨ë¸ì„ ì§€ì •ëœ GPUë¡œ
                }
            
            self.model = Qwen2VLForConditionalGeneration.from_pretrained(
                self.model_name,
                **model_kwargs
            )
            
            # CPUë¡œ ì´ë™ (í•„ìš”í•œ ê²½ìš°)
            if self.device.type != "cuda":
                self.model = self.model.to(self.device)
            
            # íŒ¨ë”© í† í° ì„¤ì •
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # ìƒì„± ì„¤ì •
            self.generation_config = GenerationConfig(
                max_new_tokens=self.max_new_tokens,
                temperature=self.temperature,
                top_p=0.9,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )
            
            logger.info("âœ… Model loaded successfully")
            
        except Exception as e:
            logger.error(f"âŒ Failed to load model: {e}")
            raise
    
    def _setup_prompt_template(self):
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì • - user_prompt + placeholder ë°©ì‹"""
        # Placeholder í…œí”Œë¦¿ (user_prompt ë’¤ì— ì¶”ê°€ë¨)
        self.placeholder_template = ", high quality, detailed, professional photography, cinematic lighting, artistic composition, 4k resolution"
        
        # ìƒì„± ì§€ì‹œë¥¼ ìœ„í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        self.system_prompt = """You are an expert at enhancing image generation prompts. 
Given a user prompt with a placeholder, complete the prompt by adding artistic details, style descriptions, and technical specifications.

Guidelines:
- Keep the original user prompt unchanged
- Add detailed descriptions after the placeholder
- Include artistic style, lighting, composition details
- Make it suitable for high-quality image generation
- Be concise but descriptive"""

        # ì‚¬ìš©ì ì…ë ¥ í…œí”Œë¦¿ (user_prompt + placeholder)
        self.user_template = """{user_prompt_with_placeholder}

Complete this prompt with detailed enhancements: """
    
    def enhance_prompt(self, user_prompt: str) -> Dict[str, str]:
        """
        User prompt + placeholderë¥¼ í–¥ìƒëœ promptë¡œ ë³€í™˜
        
        Args:
            user_prompt (str): ì›ë³¸ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
            
        Returns:
            Dict[str, str]: ì›ë³¸, placeholder ì¶”ê°€, ìµœì¢… í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ë¥¼ í¬í•¨í•œ ê²°ê³¼
        """
        try:
            # Step 1: user_prompt + placeholder ìƒì„±
            user_prompt_with_placeholder = user_prompt + self.placeholder_template
            
            # Step 2: VLMì— ì…ë ¥í•  ë©”ì‹œì§€ êµ¬ì„±
            messages = [
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": self.user_template.format(user_prompt_with_placeholder=user_prompt_with_placeholder)}
            ]
            
            # í…œí”Œë¦¿ ì ìš©
            prompt = self.tokenizer.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
            
            # í† í¬ë‚˜ì´ì§•
            inputs = self.tokenizer(
                prompt, 
                return_tensors="pt", 
                padding=True, 
                truncation=True
            ).to(self.device)
            
            # ìƒì„±
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    generation_config=self.generation_config,
                    pad_token_id=self.tokenizer.pad_token_id
                )
            
            # ë””ì½”ë”©
            generated_text = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:], 
                skip_special_tokens=True
            ).strip()
            
            # í›„ì²˜ë¦¬
            enhanced_prompt = self._post_process_output(generated_text)
            
            result = {
                'original_prompt': user_prompt,
                'prompt_with_placeholder': user_prompt_with_placeholder,
                'enhanced_prompt': enhanced_prompt,
                'raw_output': generated_text
            }
            
            logger.info(f"âœ¨ Enhanced prompt: '{user_prompt}' + placeholder -> '{enhanced_prompt[:50]}...'")
            return result
            
        except Exception as e:
            logger.error(f"âŒ Prompt enhancement failed: {e}")
            # ì‹¤íŒ¨ ì‹œ placeholderë§Œ ì¶”ê°€ëœ ë²„ì „ ë°˜í™˜
            user_prompt_with_placeholder = user_prompt + self.placeholder_template
            return {
                'original_prompt': user_prompt,
                'prompt_with_placeholder': user_prompt_with_placeholder,
                'enhanced_prompt': user_prompt_with_placeholder,
                'raw_output': f"Error: {e}"
            }
    
    def _post_process_output(self, raw_output: str) -> str:
        """ìƒì„±ëœ ì¶œë ¥ í›„ì²˜ë¦¬"""
        # ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±°
        enhanced = raw_output.strip()
        
        # "Enhanced prompt:" ë“±ì˜ ë ˆì´ë¸” ì œê±°
        enhanced = re.sub(r'^(Enhanced prompt:|Prompt:|Result:)\s*', '', enhanced, flags=re.IGNORECASE)
        enhanced = enhanced.strip()
        
        # ë”°ì˜´í‘œ ì œê±°
        enhanced = enhanced.strip('"\'')
        
        # ë¹ˆ ê²°ê³¼ ì²˜ë¦¬
        if not enhanced:
            return "high quality, detailed"
        
        return enhanced
    
    def enhance_prompts_batch(self, user_prompts: List[str]) -> List[Dict[str, str]]:
        """ë°°ì¹˜ë¡œ ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ í–¥ìƒ"""
        results = []
        for prompt in user_prompts:
            result = self.enhance_prompt(prompt)
            results.append(result)
        return results
    
    def save_model(self, save_path: str):
        """ëª¨ë¸ ì €ì¥"""
        try:
            import os
            os.makedirs(save_path, exist_ok=True)
            
            # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì €ì¥
            self.model.save_pretrained(save_path)
            self.tokenizer.save_pretrained(save_path)
            
            logger.info(f"ğŸ’¾ Model saved to {save_path}")
            
        except Exception as e:
            logger.error(f"âŒ Failed to save model: {e}")
            raise
    
    def get_model_info(self) -> Dict:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            'model_name': self.model_name,
            'device': str(self.device),
            'max_new_tokens': self.max_new_tokens,
            'temperature': self.temperature,
            'vocab_size': len(self.tokenizer)
        } 