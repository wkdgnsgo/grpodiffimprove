"""
GRPO Training Main Script
========================

QWEN ëª¨ë¸ì„ GRPO ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ í•™ìŠµì‹œí‚¤ëŠ” ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.

ì‹¤í–‰ ì˜ˆì‹œ:
    python train_grpo.py

Author: AI Assistant
Date: 2025-01-22
"""

import sys
import os
import logging
import matplotlib.pyplot as plt
import numpy as np
from typing import List

# ë¡œì»¬ ëª¨ë“ˆ ì„í¬íŠ¸
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# Multi-GPU ì„¤ì • ë¨¼ì € ì´ˆê¸°í™”
from gpu_config import setup_multi_gpu

from models.qwen_wrapper import QwenWrapper
from models.sd3_generator import SD3Generator
from models.clip_reward import CLIPRewardCalculator
from training.grpo_trainer import GRPOTrainer, GRPOConfig

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('grpo_training.log')
    ]
)

logger = logging.getLogger(__name__)

def get_training_prompts() -> List[str]:
    """í•™ìŠµìš© í”„ë¡¬í”„íŠ¸ ì„¸íŠ¸ ë°˜í™˜ - Challenging cases í¬í•¨"""
    return [
        # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ë“¤
        "a cute cat sitting on a windowsill",
        "a beautiful sunset over mountains",
        "a robot walking in a futuristic city",
        "a delicious pizza with melted cheese",
        "a magical forest with glowing trees",
        
        # SD3ê°€ ì–´ë ¤ì›Œí•˜ëŠ” ìƒ‰ìƒ ì¡°í•© (Challenging cases)
        "a purple rabbit eating carrots",
        "a green cat with blue eyes",
        "a yellow dog with pink spots",
        "a red elephant with white stripes",
        "a blue horse running in a field",
        "an orange penguin on ice",
        "a pink tiger in the jungle",
        "a silver monkey climbing trees",
        
        # ë³µì¡í•œ ìƒ‰ìƒê³¼ ì†ì„± ì¡°í•©
        "a rainbow colored fish swimming underwater",
        "a transparent glass butterfly",
        "a metallic chrome fox in a forest",
        "a neon glowing wolf at night",
        "a crystal ice bear in a cave",
        "a golden feathered dragon",
        
        # ì´ìƒí•œ/ëª¨ìˆœì ì¸ ì¡°í•©ë“¤
        "a square wheel rolling down a hill",
        "a flying fish with wings made of leaves",
        "a tree growing upside down with roots in the sky",
        "a house made of clouds floating in water",
        "a car with legs instead of wheels",
        "a book that is also a bird",
        
        # ë³µì¡í•œ ì¥ë©´ë“¤
        "multiple colored cats playing chess",
        "a purple wizard cat casting green magic",
        "a robot made of different colored fruits",
        "a rainbow bridge connecting two moons",
        "a city where all buildings are different colors",
        "a garden with flowers that are also animals",
        
        # ì¶”ìƒì /ê°œë…ì  í”„ë¡¬í”„íŠ¸ë“¤
        "the concept of happiness as a creature",
        "time flowing backwards in a room",
        "music made visible as colorful shapes",
        "a dream within a dream landscape",
        "thoughts becoming physical objects",
        "emotions taking the form of weather"
    ]

def plot_training_results(trainer: GRPOTrainer, save_path: str = "grpo_results.png"):
    """í•™ìŠµ ê²°ê³¼ í”Œë¡¯ ìƒì„±"""
    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    
    # 1. í‰ê·  ë³´ìƒ
    axes[0, 0].plot(trainer.iteration_rewards, label='Avg Reward')
    axes[0, 0].set_title('GRPO QWEN: Average Reward per Iteration')
    axes[0, 0].set_xlabel('Iteration')
    axes[0, 0].set_ylabel('Avg Reward')
    axes[0, 0].grid(True)
    axes[0, 0].legend()
    
    # ì´ë™ í‰ê·  ì¶”ê°€
    if len(trainer.iteration_rewards) >= 5:
        window = min(5, len(trainer.iteration_rewards))
        ma_rewards = np.convolve(trainer.iteration_rewards, np.ones(window)/window, mode='valid')
        axes[0, 0].plot(range(window-1, len(trainer.iteration_rewards)), ma_rewards, 
                       label=f'{window}-iter MA', linestyle='--')
        axes[0, 0].legend()
    
    # 2. ì •ì±… ëª©ì  í•¨ìˆ˜
    axes[0, 1].plot(trainer.iteration_policy_losses, label='Policy Objective')
    axes[0, 1].set_title('GRPO QWEN: Policy Objective per Iteration')
    axes[0, 1].set_xlabel('Iteration')
    axes[0, 1].set_ylabel('Objective Value')
    axes[0, 1].grid(True)
    axes[0, 1].legend()
    
    # 3. KL ë°œì‚°
    axes[0, 2].plot(trainer.iteration_kl_divs, label='KL Divergence')
    axes[0, 2].set_title('GRPO QWEN: KL Divergence per Iteration')
    axes[0, 2].set_xlabel('Iteration')
    axes[0, 2].set_ylabel('KL Divergence')
    axes[0, 2].grid(True)
    axes[0, 2].legend()
    
    # 4. ì—”íŠ¸ë¡œí”¼
    axes[1, 0].plot(trainer.iteration_entropies, label='Entropy')
    axes[1, 0].set_title('GRPO QWEN: Policy Entropy per Iteration')
    axes[1, 0].set_xlabel('Iteration')
    axes[1, 0].set_ylabel('Entropy')
    axes[1, 0].grid(True)
    axes[1, 0].legend()
    
    # 5. ë³´ìƒ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
    if trainer.iteration_rewards:
        axes[1, 1].hist(trainer.iteration_rewards, bins=20, alpha=0.7, edgecolor='black')
        axes[1, 1].set_title('Reward Distribution')
        axes[1, 1].set_xlabel('Reward Value')
        axes[1, 1].set_ylabel('Frequency')
        axes[1, 1].grid(True)
    
    # 6. í•™ìŠµ í†µê³„ ìš”ì•½
    axes[1, 2].axis('off')
    stats_text = f"""
Learning Statistics:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Total Iterations: {len(trainer.iteration_rewards)}
Final Avg Reward: {trainer.iteration_rewards[-1]:.3f if trainer.iteration_rewards else 0:.3f}
Max Reward: {max(trainer.iteration_rewards) if trainer.iteration_rewards else 0:.3f}
Min Reward: {min(trainer.iteration_rewards) if trainer.iteration_rewards else 0:.3f}
Reward Std: {np.std(trainer.iteration_rewards) if trainer.iteration_rewards else 0:.3f}

Final KL Div: {trainer.iteration_kl_divs[-1]:.4f if trainer.iteration_kl_divs else 0:.4f}
Final Entropy: {trainer.iteration_entropies[-1]:.4f if trainer.iteration_entropies else 0:.4f}
    """
    axes[1, 2].text(0.05, 0.95, stats_text, transform=axes[1, 2].transAxes, 
                    fontsize=12, verticalalignment='top', fontfamily='monospace')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.show()
    
    logger.info(f"ğŸ“Š Training results saved to {save_path}")

def main():
    """ë©”ì¸ í•™ìŠµ í•¨ìˆ˜"""
    logger.info("ğŸš€ Starting GRPO Training for QWEN Model")
    
    try:
        # 0. Multi-GPU í™˜ê²½ ì„¤ì •
        logger.info("ğŸ”§ Setting up Multi-GPU environment...")
        gpu_config = setup_multi_gpu()
        
        # 1. ëª¨ë¸ ì´ˆê¸°í™” (ê°ê° ë‹¤ë¥¸ GPUì— ë°°ì¹˜)
        logger.info("ğŸ“¥ Initializing models on assigned GPUs...")
        logger.info("  ğŸ¯ QWEN VL â†’ GPU 1 (cuda:0)")
        logger.info("  ğŸ–¼ï¸ SD3 â†’ GPU 2 (cuda:1)")  
        logger.info("  ğŸ“ CLIP â†’ GPU 3 (cuda:2)")
        
        # QWEN ëª¨ë¸ ë¡œë“œ (GPU 1)
        qwen_model = QwenWrapper()
        
        # SD3 Generator ë¡œë“œ (GPU 2)
        sd3_generator = SD3Generator(
            height=1024,
            width=1024,
            num_inference_steps=28,
            guidance_scale=7.0
        )
        
        # CLIP ë³´ìƒ ê³„ì‚°ê¸° ë¡œë“œ (GPU 3)
        clip_calculator = CLIPRewardCalculator()
        
        logger.info("âœ… All models loaded successfully on assigned GPUs")
        
        # GPU ìƒíƒœ ì¶œë ¥
        gpu_config.print_gpu_status()
        
        # 2. GRPO ì„¤ì •
        config = GRPOConfig(
            learning_rate=1e-5,
            group_size=2,  # ë” ì‘ì€ ë°°ì¹˜ (í° ì•¡ì…˜ ê³µê°„ ë•Œë¬¸ì—)
            num_iterations=50,  # ë” ë§ì€ iterationìœ¼ë¡œ ë‹¤ì–‘í•œ í”„ë¡¬í”„íŠ¸ í•™ìŠµ
            grpo_epochs=3,
            max_new_tokens=12,  # ë” ììœ ë¡œìš´ ìƒì„±ì„ ìœ„í•´ í† í° ìˆ˜ ì¦ê°€
            
            # GRPO í•˜ì´í¼íŒŒë¼ë¯¸í„° (í° ì•¡ì…˜ ê³µê°„ì— ë§ê²Œ ì¡°ì •)
            gamma=0.95,  # ì•½ê°„ ë‚®ì€ í• ì¸ íŒ©í„°
            grpo_kl_beta=0.05,  # ë†’ì€ KL í˜ë„í‹°ë¡œ ì•ˆì •ì„± í™•ë³´
            grpo_clip_epsilon=0.2,
            entropy_coeff=0.02  # ë†’ì€ ì—”íŠ¸ë¡œí”¼ë¡œ íƒí—˜ ì¥ë ¤
        )
        
        # 3. íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
        logger.info("ğŸ”§ Initializing GRPO trainer...")
        trainer = GRPOTrainer(qwen_model, sd3_generator, clip_calculator, config)
        
        # 4. í•™ìŠµ ë° ê²€ì¦ í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
        all_prompts = get_training_prompts()
        
        # í”„ë¡¬í”„íŠ¸ë¥¼ í•™ìŠµ/ê²€ì¦ìœ¼ë¡œ ë¶„í• 
        np.random.seed(42)  # ì¬í˜„ ê°€ëŠ¥ì„±
        indices = np.random.permutation(len(all_prompts))
        split_idx = int(0.8 * len(all_prompts))
        
        training_prompts = [all_prompts[i] for i in indices[:split_idx]]
        validation_prompts = [all_prompts[i] for i in indices[split_idx:]]
        
        logger.info(f"ğŸ“ Prepared {len(training_prompts)} training prompts")
        logger.info(f"ğŸ“ Prepared {len(validation_prompts)} validation prompts")
        
        # Challenging í”„ë¡¬í”„íŠ¸ ì¹´í…Œê³ ë¦¬ ë¡œê¹…
        challenging_keywords = ["purple", "green cat", "rainbow", "transparent", "square wheel", "upside down", "concept of"]
        challenging_count = sum(1 for prompt in training_prompts if any(keyword in prompt for keyword in challenging_keywords))
        logger.info(f"ğŸ¯ Challenging prompts in training set: {challenging_count}/{len(training_prompts)}")
        
        # 5. í•™ìŠµ ë£¨í”„
        logger.info("ğŸ¯ Starting GRPO training loop...")
        
        for iteration in range(config.num_iterations):
            logger.info(f"\n{'='*50}")
            logger.info(f"ğŸ”„ Iteration {iteration + 1}/{config.num_iterations}")
            logger.info(f"{'='*50}")
            
            # í˜„ì¬ iterationì˜ í”„ë¡¬í”„íŠ¸ ì„ íƒ
            np.random.seed(iteration)  # ì¬í˜„ ê°€ëŠ¥ì„±ì„ ìœ„í•œ ì‹œë“œ
            selected_prompts = np.random.choice(
                training_prompts, 
                size=config.group_size, 
                replace=False
            ).tolist()
            
            logger.info(f"ğŸ“‹ Selected prompts for this iteration:")
            for i, prompt in enumerate(selected_prompts):
                logger.info(f"  {i+1}. {prompt}")
            
            # í•™ìŠµ iteration ì‹¤í–‰
            try:
                results = trainer.train_iteration(selected_prompts)
                
                # ê²°ê³¼ ë¡œê¹…
                logger.info(f"ğŸ“Š Iteration {iteration + 1} Results:")
                logger.info(f"  Avg Reward: {results['avg_reward']:.4f}")
                logger.info(f"  Avg Length: {results['avg_length']:.1f}")
                logger.info(f"  Policy Obj: {results['policy_objective']:.6f}")
                logger.info(f"  KL Div: {results['kl_divergence']:.6f}")
                logger.info(f"  Entropy: {results['entropy']:.4f}")
                
                # GPU ë©”ëª¨ë¦¬ ì •ë¦¬ (ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€)
                if (iteration + 1) % 5 == 0:
                    gpu_config.clear_gpu_memory()
                    logger.info("ğŸ§¹ GPU memory cleared")
                
                # ì£¼ê¸°ì  ê²€ì¦ ë° í”Œë¡¯
                if (iteration + 1) % 10 == 0:
                    logger.info(f"ğŸ” Running validation at iteration {iteration + 1}...")
                    
                    # ê²€ì¦ ì‹¤í–‰ (ì‘ì€ ë°°ì¹˜ë¡œ)
                    val_prompts = np.random.choice(validation_prompts, size=min(3, len(validation_prompts)), replace=False).tolist()
                    val_results = trainer.train_iteration(val_prompts)
                    
                    logger.info(f"ğŸ“ˆ Validation Results:")
                    logger.info(f"  Val Reward: {val_results['avg_reward']:.4f}")
                    logger.info(f"  Val Length: {val_results['avg_length']:.1f}")
                    
                    # íŠ¹ë³„íˆ challenging í”„ë¡¬í”„íŠ¸ë“¤ í…ŒìŠ¤íŠ¸
                    challenging_prompts = [p for p in validation_prompts if any(keyword in p for keyword in challenging_keywords)]
                    if challenging_prompts:
                        test_prompt = np.random.choice(challenging_prompts)
                        logger.info(f"ğŸ¯ Testing challenging prompt: '{test_prompt}'")
                        challenge_results = trainer.train_iteration([test_prompt])
                        logger.info(f"  Challenge Reward: {challenge_results['avg_reward']:.4f}")
                    
                    plot_training_results(trainer, f"grpo_results_iter_{iteration+1}.png")
                
            except Exception as e:
                logger.error(f"âŒ Error in iteration {iteration + 1}: {e}")
                continue
        
        # 6. ìµœì¢… ê²°ê³¼ ì €ì¥
        logger.info("\nğŸ‰ Training completed successfully!")
        
        # ìµœì¢… í”Œë¡¯ ìƒì„±
        plot_training_results(trainer, "grpo_final_results.png")
        
        # í•™ìŠµëœ ëª¨ë¸ ì €ì¥
        try:
            qwen_model.save_model("./saved_models/qwen_grpo_trained")
            logger.info("ğŸ’¾ Trained model saved to ./saved_models/qwen_grpo_trained")
        except Exception as e:
            logger.warning(f"âš ï¸ Could not save model: {e}")
        
        # ìµœì¢… challenging í”„ë¡¬í”„íŠ¸ í…ŒìŠ¤íŠ¸
        logger.info(f"\nğŸ¯ Final Challenging Prompts Test:")
        challenging_test_prompts = [
            "a purple rabbit eating carrots",
            "a green cat with blue eyes", 
            "a square wheel rolling down a hill",
            "the concept of happiness as a creature",
            "a transparent glass butterfly"
        ]
        
        for i, prompt in enumerate(challenging_test_prompts):
            logger.info(f"\nğŸ§ª Testing Challenge {i+1}: '{prompt}'")
            try:
                challenge_result = trainer.train_iteration([prompt])
                logger.info(f"  Final Challenge Reward: {challenge_result['avg_reward']:.4f}")
                logger.info(f"  Generated Length: {challenge_result['avg_length']:.1f} tokens")
            except Exception as e:
                logger.warning(f"  Challenge test failed: {e}")
        
        # ìµœì¢… í†µê³„ ì¶œë ¥
        if trainer.iteration_rewards:
            logger.info(f"\nğŸ“ˆ Final Training Statistics:")
            logger.info(f"  Final Reward: {trainer.iteration_rewards[-1]:.4f}")
            logger.info(f"  Best Reward: {max(trainer.iteration_rewards):.4f}")
            logger.info(f"  Reward Improvement: {trainer.iteration_rewards[-1] - trainer.iteration_rewards[0]:.4f}")
            logger.info(f"  Training Stability (Reward Std): {np.std(trainer.iteration_rewards):.4f}")
            
            # Challenging í”„ë¡¬í”„íŠ¸ ì„±ëŠ¥ ë¶„ì„
            recent_rewards = trainer.iteration_rewards[-10:]  # ìµœê·¼ 10ê°œ
            logger.info(f"  Recent Performance (last 10 iter): {np.mean(recent_rewards):.4f} Â± {np.std(recent_rewards):.4f}")
            
            # ê°œì„  ì—¬ë¶€ íŒë‹¨
            if len(trainer.iteration_rewards) >= 20:
                early_rewards = trainer.iteration_rewards[:10]
                late_rewards = trainer.iteration_rewards[-10:]
                improvement = np.mean(late_rewards) - np.mean(early_rewards)
                logger.info(f"  Overall Improvement: {improvement:.4f} {'âœ…' if improvement > 0 else 'âŒ'}")
            
            logger.info(f"\nğŸ‰ Training completed! The model should now generate more creative and reward-optimized prompts.")
        
    except KeyboardInterrupt:
        logger.info("\nâ¹ï¸ Training interrupted by user")
        if 'trainer' in locals():
            plot_training_results(trainer, "grpo_interrupted_results.png")
    
    except Exception as e:
        logger.error(f"âŒ Training failed with error: {e}")
        raise

if __name__ == "__main__":
    main() 