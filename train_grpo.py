"""
GRPO Training Script for QWEN Model
===================================

EasyR1 ê¸°ë°˜ì˜ ìƒˆë¡œìš´ GRPO íŠ¸ë ˆì´ë„ˆë¥¼ ì‚¬ìš©í•œ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸

Author: AI Assistant
Date: 2025-01-22
"""

import os
import sys
import logging
import argparse
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
from typing import List, Dict, Any
import json

# Multi-GPU ì„¤ì • import
from gpu_config import setup_multi_gpu

# ëª¨ë¸ imports
from models.qwen_wrapper import QwenWrapper
from models.sd3_generator import SD3Generator
from models.clip_reward import CLIPRewardCalculator

# í•™ìŠµ ê´€ë ¨ imports
from training.grpo_trainer import GRPOTrainer, GRPOConfig

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('grpo_training.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ë„ì „ì ì¸ í”„ë¡¬í”„íŠ¸ ë°ì´í„°ì…‹
CHALLENGING_PROMPTS = [
    # ê¸°ë³¸ ë™ë¬¼/ê°ì²´
    "a cat sitting on a chair",
    "a beautiful sunset over mountains",
    "a robot playing guitar",
    "a flower garden in spring",
    "an old castle on a hill",
    
    # SD3 ì–´ë ¤ìš´ ìƒ‰ìƒ ì¡°í•©
    "a purple rabbit sitting in grass",
    "a green cat with yellow eyes",
    "a blue elephant in the desert",
    "a red bird with black wings",
    "a yellow dog with pink spots",
    
    # ëª¨ìˆœì ì¸ ê°œë…ë“¤
    "a square wheel rolling down a hill",
    "an upside down tree growing in the sky",
    "a transparent fish swimming in air",
    "a silent thunderstorm with visible sound waves",
    "a car with legs instead of wheels",
    
    # ì¶”ìƒì  ê°œë…ë“¤
    "the concept of happiness visualized as colors",
    "time flowing backwards in a clock",
    "music made visible as geometric shapes",
    "the feeling of nostalgia as a landscape",
    "dreams transforming into reality",
    
    # ë³µì¡í•œ ì¬ì§ˆ/í…ìŠ¤ì²˜
    "a glass sculpture of a dragon",
    "a metallic chrome rose on black velvet",
    "a wooden elephant with crystal eyes",
    "a paper airplane made of liquid mercury",
    "a stone butterfly with feather wings",
    
    # í™˜ìƒì /ì´ˆí˜„ì‹¤ì 
    "a floating island with waterfalls going upward",
    "a library where books fly like birds",
    "a mirror that shows different seasons",
    "a doorway leading to another dimension",
    "a phoenix made of pure light",
    
    # ê³ ê¸‰ ì¡°ëª…/ë¶„ìœ„ê¸°
    "a portrait lit by candlelight",
    "neon lights reflecting on wet streets",
    "sunbeams through stained glass windows",
    "aurora borealis over a frozen lake",
    "a lighthouse beam cutting through fog"
]

def create_train_val_split(prompts: List[str], train_ratio: float = 0.8) -> tuple:
    """í”„ë¡¬í”„íŠ¸ë¥¼ train/validationìœ¼ë¡œ ë¶„í• """
    np.random.shuffle(prompts)
    split_idx = int(len(prompts) * train_ratio)
    return prompts[:split_idx], prompts[split_idx:]

def plot_training_progress(trainer: GRPOTrainer, save_path: str = "training_progress.png"):
    """í•™ìŠµ ì§„í–‰ ìƒí™©ì„ í”Œë¡¯"""
    try:
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('GRPO Training Progress (EasyR1)', fontsize=16, fontweight='bold')
        
        iterations = range(1, len(trainer.iteration_rewards) + 1)
        
        # ë³´ìƒ ê·¸ë˜í”„
        axes[0, 0].plot(iterations, trainer.iteration_rewards, 'b-', linewidth=2, marker='o')
        axes[0, 0].set_title('Average Reward per Iteration')
        axes[0, 0].set_xlabel('Iteration')
        axes[0, 0].set_ylabel('Reward')
        axes[0, 0].grid(True, alpha=0.3)
        
        # Policy Loss ê·¸ë˜í”„
        if trainer.iteration_policy_losses:
            axes[0, 1].plot(iterations, trainer.iteration_policy_losses, 'r-', linewidth=2, marker='s')
            axes[0, 1].set_title('Policy Loss per Iteration')
            axes[0, 1].set_xlabel('Iteration')
            axes[0, 1].set_ylabel('Loss')
            axes[0, 1].grid(True, alpha=0.3)
        
        # KL Divergence ê·¸ë˜í”„
        if trainer.iteration_kl_divs:
            axes[1, 0].plot(iterations, trainer.iteration_kl_divs, 'g-', linewidth=2, marker='^')
            axes[1, 0].set_title('KL Divergence per Iteration')
            axes[1, 0].set_xlabel('Iteration')
            axes[1, 0].set_ylabel('KL Divergence')
            axes[1, 0].grid(True, alpha=0.3)
        
        # ë³´ìƒ íˆìŠ¤í† ê·¸ë¨ (ìµœê·¼ 10ê°œ iteration)
        if len(trainer.iteration_rewards) >= 10:
            recent_rewards = trainer.iteration_rewards[-10:]
            axes[1, 1].hist(recent_rewards, bins=min(10, len(recent_rewards)), alpha=0.7, color='purple')
            axes[1, 1].set_title('Recent Reward Distribution (Last 10 Iterations)')
            axes[1, 1].set_xlabel('Reward')
            axes[1, 1].set_ylabel('Frequency')
            axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"ğŸ“Š Training progress plot saved to {save_path}")
        
    except Exception as e:
        logger.error(f"âŒ Failed to create training plot: {e}")

def validate_model(trainer: GRPOTrainer, val_prompts: List[str]) -> Dict[str, float]:
    """ê²€ì¦ ì„¸íŠ¸ë¡œ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€"""
    logger.info(f"ğŸ” Validating on {len(val_prompts)} prompts...")
    
    # ê²€ì¦ trajectory ìˆ˜ì§‘
    val_data = trainer.collect_group_trajectories(val_prompts)
    
    val_rewards = val_data.get('episode_rewards', [])
    val_lengths = val_data.get('episode_lengths', [])
    
    if not val_rewards:
        return {'val_avg_reward': 0.0, 'val_avg_length': 0.0}
    
    return {
        'val_avg_reward': np.mean(val_rewards),
        'val_avg_length': np.mean(val_lengths),
        'val_std_reward': np.std(val_rewards),
        'val_min_reward': np.min(val_rewards),
        'val_max_reward': np.max(val_rewards)
    }

def save_model_checkpoint(trainer: GRPOTrainer, iteration: int, save_dir: str = "checkpoints"):
    """ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
    try:
        os.makedirs(save_dir, exist_ok=True)
        
        checkpoint_path = os.path.join(save_dir, f"grpo_model_iter_{iteration:03d}.pt")
        
        import torch
        torch.save({
            'iteration': iteration,
            'action_head_state_dict': trainer.action_head.state_dict(),
            'optimizer_state_dict': trainer.optimizer.state_dict(),
            'kl_coef': trainer.kl_controller.kl_coef,
            'config': trainer.config,
            'iteration_rewards': trainer.iteration_rewards,
            'iteration_policy_losses': trainer.iteration_policy_losses,
            'iteration_kl_divs': trainer.iteration_kl_divs
        }, checkpoint_path)
        
        logger.info(f"ğŸ’¾ Model checkpoint saved to {checkpoint_path}")
        
    except Exception as e:
        logger.error(f"âŒ Failed to save checkpoint: {e}")

def main():
    parser = argparse.ArgumentParser(description='GRPO Training for QWEN Model (EasyR1)')
    parser.add_argument('--num_iterations', type=int, default=50, help='Number of training iterations')
    parser.add_argument('--group_size', type=int, default=4, help='Group size for GRPO')
    parser.add_argument('--learning_rate', type=float, default=1e-6, help='Learning rate')
    parser.add_argument('--kl_type', type=str, default='adaptive', choices=['adaptive', 'fixed'], help='KL controller type')
    parser.add_argument('--max_new_tokens', type=int, default=10, help='Maximum new tokens to generate')
    parser.add_argument('--save_training_data', action='store_true', help='Save training data')
    parser.add_argument('--checkpoint_interval', type=int, default=10, help='Checkpoint saving interval')
    parser.add_argument('--validation_interval', type=int, default=5, help='Validation interval')
    
    args = parser.parse_args()
    
    logger.info("ğŸš€ Starting GRPO Training (EasyR1 Implementation)")
    logger.info(f"Configuration: {vars(args)}")
    
    try:
        # 1. Multi-GPU í™˜ê²½ ì„¤ì •
        logger.info("ğŸ”§ Setting up multi-GPU environment...")
        setup_multi_gpu()
        
        # 2. ëª¨ë¸ ì´ˆê¸°í™”
        logger.info("ğŸ¤– Initializing models...")
        qwen_model = QwenWrapper()
        sd3_generator = SD3Generator()
        clip_calculator = CLIPRewardCalculator()
        
        # 3. GRPO ì„¤ì •
        config = GRPOConfig(
            learning_rate=args.learning_rate,
            group_size=args.group_size,
            num_iterations=args.num_iterations,
            max_new_tokens=args.max_new_tokens,
            kl_type=args.kl_type,
            save_training_data=args.save_training_data,
            device="cuda"
        )
        
        logger.info(f"ğŸ“‹ GRPO Config: {config}")
        
        # 4. íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
        logger.info("ğŸ¯ Initializing GRPO trainer...")
        trainer = GRPOTrainer(qwen_model, sd3_generator, clip_calculator, config)
        
        # 5. ë°ì´í„°ì…‹ ë¶„í• 
        logger.info("ğŸ“Š Preparing training data...")
        train_prompts, val_prompts = create_train_val_split(CHALLENGING_PROMPTS, train_ratio=0.8)
        
        logger.info(f"ğŸ“ˆ Training prompts: {len(train_prompts)}")
        logger.info(f"ğŸ“‰ Validation prompts: {len(val_prompts)}")
        
        # 6. í•™ìŠµ ë£¨í”„
        logger.info("ğŸ”„ Starting training loop...")
        best_val_reward = -float('inf')
        
        for iteration in range(1, args.num_iterations + 1):
            logger.info(f"\n{'='*60}")
            logger.info(f"ğŸ¯ Iteration {iteration}/{args.num_iterations}")
            logger.info(f"{'='*60}")
            
            # í˜„ì¬ iteration ì„¤ì •
            trainer.current_iteration = iteration
            
            # í•™ìŠµ í”„ë¡¬í”„íŠ¸ ìƒ˜í”Œë§
            np.random.shuffle(train_prompts)
            current_prompts = train_prompts[:config.group_size]
            
            # í•™ìŠµ ìˆ˜í–‰
            try:
                results = trainer.train_iteration(current_prompts)
                
                if results:
                    logger.info(f"ğŸ“Š Training Results:")
                    for key, value in results.items():
                        logger.info(f"  {key}: {value:.4f}")
                
            except Exception as e:
                logger.error(f"âŒ Training failed at iteration {iteration}: {e}")
                continue
            
            # ê²€ì¦ ìˆ˜í–‰
            if iteration % args.validation_interval == 0:
                val_results = validate_model(trainer, val_prompts)
                logger.info(f"ğŸ” Validation Results:")
                for key, value in val_results.items():
                    logger.info(f"  {key}: {value:.4f}")
                
                # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
                current_val_reward = val_results.get('val_avg_reward', 0.0)
                if current_val_reward > best_val_reward:
                    best_val_reward = current_val_reward
                    save_model_checkpoint(trainer, iteration, "best_models")
                    logger.info(f"ğŸ† New best validation reward: {best_val_reward:.4f}")
            
            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            if iteration % args.checkpoint_interval == 0:
                save_model_checkpoint(trainer, iteration)
            
            # ì§„í–‰ ìƒí™© í”Œë¡¯
            if iteration % 5 == 0:
                plot_training_progress(trainer, f"training_progress_iter_{iteration:03d}.png")
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if iteration % 10 == 0:
                try:
                    import torch
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        logger.info("ğŸ§¹ GPU memory cleared")
                except:
                    pass
        
        # ìµœì¢… ê²°ê³¼
        logger.info(f"\n{'='*60}")
        logger.info("ğŸ‰ Training completed!")
        logger.info(f"{'='*60}")
        
        if trainer.iteration_rewards:
            logger.info(f"ğŸ“Š Final Results:")
            logger.info(f"  Best Reward: {max(trainer.iteration_rewards):.4f}")
            logger.info(f"  Final Reward: {trainer.iteration_rewards[-1]:.4f}")
            logger.info(f"  Average Reward: {np.mean(trainer.iteration_rewards):.4f}")
        
        # ìµœì¢… í”Œë¡¯ ì €ì¥
        plot_training_progress(trainer, "final_training_progress.png")
        
        # ìµœì¢… ëª¨ë¸ ì €ì¥
        save_model_checkpoint(trainer, args.num_iterations, "final_models")
        
        logger.info("âœ… All training tasks completed successfully!")
        
    except KeyboardInterrupt:
        logger.info("â¹ï¸ Training interrupted by user")
    except Exception as e:
        logger.error(f"âŒ Fatal error: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main() 