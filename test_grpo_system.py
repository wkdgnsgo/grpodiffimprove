#!/usr/bin/env python3
"""
GRPO System Integration Test
===========================

ì „ë°˜ì ì¸ parameter ì—ëŸ¬ê°€ ì—†ê²Œ ì ê²€í•˜ê³  ref ëª¨ë¸ë„ í˜„ì¬ policyëª¨ë¸ê³¼ ì—°ë™ì´ ì˜ ë˜ëŠ”ì§€ í™•ì¸í•˜ëŠ” í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‹¤ìŒì„ ê²€ì¦í•©ë‹ˆë‹¤:
1. ëª¨ë“  ì»´í¬ë„ŒíŠ¸ì˜ ì˜¬ë°”ë¥¸ ì´ˆê¸°í™”
2. íŒŒë¼ë¯¸í„° ì—°ë™ ë° í˜¸í™˜ì„±
3. ì°¸ì¡° ëª¨ë¸(ref model)ê³¼ ì •ì±… ëª¨ë¸ì˜ ì—°ë™
4. GRPO í•™ìŠµ ë£¨í”„ì˜ ì •ìƒ ì‘ë™
5. ì œê³µëœ ì½”ë“œ í˜•ì‹ê³¼ì˜ í˜¸í™˜ì„±

Author: AI Assistant
Date: 2025-01-22
"""

import sys
import logging
from pathlib import Path

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent))

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def test_imports():
    """ëª¨ë“  í•„ìˆ˜ ëª¨ë“ˆ ì„í¬íŠ¸ í…ŒìŠ¤íŠ¸"""
    print("ğŸ”§ Testing Module Imports...")
    
    try:
        from training.grpo_trainer import GRPOTrainer, GRPOConfig
        from models.vlm_wrapper import VLMWrapper
        from models.sd_generator import SD3Generator
        from models.clip_reward import CLIPRewardCalculator
        from integration.main_trainer import VLMGRPOSystem
        print("âœ… All core modules imported successfully")
        return True
    except Exception as e:
        print(f"âŒ Import failed: {e}")
        return False

def test_grpo_config():
    """GRPO ì„¤ì • í…ŒìŠ¤íŠ¸"""
    print("\nğŸ“Š Testing GRPO Configuration...")
    
    try:
        from training.grpo_trainer import GRPOConfig
        
        # --- GRPO Configuration (ì œê³µëœ ì½”ë“œ í˜•ì‹ ìŠ¤íƒ€ì¼) ---
        config = GRPOConfig(
            learning_rate=5e-6,
            group_size=4,
            num_iterations=100,
            grpo_epochs=3,
            gamma=0.99,
            kl_beta=0.02,
            clip_epsilon=0.2,
            entropy_coeff=0.01,
            max_grad_norm=1.0,
            max_new_tokens=25,
            temperature=0.8,
            device='cpu'
        )
        
        print(f"âœ… GRPO Config created successfully:")
        print(f"  - Learning Rate: {config.learning_rate}")
        print(f"  - Group Size: {config.group_size}")
        print(f"  - Iterations: {config.num_iterations}")
        print(f"  - GRPO Epochs: {config.grpo_epochs}")
        print(f"  - Gamma: {config.gamma}")
        print(f"  - KL Beta: {config.kl_beta}")
        print(f"  - Device: {config.device}")
        
        return config
    except Exception as e:
        print(f"âŒ GRPO Config test failed: {e}")
        return None

def create_mock_components():
    """Mock ì»´í¬ë„ŒíŠ¸ ìƒì„± (ì œê³µëœ ì½”ë“œ í˜•ì‹ê³¼ ìœ ì‚¬)"""
    print("\nğŸ”§ Creating Mock Components...")
    
    import torch
    import torch.nn as nn
    from torch.distributions import Categorical
    import numpy as np
    
    class MockVLM:
        """Mock VLM Policy Network"""
        def __init__(self):
            # --- Policy Network Initialization ---
            # Embedding layer to handle token IDs properly
            self.embedding = nn.Embedding(50000, 128)
            self.model = nn.Sequential(
                nn.Linear(128, 512),
                nn.ReLU(),
                nn.Linear(512, 256),
                nn.ReLU(),
                nn.Linear(256, 50000)
            )
            self.tokenizer = type('MockTokenizer', (), {
                '__len__': lambda self: 50000,
                'eos_token_id': 2
            })()
            self.model_name = 'mock-qwen2.5-vl'
            
        def forward(self, input_ids, attention_mask=None):
            """Policy Network Forward Pass"""
            # Properly handle token IDs through embedding
            embedded = self.embedding(input_ids)  # [batch_size, seq_len, embed_dim]
            # Use mean pooling to get fixed-size representation
            pooled = embedded.mean(dim=1)  # [batch_size, embed_dim]
            logits = self.model(pooled)  # [batch_size, vocab_size]
            return Categorical(logits=logits)
            
        def generate_sequence(self, prompt, max_new_tokens=5):
            """Token-by-token Generation for GRPO"""
            states = []
            actions = []
            log_probs = []
            
            # Consistent tensor shapes
            seq_len = 10
            
            for i in range(max_new_tokens):
                # State (input sequence) - ensure proper dtype
                state = torch.randint(0, 1000, (seq_len,), dtype=torch.long)
                states.append(state)
                
                # Action (next token)
                action = torch.randint(0, 1000, (), dtype=torch.long)
                actions.append(action)
                
                # Log probability
                log_prob = torch.randn((), dtype=torch.float32) * 0.1
                log_probs.append(log_prob)
                
                # Early stopping
                if i > 1 and np.random.random() < 0.3:
                    break
            
            return {
                'generated_text': f'enhanced {prompt} with artistic creative details',
                'generated_ids': torch.randint(0, 1000, (1, len(states))),
                'states': states,
                'actions': actions,
                'log_probs': log_probs
            }
            
        def get_log_prob(self, input_ids, target_token, attention_mask=None):
            """Calculate log probability for specific token"""
            policy_dist = self.forward(input_ids, attention_mask)
            return policy_dist.log_prob(target_token)
    
    class MockSDGenerator:
        """Mock SD3 Generator (Frozen)"""
        def __init__(self):
            self.model_name = 'mock-sd3'
        
        def generate_image(self, prompt):
            return f'high_quality_image_for_{prompt[:40]}'
    
    class MockCLIPReward:
        """Mock CLIP Reward Calculator (Frozen)"""
        def __init__(self):
            self.model_name = 'mock-clip'
        
        def calculate_reward(self, image, text):
            # Realistic reward calculation
            base_reward = 0.5
            text_quality = min(len(text.split()) * 0.015, 0.25)
            creativity_bonus = 0.1 if any(word in text.lower() for word in ['creative', 'artistic', 'beautiful']) else 0.0
            noise = np.random.normal(0, 0.08)
            return np.clip(base_reward + text_quality + creativity_bonus + noise, 0.0, 1.0)
    
    # --- Component Initialization (ì œê³µëœ ì½”ë“œ í˜•ì‹) ---
    vlm_policy = MockVLM()
    sd_generator = MockSDGenerator()
    clip_reward = MockCLIPReward()
    
    print(f"âœ… Mock components created:")
    print(f"  - VLM Policy: {vlm_policy.model_name}")
    print(f"  - SD Generator: {sd_generator.model_name}")
    print(f"  - CLIP Reward: {clip_reward.model_name}")
    
    return vlm_policy, sd_generator, clip_reward

def test_grpo_trainer_initialization(vlm_policy, sd_generator, clip_reward, config):
    """GRPO Trainer ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸"""
    print("\nğŸ¯ Testing GRPO Trainer Initialization...")
    
    try:
        from training.grpo_trainer import GRPOTrainer
        
        # --- GRPO Trainer Initialization (ì œê³µëœ ì½”ë“œ í˜•ì‹) ---
        grpo_trainer = GRPOTrainer(
            vlm_model=vlm_policy,
            sd_generator=sd_generator,
            clip_reward=clip_reward,
            config=config
        )
        
        print("âœ… GRPO Trainer initialized successfully")
        
        # --- Parameter Verification ---
        vlm_params = sum(p.numel() for p in grpo_trainer.vlm_policy.model.parameters())
        print(f"ğŸ“Š VLM Policy parameters: {vlm_params:,}")
        
        # --- Reference Policy Verification ---
        if grpo_trainer.vlm_policy_ref is not None:
            ref_params = sum(p.numel() for p in grpo_trainer.vlm_policy_ref.parameters())
            print(f"ğŸ“‹ Reference policy parameters: {ref_params:,}")
            
            # Check if ref model is frozen
            ref_requires_grad = any(p.requires_grad for p in grpo_trainer.vlm_policy_ref.parameters())
            if not ref_requires_grad:
                print("âœ… Reference policy properly frozen")
            else:
                print("âš ï¸ Reference policy not frozen")
        else:
            print("âŒ Reference policy not created")
            return None
        
        # --- Optimizer Verification ---
        if grpo_trainer.vlm_optimizer is not None:
            lr = grpo_trainer.vlm_optimizer.param_groups[0]['lr']
            print(f"ğŸ”§ Optimizer learning rate: {lr}")
            print("âœ… VLM optimizer created")
        else:
            print("âŒ VLM optimizer not created")
            return None
        
        return grpo_trainer
        
    except Exception as e:
        print(f"âŒ GRPO Trainer initialization failed: {e}")
        import traceback
        traceback.print_exc()
        return None

def test_training_iteration(grpo_trainer):
    """í•™ìŠµ ë°˜ë³µ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ”„ Testing Training Iteration...")
    
    try:
        # --- Training Prompts ---
        train_prompts = [
            "a breathtaking mountain landscape at golden hour",
            "a curious kitten exploring a colorful garden",
            "abstract art with vibrant flowing colors",
            "a peaceful forest scene with morning mist"
        ]
        
        print(f"ğŸ“ Training with {len(train_prompts)} prompts")
        
        # --- Single Training Iteration ---
        metrics = grpo_trainer.train_iteration(train_prompts)
        
        print("âœ… Training iteration completed successfully")
        print(f"ğŸ“Š Iteration Metrics:")
        print(f"  - Average Reward: {metrics['avg_reward']:.4f}")
        print(f"  - Policy Loss: {metrics['policy_loss']:.4f}")
        print(f"  - Entropy: {metrics['entropy']:.4f}")
        print(f"  - KL Divergence: {metrics['kl_div']:.4f}")
        
        # --- Training Statistics Verification ---
        stats = grpo_trainer.get_training_stats()
        print(f"ğŸ“ˆ Training Statistics:")
        print(f"  - Current Iteration: {stats['iteration']}")
        print(f"  - Average Reward: {stats['avg_reward']:.4f}")
        
        # --- History Verification ---
        print(f"ğŸ“‹ History Lengths:")
        print(f"  - Rewards: {len(grpo_trainer.iteration_rewards)}")
        print(f"  - Policy Losses: {len(grpo_trainer.iteration_policy_losses)}")
        print(f"  - Entropies: {len(grpo_trainer.iteration_entropies)}")
        print(f"  - KL Divergences: {len(grpo_trainer.iteration_kl_divs)}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Training iteration failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_full_training_loop(grpo_trainer):
    """ì „ì²´ í•™ìŠµ ë£¨í”„ í…ŒìŠ¤íŠ¸ (ì œê³µëœ ì½”ë“œ í˜•ì‹)"""
    print("\nğŸš€ Testing Full Training Loop...")
    
    try:
        # --- Training Configuration ---
        NUM_ITERATIONS_GRPO = 3
        GROUP_SIZE = 2
        INTERV_PRINT = 1
        
        # --- Training Prompts ---
        train_prompts = [
            "a serene lake reflecting autumn colors",
            "a majestic eagle soaring through clouds",
            "vibrant street art on urban walls",
            "a cozy cabin in a snowy forest"
        ]
        
        print(f"ğŸ”„ Starting {NUM_ITERATIONS_GRPO} iterations with group size {GROUP_SIZE}")
        
        # --- GRPO Training Loop (ì œê³µëœ ì½”ë“œ í˜•ì‹ê³¼ ìœ ì‚¬) ---
        for iteration in range(NUM_ITERATIONS_GRPO):
            print(f"\n--- Iteration {iteration+1}/{NUM_ITERATIONS_GRPO} ---")
            
            # --- Sample Group Prompts ---
            import random
            if len(train_prompts) >= GROUP_SIZE:
                group_prompts = random.sample(train_prompts, GROUP_SIZE)
            else:
                group_prompts = (train_prompts * ((GROUP_SIZE // len(train_prompts)) + 1))[:GROUP_SIZE]
            
            # --- Perform GRPO Training Iteration ---
            metrics = grpo_trainer.train_iteration(group_prompts)
            
            # --- Logging ---
            if (iteration + 1) % INTERV_PRINT == 0 or iteration == NUM_ITERATIONS_GRPO - 1:
                print(f"ğŸ“Š Iteration {iteration+1}/{NUM_ITERATIONS_GRPO} Summary:")
                print(f"  Avg Reward (Group): {metrics['avg_reward']:.4f}")
                print(f"  Policy Loss: {metrics['policy_loss']:.4f}")
                print(f"  Entropy: {metrics['entropy']:.4f}")
                print(f"  KL Divergence: {metrics['kl_div']:.4f}")
        
        print(f"\nğŸ‰ GRPO Training Loop Finished!")
        
        # --- Final Statistics ---
        final_stats = grpo_trainer.get_training_stats()
        print(f"ğŸ“Š Final Statistics:")
        print(f"  Total Iterations: {final_stats['iteration']}")
        print(f"  Final Average Reward: {final_stats['avg_reward']:.4f}")
        print(f"  Reward Progression: {[f'{r:.3f}' for r in grpo_trainer.iteration_rewards]}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Full training loop failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸ§ª GRPO System Integration Test")
    print("=" * 50)
    print("ì „ë°˜ì ì¸ parameter ì—ëŸ¬ê°€ ì—†ê²Œ ì ê²€í•˜ê³  ref ëª¨ë¸ë„ í˜„ì¬ policyëª¨ë¸ê³¼ ì—°ë™ì´ ì˜ ë˜ëŠ”ì§€ í™•ì¸")
    print("=" * 50)
    
    # 1. ì„í¬íŠ¸ í…ŒìŠ¤íŠ¸
    if not test_imports():
        return False
    
    # 2. GRPO ì„¤ì • í…ŒìŠ¤íŠ¸
    config = test_grpo_config()
    if config is None:
        return False
    
    # 3. Mock ì»´í¬ë„ŒíŠ¸ ìƒì„±
    vlm_policy, sd_generator, clip_reward = create_mock_components()
    
    # 4. GRPO Trainer ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
    grpo_trainer = test_grpo_trainer_initialization(vlm_policy, sd_generator, clip_reward, config)
    if grpo_trainer is None:
        return False
    
    # 5. ë‹¨ì¼ í•™ìŠµ ë°˜ë³µ í…ŒìŠ¤íŠ¸
    if not test_training_iteration(grpo_trainer):
        return False
    
    # 6. ì „ì²´ í•™ìŠµ ë£¨í”„ í…ŒìŠ¤íŠ¸
    if not test_full_training_loop(grpo_trainer):
        return False
    
    print("\n" + "=" * 50)
    print("ğŸ‰ All Tests Passed Successfully!")
    print("âœ… Parameter errors resolved")
    print("âœ… Reference model properly integrated")
    print("âœ… GRPO trainer working correctly")
    print("âœ… System ready for real training")
    print("=" * 50)
    
    return True

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 