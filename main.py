#!/usr/bin/env python3
"""
ìˆœìˆ˜ GRPO VLM í•™ìŠµ ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸
GPU í™˜ê²½ì—ì„œ ì‹¤ì œ QWEN VL, Stable Diffusion 3, CLIP ëª¨ë¸ì„ ì‚¬ìš©í•œ í•™ìŠµ
"""

import os
import sys
import logging
import torch
from pathlib import Path

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python pathì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent))

from trainer_grpo_pure import PureGRPOConfig, PureGRPOTrainer
from qwen import QWENModel
from clip_reward import CLIPReward

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('grpo_training.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def load_stable_diffusion_pipeline(device="cuda:1"):
    """Stable Diffusion 3 íŒŒì´í”„ë¼ì¸ ë¡œë“œ (GPU 1ë²ˆ)"""
    try:
        from diffusers import StableDiffusion3Pipeline
        import torch
        
        logger.info(f"ğŸ¨ Stable Diffusion 3 íŒŒì´í”„ë¼ì¸ ë¡œë”©... (Device: {device})")
        
        # GPU ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•˜ì§€ ì•Šì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ì„¤ì •
        pipe = StableDiffusion3Pipeline.from_pretrained(
            "stabilityai/stable-diffusion-3-medium-diffusers",
            torch_dtype=torch.float16,
            use_safetensors=True
        )
        
        # ì§€ì •ëœ GPUë¡œ ì´ë™
        if torch.cuda.is_available():
            pipe = pipe.to(device)
            logger.info(f"âœ… SD3 íŒŒì´í”„ë¼ì¸ì„ {device}ë¡œ ì´ë™")
        else:
            logger.warning("âš ï¸ CUDA ì‚¬ìš© ë¶ˆê°€, CPU ì‚¬ìš©")
        
        # ë©”ëª¨ë¦¬ ìµœì í™”
        pipe.enable_model_cpu_offload()
        pipe.enable_attention_slicing()
        
        # í”„ë¡œê·¸ë ˆìŠ¤ ë°” ë¹„í™œì„±í™”
        pipe.set_progress_bar_config(disable=True)
        
        logger.info("âœ… Stable Diffusion 3 íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì™„ë£Œ")
        return pipe
        
    except Exception as e:
        logger.error(f"âŒ SD3 íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise

def get_training_prompts():
    """í•™ìŠµìš© í”„ë¡¬í”„íŠ¸ ë°ì´í„°ì…‹"""
    return [
        # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸
        "a beautiful cat sitting on a chair",
        "sunset over mountains with golden light",
        "abstract art painting with vibrant colors",
        "portrait of a woman with flowing hair",
        "futuristic city skyline at night",
        
        # ë„ì „ì ì¸ í”„ë¡¬í”„íŠ¸ (SD3ê°€ ì–´ë ¤ì›Œí•˜ëŠ” ê²ƒë“¤)
        "red apple on blue table with green background",
        "transparent glass sphere floating in purple space",
        "wooden texture mixed with metallic surface",
        "fire and ice elements combined in one scene",
        "microscopic view of crystal structure",
        
        # ë³µì¡í•œ ì¥ë©´
        "crowded marketplace with many people and colorful stalls",
        "underwater scene with coral reef and tropical fish",
        "ancient temple ruins covered with jungle vegetation",
        "steampunk mechanical device with gears and pipes",
        "surreal landscape with floating islands and waterfalls"
    ]

def main():
    """ë©”ì¸ í•™ìŠµ í•¨ìˆ˜"""
    logger.info("ğŸš€ ìˆœìˆ˜ GRPO VLM í•™ìŠµ ì‹œì‘")
    logger.info("=" * 80)
    
    # GPU í™•ì¸ ë° ë°°ì¹˜ ê³„íš
    if torch.cuda.is_available():
        logger.info(f"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥ - GPU ê°œìˆ˜: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            logger.info(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
        
        logger.info("\nğŸ¯ GPU ë°°ì¹˜ ê³„íš:")
        logger.info("  GPU 0: QWEN VL ëª¨ë¸ (í”„ë¡¬í”„íŠ¸ í–¥ìƒ)")
        logger.info("  GPU 1: Stable Diffusion 3 (ì´ë¯¸ì§€ ìƒì„±)")
        logger.info("  GPU 2: CLIP ë¦¬ì›Œë“œ ëª¨ë¸ (ë¦¬ì›Œë“œ ê³„ì‚°)")
    else:
        logger.warning("âš ï¸ CUDA ì‚¬ìš© ë¶ˆê°€ - CPUë¡œ ì‹¤í–‰")
    
    # ì„¤ì •
    config = PureGRPOConfig(
        learning_rate=1e-6,
        batch_size=4,
        num_rollouts=5,
        max_prompt_length=77,
        max_new_tokens=20,
        temperature=1.2,
        top_p=0.9,
        top_k=100,
        kl_coef=0.02,
        clip_ratio=0.2,
        entropy_coef=0.01
    )
    
    logger.info("ğŸ“‹ í•™ìŠµ ì„¤ì •:")
    logger.info(f"  - í•™ìŠµë¥ : {config.learning_rate}")
    logger.info(f"  - ë°°ì¹˜ í¬ê¸°: {config.batch_size}")
    logger.info(f"  - ë¡¤ì•„ì›ƒ ìˆ˜: {config.num_rollouts}")
    logger.info(f"  - ìµœëŒ€ í† í°: {config.max_new_tokens}")
    logger.info(f"  - ì˜¨ë„: {config.temperature}")
    logger.info(f"  - KL ê³„ìˆ˜: {config.kl_coef}")
    
    try:
        # 1. QWEN VL ëª¨ë¸ ë¡œë“œ (GPU 0ë²ˆ)
        logger.info("\nğŸ§  QWEN VL ëª¨ë¸ ë¡œë”©...")
        qwen_model = QWENModel(device="cuda:0")
        logger.info("âœ… QWEN VL ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (GPU 0)")
        
        # 2. CLIP ë¦¬ì›Œë“œ ëª¨ë¸ ë¡œë“œ (GPU 2ë²ˆ)
        logger.info("\nğŸ¯ CLIP ë¦¬ì›Œë“œ ëª¨ë¸ ë¡œë”©...")
        reward_model = CLIPReward(device="cuda:2")
        logger.info("âœ… CLIP ë¦¬ì›Œë“œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (GPU 2)")
        
        # 3. Stable Diffusion 3 íŒŒì´í”„ë¼ì¸ ë¡œë“œ (GPU 1ë²ˆ)
        logger.info("\nğŸ¨ Stable Diffusion 3 íŒŒì´í”„ë¼ì¸ ë¡œë”©...")
        sd_pipeline = load_stable_diffusion_pipeline(device="cuda:1")
        logger.info("âœ… SD3 íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì™„ë£Œ (GPU 1)")
        
        # 4. ìˆœìˆ˜ GRPO íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
        logger.info("\nğŸ¯ ìˆœìˆ˜ GRPO íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”...")
        trainer = PureGRPOTrainer(qwen_model, reward_model, sd_pipeline, config)
        logger.info("âœ… íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # 5. í•™ìŠµ ë°ì´í„° ì¤€ë¹„
        train_prompts = get_training_prompts()
        logger.info(f"\nğŸ“ í•™ìŠµ í”„ë¡¬í”„íŠ¸: {len(train_prompts)}ê°œ")
        for i, prompt in enumerate(train_prompts[:5]):  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
            logger.info(f"  {i+1}. '{prompt}'")
        if len(train_prompts) > 5:
            logger.info(f"  ... ì´ {len(train_prompts)}ê°œ")
        
        # 6. ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ ì¸¡ì •
        logger.info("\nğŸ“Š ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ ì¸¡ì •...")
        baseline_rewards = []
        
        for i, prompt in enumerate(train_prompts[:3]):  # ì²˜ìŒ 3ê°œë¡œ ë² ì´ìŠ¤ë¼ì¸ ì¸¡ì •
            logger.info(f"  í…ŒìŠ¤íŠ¸ {i+1}/3: '{prompt}'")
            
            state = trainer.env.reset(prompt)
            original_prompt = trainer.env.current_prompt
            
            # ëª‡ ìŠ¤í… ì‹¤í–‰
            for _ in range(config.max_new_tokens):
                action, _, _ = trainer.policy.get_action_and_log_prob(state)
                state, reward, done, info = trainer.env.step(action)
                if done:
                    baseline_rewards.append(reward)
                    enhanced_prompt = info['current_prompt']
                    logger.info(f"    '{original_prompt}' -> '{enhanced_prompt}' (reward: {reward:.3f})")
                    break
        
        avg_baseline = sum(baseline_rewards) / len(baseline_rewards) if baseline_rewards else 0.0
        logger.info(f"ğŸ“ˆ ë² ì´ìŠ¤ë¼ì¸ í‰ê·  ë¦¬ì›Œë“œ: {avg_baseline:.3f}")
        
        # 7. GRPO í•™ìŠµ ì‹¤í–‰
        logger.info("\nğŸš€ ìˆœìˆ˜ GRPO í•™ìŠµ ì‹œì‘...")
        logger.info("=" * 80)
        
        num_epochs = 10
        trainer.train(train_prompts, num_epochs=num_epochs)
        
        logger.info("âœ… í•™ìŠµ ì™„ë£Œ!")
        
        # 8. í•™ìŠµ í›„ ì„±ëŠ¥ ì¸¡ì •
        logger.info("\nğŸ“Š í•™ìŠµ í›„ ì„±ëŠ¥ ì¸¡ì •...")
        trained_rewards = []
        
        for i, prompt in enumerate(train_prompts[:3]):  # ê°™ì€ í”„ë¡¬í”„íŠ¸ë¡œ í‰ê°€
            logger.info(f"  í‰ê°€ {i+1}/3: '{prompt}'")
            
            state = trainer.env.reset(prompt)
            original_prompt = trainer.env.current_prompt
            
            # ëª‡ ìŠ¤í… ì‹¤í–‰
            for _ in range(config.max_new_tokens):
                action, _, _ = trainer.policy.get_action_and_log_prob(state)
                state, reward, done, info = trainer.env.step(action)
                if done:
                    trained_rewards.append(reward)
                    enhanced_prompt = info['current_prompt']
                    logger.info(f"    '{original_prompt}' -> '{enhanced_prompt}' (reward: {reward:.3f})")
                    break
        
        avg_trained = sum(trained_rewards) / len(trained_rewards) if trained_rewards else 0.0
        logger.info(f"ğŸ“ˆ í•™ìŠµ í›„ í‰ê·  ë¦¬ì›Œë“œ: {avg_trained:.3f}")
        
        # 9. ê²°ê³¼ ë¶„ì„ ë° ì €ì¥
        logger.info("\nğŸ“‹ ìµœì¢… ê²°ê³¼:")
        logger.info("=" * 80)
        logger.info(f"ğŸ¯ ìˆœìˆ˜ GRPO í•™ìŠµ ê²°ê³¼ (Value Network ì—†ìŒ)")
        logger.info(f"ğŸ“Š ë² ì´ìŠ¤ë¼ì¸ ë¦¬ì›Œë“œ: {avg_baseline:.3f}")
        logger.info(f"ğŸ“ˆ í•™ìŠµ í›„ ë¦¬ì›Œë“œ: {avg_trained:.3f}")
        logger.info(f"ğŸ”„ ê°œì„ ë„: {avg_trained - avg_baseline:.3f}")
        logger.info(f"ğŸ“ˆ ê°œì„ ë¥ : {((avg_trained - avg_baseline) / avg_baseline * 100):.1f}%")
        
        if avg_trained > avg_baseline:
            logger.info("âœ… í•™ìŠµì´ ì„±ê³µì ìœ¼ë¡œ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤!")
        else:
            logger.info("âš ï¸ í•™ìŠµ ê°œì„ ì´ ë¯¸ë¯¸í•©ë‹ˆë‹¤. í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        # 10. ëª¨ë¸ ì €ì¥
        logger.info("\nğŸ’¾ ëª¨ë¸ ì €ì¥...")
        save_dir = Path("checkpoints")
        save_dir.mkdir(exist_ok=True)
        
        model_path = save_dir / "pure_grpo_policy.pth"
        torch.save({
            'policy_state_dict': trainer.policy.state_dict(),
            'config': config,
            'baseline_reward': avg_baseline,
            'trained_reward': avg_trained,
            'improvement': avg_trained - avg_baseline
        }, model_path)
        
        logger.info(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")
        
        logger.info("\nğŸ‰ ìˆœìˆ˜ GRPO í•™ìŠµ ì™„ë£Œ!")
        
    except Exception as e:
        logger.error(f"âŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True

if __name__ == "__main__":
    success = main()
    if success:
        logger.info("\nğŸ‰ í”„ë¡œê·¸ë¨ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        logger.error("\nâŒ í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)

            