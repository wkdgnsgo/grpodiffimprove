#!/usr/bin/env python3
"""
QWEN í†µí•© GRPO VLM í•™ìŠµ ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸
QWEN ëª¨ë¸ì˜ enhance_prompt ê¸°ëŠ¥ê³¼ GRPOë¥¼ í†µí•©í•œ í”„ë¡¬í”„íŠ¸ ê°œì„  ì‹œìŠ¤í…œ
"""

import sys
import logging
import torch
from pathlib import Path

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('qwen_grpo_training.log')
    ]
)

logger = logging.getLogger(__name__)

# ëª¨ë¸ ì„í¬íŠ¸
from qwen import QWENModel, QWENGRPOConfig
from clip_reward import CLIPReward
from trainer_grpo_pure import QWENGRPOTrainer

def load_stable_diffusion_pipeline(device="cuda:1"):
    """Stable Diffusion 3 íŒŒì´í”„ë¼ì¸ ë¡œë“œ (GPU 1ë²ˆ)"""
    try:
        from diffusers import StableDiffusion3Pipeline
        import torch
        
        logger.info(f"ğŸ¨ Stable Diffusion 3 íŒŒì´í”„ë¼ì¸ ë¡œë”©... (Device: {device})")
        
        # GPU ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•˜ì§€ ì•Šì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ì„¤ì •
        pipe = StableDiffusion3Pipeline.from_pretrained(
            "stabilityai/stable-diffusion-3-medium-diffusers",
            torch_dtype=torch.float16,
            use_safetensors=True
        )
        
        # ì§€ì •ëœ GPUë¡œ ì´ë™
        if torch.cuda.is_available():
            pipe = pipe.to(device)
            logger.info(f"âœ… SD3 íŒŒì´í”„ë¼ì¸ì„ {device}ë¡œ ì´ë™")
        else:
            logger.warning("âš ï¸ CUDA ì‚¬ìš© ë¶ˆê°€, CPU ì‚¬ìš©")
        
        # ë©”ëª¨ë¦¬ ìµœì í™”
        pipe.enable_model_cpu_offload()
        pipe.enable_attention_slicing()
        
        # í”„ë¡œê·¸ë ˆìŠ¤ ë°” ë¹„í™œì„±í™”
        pipe.set_progress_bar_config(disable=True)
        
        logger.info("âœ… Stable Diffusion 3 íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì™„ë£Œ")
        return pipe
        
    except Exception as e:
        logger.error(f"âŒ SD3 íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise

def get_training_prompts():
    """í•™ìŠµìš© í”„ë¡¬í”„íŠ¸ ë°ì´í„°ì…‹ (ë‹¤ì–‘ì„± í™•ë³´)"""
    import random
    
    # ì „ì²´ í”„ë¡¬í”„íŠ¸ í’€
    all_prompts = [
        # ë™ë¬¼ë“¤
        "a beautiful cat sitting on a chair",
        "majestic lion in African savanna",
        "colorful parrot in tropical rainforest",
        "graceful swan on peaceful lake",
        "playful dolphin jumping in ocean",
        
        # ìì—° í’ê²½
        "sunset over mountains with golden light",
        "misty forest with tall pine trees",
        "desert landscape with sand dunes",
        "rocky coastline with crashing waves",
        "cherry blossoms in spring garden",
        
        # ì˜ˆìˆ ê³¼ ì¶”ìƒ
        "abstract art painting with vibrant colors",
        "geometric patterns in bright neon colors",
        "watercolor painting of flowers",
        "minimalist sculpture in white marble",
        "street art mural on brick wall",
        
        # ì¸ë¬¼
        "portrait of a woman with flowing hair",
        "elderly man reading book by fireplace",
        "child playing in summer meadow",
        "dancer in elegant pose",
        "musician playing violin on stage",
        
        # ë„ì‹œì™€ ê±´ì¶•
        "futuristic city skyline at night",
        "ancient castle on mountain peak",
        "modern glass building reflecting sky",
        "cozy cafe with warm lighting",
        "busy train station with commuters",
        
        # ë„ì „ì ì¸ í”„ë¡¬í”„íŠ¸
        "red apple on blue table with green background",
        "transparent glass sphere floating in purple space",
        "wooden texture mixed with metallic surface",
        "fire and ice elements combined in one scene",
        "microscopic view of crystal structure",
        
        # ë³µì¡í•œ ì¥ë©´
        "crowded marketplace with many people and colorful stalls",
        "underwater scene with coral reef and tropical fish",
        "ancient temple ruins covered with jungle vegetation",
        "steampunk mechanical device with gears and pipes",
        "surreal landscape with floating islands and waterfalls"
    ]
    
    # ë§¤ë²ˆ ë‹¤ë¥¸ ìˆœì„œë¡œ ì„ì–´ì„œ ë°˜í™˜ (ë‹¤ì–‘ì„± í™•ë³´)
    random.shuffle(all_prompts)
    
    # ì²˜ìŒ 15ê°œ ì„ íƒ (ì¶©ë¶„í•œ ë‹¤ì–‘ì„± + ì ë‹¹í•œ í¬ê¸°)
    selected_prompts = all_prompts[:15]
    
    return selected_prompts

def main():
    """ë©”ì¸ í•™ìŠµ í•¨ìˆ˜"""
    logger.info("ğŸš€ QWEN í†µí•© GRPO VLM í•™ìŠµ ì‹œì‘")
    logger.info("=" * 80)
    
    # GPU í™•ì¸ ë° ë°°ì¹˜ ê³„íš
    if torch.cuda.is_available():
        logger.info(f"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥ - GPU ê°œìˆ˜: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            logger.info(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
        
        logger.info("\nğŸ¯ GPU ë°°ì¹˜ ê³„íš:")
        logger.info("  GPU 0: QWEN VL ëª¨ë¸ + GRPO ì •ì±… (í”„ë¡¬í”„íŠ¸ í–¥ìƒ ë° í•™ìŠµ)")
        logger.info("  GPU 1: Stable Diffusion 3 (ì´ë¯¸ì§€ ìƒì„±)")
        logger.info("  GPU 2: CLIP ë¦¬ì›Œë“œ ëª¨ë¸ (ë¦¬ì›Œë“œ ê³„ì‚°)")
    else:
        logger.warning("âš ï¸ CUDA ì‚¬ìš© ë¶ˆê°€ - CPUë¡œ ì‹¤í–‰")
    
    # QWEN GRPO ì„¤ì •
            config = QWENGRPOConfig(
        learning_rate=1e-6,
        batch_size=4,
        num_rollouts=3,  # ë¡¤ì•„ì›ƒ ìˆ˜ ì¤„ì„ (ê° í”„ë¡¬í”„íŠ¸ë‹¹ 3ê°œ ë¡¤ì•„ì›ƒ)
        max_prompt_length=77,
        max_new_tokens=30,
        temperature=1.2,
        top_p=0.9,
        top_k=100,
        kl_coef=0.02,
        clip_ratio=0.2,
        entropy_coef=0.01,
        save_images=True,
        log_dir="qwen_grpo_results"
    )
    
    logger.info("ğŸ“‹ QWEN GRPO ì„¤ì •:")
    logger.info(f"  - í•™ìŠµë¥ : {config.learning_rate}")
    logger.info(f"  - ë°°ì¹˜ í¬ê¸°: {config.batch_size}")
    logger.info(f"  - ë¡¤ì•„ì›ƒ ìˆ˜: {config.num_rollouts}")
    logger.info(f"  - ë¡¤ì•„ì›ƒ ìˆ˜: {config.num_rollouts}")
    logger.info(f"  - ì˜¨ë„: {config.temperature}")
    logger.info(f"  - KL ê³„ìˆ˜: {config.kl_coef}")
    
    try:
        # 1. QWEN VL ëª¨ë¸ ë¡œë“œ (GRPO í†µí•©) (GPU 0ë²ˆ)
        logger.info("\nğŸ§  QWEN VL ëª¨ë¸ + GRPO ë¡œë”©...")
        qwen_model = QWENModel(
            model_name="Qwen/Qwen2-VL-7B-Instruct",
            device="cuda:0",
            temperature=0.7,
            grpo_config=config  # GRPO ì»´í¬ë„ŒíŠ¸ í™œì„±í™”
        )
        logger.info("âœ… QWEN VL + GRPO ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (GPU 0)")
        
        # 2. CLIP ë¦¬ì›Œë“œ ëª¨ë¸ ë¡œë“œ (GPU 2ë²ˆ)
        logger.info("\nğŸ¯ CLIP ë¦¬ì›Œë“œ ëª¨ë¸ ë¡œë”©...")
        reward_model = CLIPReward(device="cuda:2")
        logger.info("âœ… CLIP ë¦¬ì›Œë“œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (GPU 2)")
        
        # 3. Stable Diffusion 3 íŒŒì´í”„ë¼ì¸ ë¡œë“œ (GPU 1ë²ˆ)
        logger.info("\nğŸ¨ Stable Diffusion 3 íŒŒì´í”„ë¼ì¸ ë¡œë”©...")
        sd_pipeline = load_stable_diffusion_pipeline(device="cuda:1")
        logger.info("âœ… SD3 íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì™„ë£Œ (GPU 1)")
        
        # 4. QWEN GRPO íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
        logger.info("\nğŸ¯ QWEN GRPO íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”...")
        trainer = QWENGRPOTrainer(qwen_model, reward_model, sd_pipeline, config)
        logger.info("âœ… íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # 5. í•™ìŠµ ë°ì´í„° ì¤€ë¹„
        train_prompts = get_training_prompts()
        logger.info(f"\nğŸ“ í•™ìŠµ í”„ë¡¬í”„íŠ¸: {len(train_prompts)}ê°œ")
        for i, prompt in enumerate(train_prompts[:5]):  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
            logger.info(f"  {i+1}. '{prompt}'")
        if len(train_prompts) > 5:
            logger.info(f"  ... ì´ {len(train_prompts)}ê°œ")
        
        # 6. ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ ì¸¡ì • (ê¸°ë³¸ QWEN enhance_prompt)
        logger.info("\nğŸ“Š ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ ì¸¡ì • (ê¸°ë³¸ QWEN)...")
        baseline_rewards = []
        
        # ë‹¤ì–‘í•œ í”„ë¡¬í”„íŠ¸ë¡œ ë² ì´ìŠ¤ë¼ì¸ ì¸¡ì • (ì²« ë²ˆì§¸, ì¤‘ê°„, ë§ˆì§€ë§‰)
        baseline_test_indices = [0, len(train_prompts)//2, len(train_prompts)-1]
        baseline_test_prompts = [train_prompts[i] for i in baseline_test_indices]
        
        for i, prompt in enumerate(baseline_test_prompts):
            logger.info(f"  í…ŒìŠ¤íŠ¸ {i+1}/3: '{prompt}'")
            
            try:
                # ê¸°ë³¸ QWEN í–¥ìƒ
                with torch.cuda.device(0):
                    basic_result = qwen_model.enhance_prompt(prompt)
                    enhanced_prompt = basic_result['enhanced_prompt']
                
                # ì´ë¯¸ì§€ ìƒì„± ë° ë¦¬ì›Œë“œ ê³„ì‚°
                state = trainer.env.reset(prompt)
                trainer.env.current_enhanced_prompt = enhanced_prompt
                
                # ì´ë¯¸ì§€ ìƒì„±
                with torch.cuda.device(1):
                    enhanced_result = sd_pipeline(
                        prompt=enhanced_prompt,
                        num_inference_steps=28,
                        guidance_scale=7.0,
                        height=1024,
                        width=1024
                    )
                    enhanced_image = enhanced_result.images[0]
                
                # ë¦¬ì›Œë“œ ê³„ì‚°
                with torch.cuda.device(2):
                    reward = reward_model.calculate_reward(
                        prompt,
                        enhanced_prompt,
                        enhanced_image
                    )
                
                baseline_rewards.append(reward)
                logger.info(f"    '{prompt}' -> '{enhanced_prompt[:50]}...' (reward: {reward:.3f})")
                
            except Exception as e:
                logger.warning(f"    ë² ì´ìŠ¤ë¼ì¸ ì¸¡ì • ì‹¤íŒ¨: {e}")
                continue
        
        avg_baseline = sum(baseline_rewards) / len(baseline_rewards) if baseline_rewards else 0.0
        logger.info(f"ğŸ“ˆ ë² ì´ìŠ¤ë¼ì¸ í‰ê·  ë¦¬ì›Œë“œ: {avg_baseline:.3f}")
        
        # 7. QWEN GRPO í•™ìŠµ ì‹¤í–‰
        logger.info("\nğŸš€ QWEN GRPO í•™ìŠµ ì‹œì‘...")
        logger.info("=" * 80)
        
        all_metrics, baseline_data = trainer.train(
            train_prompts=train_prompts, 
            num_epochs=10, 
            num_baseline_episodes=3  # ë² ì´ìŠ¤ë¼ì¸ ì—í”¼ì†Œë“œ ìˆ˜ ì¡°ì • ê°€ëŠ¥
        )
        
        logger.info("âœ… í•™ìŠµ ì™„ë£Œ!")
        
        # 8. í•™ìŠµ í›„ ì„±ëŠ¥ ì¸¡ì • (GRPO ê¸°ë°˜)
        logger.info("\nğŸ“Š í•™ìŠµ í›„ ì„±ëŠ¥ ì¸¡ì • (GRPO)...")
        trained_rewards = []
        
        # ë² ì´ìŠ¤ë¼ì¸ê³¼ ê°™ì€ í”„ë¡¬í”„íŠ¸ë¡œ í‰ê°€
        for i, prompt in enumerate(baseline_test_prompts):
            logger.info(f"  í‰ê°€ {i+1}/3: '{prompt}'")
            
            try:
                # GRPO ê¸°ë°˜ í–¥ìƒ
                with torch.cuda.device(0):
                    grpo_enhanced, log_prob = qwen_model.generate_grpo_enhanced_prompt(prompt)
                
                # ì´ë¯¸ì§€ ìƒì„± ë° ë¦¬ì›Œë“œ ê³„ì‚°
                state = trainer.env.reset(prompt)
                trainer.env.current_enhanced_prompt = grpo_enhanced
                
                # ì´ë¯¸ì§€ ìƒì„±
                with torch.cuda.device(1):
                    enhanced_result = sd_pipeline(
                        prompt=grpo_enhanced,
                        num_inference_steps=28,
                        guidance_scale=7.0,
                        height=1024,
                        width=1024
                    )
                    enhanced_image = enhanced_result.images[0]
                
                # ë¦¬ì›Œë“œ ê³„ì‚°
                with torch.cuda.device(2):
                    reward = reward_model.calculate_reward(
                        prompt,
                        grpo_enhanced,
                        enhanced_image
                    )
                
                trained_rewards.append(reward)
                logger.info(f"    '{prompt}' -> '{grpo_enhanced[:50]}...' (reward: {reward:.3f})")
                
            except Exception as e:
                logger.warning(f"    í•™ìŠµ í›„ í‰ê°€ ì‹¤íŒ¨: {e}")
                continue
        
        avg_trained = sum(trained_rewards) / len(trained_rewards) if trained_rewards else 0.0
        logger.info(f"ğŸ“ˆ í•™ìŠµ í›„ í‰ê·  ë¦¬ì›Œë“œ: {avg_trained:.3f}")
        
        # 9. ê²°ê³¼ ë¶„ì„ ë° ì €ì¥
        logger.info("\nğŸ“‹ ìµœì¢… ê²°ê³¼:")
        logger.info("=" * 80)
        logger.info(f"ğŸ¯ QWEN GRPO í•™ìŠµ ê²°ê³¼")
        logger.info(f"ğŸ“Š ë² ì´ìŠ¤ë¼ì¸ ë¦¬ì›Œë“œ (ê¸°ë³¸ QWEN): {avg_baseline:.3f}")
        logger.info(f"ğŸ“ˆ í•™ìŠµ í›„ ë¦¬ì›Œë“œ (GRPO): {avg_trained:.3f}")
        logger.info(f"ğŸ”„ ê°œì„ ë„: {avg_trained - avg_baseline:.3f}")
        
        if avg_baseline > 0:
            logger.info(f"ğŸ“ˆ ê°œì„ ë¥ : {((avg_trained - avg_baseline) / avg_baseline * 100):.1f}%")
        
        if avg_trained > avg_baseline:
            logger.info("âœ… GRPO í•™ìŠµì´ ì„±ê³µì ìœ¼ë¡œ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤!")
        else:
            logger.info("âš ï¸ GRPO í•™ìŠµ ê°œì„ ì´ ë¯¸ë¯¸í•©ë‹ˆë‹¤. í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        # 10. ëª¨ë¸ ì €ì¥
        logger.info("\nğŸ’¾ ëª¨ë¸ ì €ì¥...")
        save_dir = Path("checkpoints")
        save_dir.mkdir(exist_ok=True)
        
        model_path = save_dir / "qwen_grpo_model.pth"
        torch.save({
            'model_state_dict': qwen_model.model.state_dict(),
            'config': config,
            'baseline_reward': avg_baseline,
            'trained_reward': avg_trained,
            'improvement': avg_trained - avg_baseline,
            'training_metrics': all_metrics
        }, model_path)
        
        logger.info(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")
        
        logger.info("\nğŸ‰ QWEN GRPO í•™ìŠµ ì™„ë£Œ!")
        
    except Exception as e:
        logger.error(f"âŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True

if __name__ == "__main__":
    success = main()
    if success:
        logger.info("\nğŸ‰ í”„ë¡œê·¸ë¨ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        logger.error("\nâŒ í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)

            