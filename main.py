#!/usr/bin/env python3
"""
QWEN í†µí•© GRPO VLM í•™ìŠµ ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸ (Accelerate ë©€í‹° GPU ë²„ì „)
QWEN ëª¨ë¸ì˜ enhance_prompt ê¸°ëŠ¥ê³¼ GRPOë¥¼ í†µí•©í•œ í”„ë¡¬í”„íŠ¸ ê°œì„  ì‹œìŠ¤í…œ
"""

import sys
import os
import logging
import torch
from pathlib import Path
from accelerate import Accelerator

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('qwen_grpo_training.log')
    ]
)

logger = logging.getLogger(__name__)

# ëª¨ë¸ ì„í¬íŠ¸
from qwen import QWENModel, QWENGRPOConfig
from clip_reward import CLIPReward
from trainer_grpo_pure import QWENGRPOTrainer

def load_stable_diffusion_pipeline(device="cuda:6"):
    """Stable Diffusion 3 íŒŒì´í”„ë¼ì¸ ë¡œë“œ (GPU 5ë²ˆ ì „ìš©)"""
    try:
        from diffusers import StableDiffusion3Pipeline
        import torch
        
        logger.info(f"ğŸ¨ Stable Diffusion 3 íŒŒì´í”„ë¼ì¸ ë¡œë”©... (Device: {device})")
        
        # GPU ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•˜ì§€ ì•Šì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ì„¤ì •
        pipe = StableDiffusion3Pipeline.from_pretrained(
            "stabilityai/stable-diffusion-3-medium-diffusers",
            torch_dtype=torch.float16,
            use_safetensors=True
        )
        
        # ì§€ì •ëœ GPUë¡œ ì´ë™
        if torch.cuda.is_available():
            pipe = pipe.to(device)
            logger.info(f"âœ… SD3 íŒŒì´í”„ë¼ì¸ì„ {device}ë¡œ ì´ë™")
        else:
            logger.warning("âš ï¸ CUDA ì‚¬ìš© ë¶ˆê°€, CPU ì‚¬ìš©")
        
        # ë©”ëª¨ë¦¬ ìµœì í™”
        pipe.enable_model_cpu_offload()
        pipe.enable_attention_slicing()
        
        # í”„ë¡œê·¸ë ˆìŠ¤ ë°” ë¹„í™œì„±í™”
        pipe.set_progress_bar_config(disable=True)
        
        logger.info("âœ… Stable Diffusion 3 íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì™„ë£Œ")
        return pipe
        
    except Exception as e:
        logger.error(f"âŒ SD3 íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise

def get_training_prompts():
    """í•™ìŠµìš© í”„ë¡¬í”„íŠ¸ ë°ì´í„°ì…‹ (ë©”ëª¨ë¦¬ ìµœì í™” - ê°œìˆ˜ ì¶•ì†Œ)"""
    import random
    
    # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ í”„ë¡¬í”„íŠ¸ ìˆ˜ ì¶•ì†Œ
    selected_prompts = [
        # ê¸°ë³¸ì ì¸ í”„ë¡¬í”„íŠ¸ë“¤ (ë©”ëª¨ë¦¬ ì ˆì•½)
        "a beautiful cat sitting on a chair",
        "sunset over mountains with golden light",
        "abstract art painting with vibrant colors",
        "portrait of a woman with flowing hair",
        "futuristic city skyline at night",
        
        # ë„ì „ì ì¸ í”„ë¡¬í”„íŠ¸ë“¤
        "red apple on blue table with green background",
        "transparent glass sphere floating in purple space",
        "crowded marketplace with many people and colorful stalls"
    ]
    
    # ë§¤ë²ˆ ë‹¤ë¥¸ ìˆœì„œë¡œ ì„ì–´ì„œ ë°˜í™˜
    random.shuffle(selected_prompts)
    
    return selected_prompts

def main():
    """ë©”ì¸ í•™ìŠµ í•¨ìˆ˜ (Accelerate ë©€í‹° GPU ë²„ì „)"""
    logger.info("ğŸš€ QWEN í†µí•© GRPO VLM í•™ìŠµ ì‹œì‘ (Accelerate ë©€í‹° GPU)")
    logger.info("=" * 80)
    
    # Accelerate ì´ˆê¸°í™”
    accelerator = Accelerator()
    logger.info(f"ğŸ¯ Accelerate ì´ˆê¸°í™” ì™„ë£Œ")
    logger.info(f"  - í”„ë¡œì„¸ìŠ¤ ìˆ˜: {accelerator.num_processes}")
    logger.info(f"  - ë¡œì»¬ í”„ë¡œì„¸ìŠ¤ ì¸ë±ìŠ¤: {accelerator.local_process_index}")
    logger.info(f"  - ë””ë°”ì´ìŠ¤: {accelerator.device}")
    
    # GPU í™•ì¸ ë° ë°°ì¹˜ ê³„íš
    if torch.cuda.is_available():
        logger.info(f"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥ - GPU ê°œìˆ˜: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            logger.info(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
            logger.info(f"    ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f}GB")
        
        logger.info("\nğŸ¯ GPU ë°°ì¹˜ ê³„íš (DeepSpeed ZeRO Stage 3 ì „ì²´ í•™ìŠµ - 8 GPU):")
        logger.info("  GPU 0-3: QWEN ì „ì²´ í•™ìŠµ (DeepSpeed ZeRO Stage 3 ë¶„ì‚°)")
        logger.info("  GPU 4-5: CLIP ë¦¬ì›Œë“œ ëª¨ë¸ (ì´ì¤‘í™”)")
        logger.info("  GPU 6-7: Stable Diffusion 3 (ì´ì¤‘í™”)")
    else:
        logger.warning("âš ï¸ CUDA ì‚¬ìš© ë¶ˆê°€ - CPUë¡œ ì‹¤í–‰")
    
    # QWEN GRPO ì„¤ì • (DeepSpeed ZeRO Stage 3 ì „ì²´ í•™ìŠµ)
    config = QWENGRPOConfig(
        learning_rate=5e-7,  # ì „ì²´ í•™ìŠµìš© ë‚®ì€ í•™ìŠµë¥ 
        batch_size=2,  # DeepSpeed ZeRO Stage 3ë¡œ ë°°ì¹˜ í¬ê¸° ì¦ê°€
        num_rollouts=2,  # 4ê°œ GPUë¡œ ë¡¤ì•„ì›ƒ ìˆ˜ ì¦ê°€
        max_prompt_length=77,
        max_new_tokens=25,  # í† í° ìˆ˜ ë³µì›
        temperature=1.2,
        top_p=0.9,
        top_k=100,
        kl_coef=0.02,
        clip_ratio=0.2,
        entropy_coef=0.01,
        save_images=True,
        log_dir="qwen_grpo_full_training_results"
    )
    
    logger.info("ğŸ“‹ QWEN GRPO ì„¤ì • (Accelerate ë©€í‹° GPU):")
    logger.info(f"  - í•™ìŠµë¥ : {config.learning_rate}")
    logger.info(f"  - ë°°ì¹˜ í¬ê¸°: {config.batch_size} (ë©€í‹° GPU)")
    logger.info(f"  - ë¡¤ì•„ì›ƒ ìˆ˜: {config.num_rollouts} (ë©€í‹° GPU)")
    logger.info(f"  - ì˜¨ë„: {config.temperature}")
    logger.info(f"  - KL ê³„ìˆ˜: {config.kl_coef}")
    
    try:
        # GPU ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì • (GPU 0 OOM ë°©ì§€)
        if torch.cuda.is_available():
            # ë©”ëª¨ë¦¬ í• ë‹¹ ìµœì í™”
            torch.cuda.empty_cache()
            
            # GPU ë©”ëª¨ë¦¬ ì œí•œ ì„¤ì • (DeepSpeed ZeRO Stage 3 - 8 GPU)
            # GPU 0-7ì— ëŒ€í•´ ë©”ëª¨ë¦¬ ì œí•œ ì„¤ì •
            gpu_memory_fractions = {
                0: 0.90,  # QWEN ë©”ì¸ í”„ë¡œì„¸ìŠ¤ (ì•½ê°„ ë‚®ê²Œ)
                1: 0.95,  # QWEN ì„œë¸Œ í”„ë¡œì„¸ìŠ¤
                2: 0.95,  # QWEN ì„œë¸Œ í”„ë¡œì„¸ìŠ¤
                3: 0.95,  # QWEN ì„œë¸Œ í”„ë¡œì„¸ìŠ¤
                4: 0.95,  # CLIP ë©”ì¸
                5: 0.95,  # CLIP ë°±ì—…
                6: 0.95,  # SD3 ë©”ì¸
                7: 0.95   # SD3 ë°±ì—…
            }
            
            for gpu_id, fraction in gpu_memory_fractions.items():
                if gpu_id < torch.cuda.device_count():
                    torch.cuda.set_per_process_memory_fraction(fraction, device=gpu_id)
                    logger.info(f"ğŸ”§ GPU {gpu_id} ë©”ëª¨ë¦¬ ì œí•œ ì„¤ì •: {int(fraction*100)}% (8 GPU ëª¨ë“œ)")
            
            # PyTorch ë©”ëª¨ë¦¬ í• ë‹¹ ìµœì í™”
            os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
            
            logger.info("ğŸ§¹ ì´ˆê¸° GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ë° ìµœì í™” ì™„ë£Œ")
        
        # 1. QWEN VL ëª¨ë¸ ë¡œë“œ (Accelerateë¡œ ë¶„ì‚°)
        logger.info("\nğŸ§  QWEN VL ëª¨ë¸ + ì „ì²´ í•™ìŠµ ë¡œë”©... (DeepSpeed ZeRO Stage 3)")
        qwen_model = QWENModel(
            model_name="Qwen/Qwen2-VL-7B-Instruct",
            device="accelerate",  # Accelerate ì „ìš© ëª¨ë“œ
            temperature=0.7,
            grpo_config=config,  # GRPO ì»´í¬ë„ŒíŠ¸ í™œì„±í™”
            is_main_process=accelerator.is_main_process  # Reference ëª¨ë¸ ìƒì„± ì—¬ë¶€
        )
        
        # Accelerateë¡œ ëª¨ë¸ ì¤€ë¹„ (ë‹¨ì¼ í˜¸ì¶œë¡œ ë³€ê²½)
        logger.info("ğŸ”§ Accelerateë¥¼ í†µí•œ ëª¨ë¸ ë¶„ì‚° ì„¤ì •...")
        qwen_model.model = accelerator.prepare(qwen_model.model)
        qwen_model.grpo_optimizer = accelerator.prepare(qwen_model.grpo_optimizer)
        
        logger.info("âœ… QWEN VL + GRPO ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (Accelerate ë¶„ì‚°)")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            logger.info("ğŸ§¹ QWEN ë¡œë“œ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬")
        
        # 2. í†µí•© ëª¨ë¸ë“¤ ë¡œë“œ (GPU 4ë²ˆ, 5ë²ˆ) - ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œë§Œ
        logger.info("\nğŸ¯ í†µí•© ëª¨ë¸ë“¤ ë¡œë”© ì²´í¬...")
        
        # ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œë§Œ ë¡œë”©
        if accelerator.is_main_process:
            logger.info("ğŸ¯ ë©”ì¸ í”„ë¡œì„¸ìŠ¤: í†µí•© ëª¨ë¸ë“¤ ë¡œë”© (GPU 4-7)")
            
            # CLIP ë¦¬ì›Œë“œ ëª¨ë¸ (GPU 4ë²ˆ ë©”ì¸)
            reward_model = CLIPReward(device="cuda:4")
            logger.info("âœ… CLIP ë¦¬ì›Œë“œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (GPU 4)")
            
            # Stable Diffusion 3 íŒŒì´í”„ë¼ì¸ (GPU 6ë²ˆ ë©”ì¸) - 1ê°œë§Œ ë¡œë”©
            sd_pipeline = load_stable_diffusion_pipeline(device="cuda:6")
            logger.info("âœ… SD3 íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì™„ë£Œ (GPU 6) - 1ê°œë§Œ ë¡œë”©")
            
            # Reference ëª¨ë¸ì€ ì „ì²´ í•™ìŠµì—ì„œ ë¹„í™œì„±í™”
            logger.info("ğŸ¯ ì „ì²´ í•™ìŠµ ëª¨ë“œ: Reference ëª¨ë¸ ë¹„í™œì„±í™”")
        else:
            # ì„œë¸Œ í”„ë¡œì„¸ìŠ¤ì—ì„œëŠ” ë¡œë”©í•˜ì§€ ì•ŠìŒ
            logger.info("ğŸ¯ ì„œë¸Œ í”„ë¡œì„¸ìŠ¤: í†µí•© ëª¨ë¸ë“¤ ë¡œë”© ê±´ë„ˆë›°ê¸°")
            reward_model = None
            sd_pipeline = None
        
        # ëª¨ë“  í”„ë¡œì„¸ìŠ¤ ë™ê¸°í™” (ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì˜ ëª¨ë¸ ë¡œë”© ì™„ë£Œ ëŒ€ê¸°)
        accelerator.wait_for_everyone()
        
        # ìµœì¢… ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            logger.info("ğŸ§¹ ëª¨ë“  ëª¨ë¸ ë¡œë“œ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬")
        
        # 3. QWEN GRPO íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™” (Accelerate ë²„ì „)
        logger.info("\nğŸ¯ QWEN GRPO íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”... (Accelerate)")
        trainer = QWENGRPOTrainer(qwen_model, reward_model, sd_pipeline, config)
        
        # Acceleratorë¥¼ íŠ¸ë ˆì´ë„ˆì— ì „ë‹¬
        trainer.accelerator = accelerator
        
        logger.info("âœ… íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™” ì™„ë£Œ (Accelerate)")
        
        # 4. í•™ìŠµ ë°ì´í„° ì¤€ë¹„
        train_prompts = get_training_prompts()
        logger.info(f"\nğŸ“ í•™ìŠµ í”„ë¡¬í”„íŠ¸: {len(train_prompts)}ê°œ")
        for i, prompt in enumerate(train_prompts):
            logger.info(f"  {i+1}. '{prompt}'")
        
        # 5. ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ ì¸¡ì • (ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œë§Œ)
        if accelerator.is_main_process:
            logger.info("\nğŸ“Š ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ ì¸¡ì •...")
            baseline_rewards = []
            
            # ì²« ë²ˆì§¸ í”„ë¡¬í”„íŠ¸ë¡œ í…ŒìŠ¤íŠ¸
            test_prompt = train_prompts[0]
            logger.info(f"  ë² ì´ìŠ¤ë¼ì¸ í…ŒìŠ¤íŠ¸: '{test_prompt}'")
            
            try:
                # ê¸°ë³¸ QWEN í–¥ìƒ
                with accelerator.device:
                    basic_result = qwen_model.enhance_prompt(test_prompt)
                    enhanced_prompt = basic_result['enhanced_prompt']
                
                # ì´ë¯¸ì§€ ìƒì„± ë° ë¦¬ì›Œë“œ ê³„ì‚°
                state = trainer.env.reset(test_prompt)
                trainer.env.current_enhanced_prompt = enhanced_prompt
                
                # ì´ë¯¸ì§€ ìƒì„± (GPU 5ë²ˆ)
                with torch.cuda.device(5):
                    enhanced_result = sd_pipeline(
                        prompt=enhanced_prompt,
                        num_inference_steps=20,
                        guidance_scale=7.0,
                        height=1024,
                        width=1024
                    )
                    enhanced_image = enhanced_result.images[0]
                
                # ë¦¬ì›Œë“œ ê³„ì‚° (GPU 4ë²ˆ)
                with torch.cuda.device(4):
                    reward = reward_model.calculate_reward(
                        test_prompt,
                        enhanced_prompt,
                        enhanced_image
                    )
                
                baseline_rewards.append(reward)
                logger.info(f"    ë² ì´ìŠ¤ë¼ì¸ ë¦¬ì›Œë“œ: {reward:.3f}")
                
            except Exception as e:
                logger.warning(f"    ë² ì´ìŠ¤ë¼ì¸ ì¸¡ì • ì‹¤íŒ¨: {e}")
                baseline_rewards.append(0.5)  # ê¸°ë³¸ê°’
            
            avg_baseline = sum(baseline_rewards) / len(baseline_rewards) if baseline_rewards else 0.5
            logger.info(f"ğŸ“ˆ ë² ì´ìŠ¤ë¼ì¸ í‰ê·  ë¦¬ì›Œë“œ: {avg_baseline:.3f}")
        else:
            avg_baseline = 0.5  # ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ëŠ” ê¸°ë³¸ê°’
        
        # ë² ì´ìŠ¤ë¼ì¸ ê°’ì„ ëª¨ë“  í”„ë¡œì„¸ìŠ¤ì— ë¸Œë¡œë“œìºìŠ¤íŠ¸
        if accelerator.num_processes > 1:
            avg_baseline_tensor = torch.tensor(avg_baseline, device=accelerator.device)
            avg_baseline = accelerator.gather(avg_baseline_tensor)[0].item()
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            logger.info("ğŸ§¹ ë² ì´ìŠ¤ë¼ì¸ ì¸¡ì • í›„ ë©”ëª¨ë¦¬ ì •ë¦¬")
        
        # 6. QWEN GRPO í•™ìŠµ ì‹¤í–‰ (Accelerate ë¶„ì‚°)
        logger.info("\nğŸš€ QWEN GRPO í•™ìŠµ ì‹œì‘... (Accelerate ë¶„ì‚°)")
        logger.info("=" * 80)
        
        all_metrics, baseline_data = trainer.train(
            train_prompts=train_prompts, 
            num_epochs=12,  # LoRAë¡œ ë” ë§ì€ ì—í¬í¬ ê°€ëŠ¥
            num_baseline_episodes=2
        )
        
        logger.info("âœ… í•™ìŠµ ì™„ë£Œ!")
        
        # 7. í•™ìŠµ í›„ ì„±ëŠ¥ ì¸¡ì • (ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œë§Œ)
        if accelerator.is_main_process:
            logger.info("\nğŸ“Š í•™ìŠµ í›„ ì„±ëŠ¥ ì¸¡ì •...")
            trained_rewards = []
            
            try:
                # GRPO ê¸°ë°˜ í–¥ìƒ
                with accelerator.device:
                    grpo_enhanced, log_prob = qwen_model.generate_grpo_enhanced_prompt(test_prompt)
                
                # ì´ë¯¸ì§€ ìƒì„± ë° ë¦¬ì›Œë“œ ê³„ì‚°
                state = trainer.env.reset(test_prompt)
                trainer.env.current_enhanced_prompt = grpo_enhanced
                
                # ì´ë¯¸ì§€ ìƒì„± (GPU 5ë²ˆ)
                with torch.cuda.device(5):
                    enhanced_result = sd_pipeline(
                        prompt=grpo_enhanced,
                        num_inference_steps=20,
                        guidance_scale=7.0,
                        height=1024,
                        width=1024
                    )
                    enhanced_image = enhanced_result.images[0]
                
                # ë¦¬ì›Œë“œ ê³„ì‚° (GPU 4ë²ˆ)
                with torch.cuda.device(4):
                    reward = reward_model.calculate_reward(
                        test_prompt,
                        grpo_enhanced,
                        enhanced_image
                    )
                
                trained_rewards.append(reward)
                logger.info(f"    í•™ìŠµ í›„ ë¦¬ì›Œë“œ: {reward:.3f}")
                
            except Exception as e:
                logger.warning(f"    í•™ìŠµ í›„ í‰ê°€ ì‹¤íŒ¨: {e}")
                trained_rewards.append(avg_baseline)  # ê¸°ë³¸ê°’
            
            avg_trained = sum(trained_rewards) / len(trained_rewards) if trained_rewards else avg_baseline
            logger.info(f"ğŸ“ˆ í•™ìŠµ í›„ í‰ê·  ë¦¬ì›Œë“œ: {avg_trained:.3f}")
            
            # 8. ê²°ê³¼ ë¶„ì„ ë° ì €ì¥ (ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œë§Œ)
            logger.info("\nğŸ“‹ ìµœì¢… ê²°ê³¼:")
            logger.info("=" * 80)
            logger.info(f"ğŸ¯ QWEN GRPO í•™ìŠµ ê²°ê³¼ (Accelerate ë©€í‹° GPU)")
            logger.info(f"ğŸ“Š ë² ì´ìŠ¤ë¼ì¸ ë¦¬ì›Œë“œ: {avg_baseline:.3f}")
            logger.info(f"ğŸ“ˆ í•™ìŠµ í›„ ë¦¬ì›Œë“œ: {avg_trained:.3f}")
            logger.info(f"ğŸ”„ ê°œì„ ë„: {avg_trained - avg_baseline:.3f}")
            
            if avg_baseline > 0:
                logger.info(f"ğŸ“ˆ ê°œì„ ë¥ : {((avg_trained - avg_baseline) / avg_baseline * 100):.1f}%")
            
            if avg_trained > avg_baseline:
                logger.info("âœ… GRPO í•™ìŠµì´ ì„±ê³µì ìœ¼ë¡œ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤!")
            else:
                logger.info("âš ï¸ GRPO í•™ìŠµ ê°œì„ ì´ ë¯¸ë¯¸í•©ë‹ˆë‹¤. ë” ë§ì€ í•™ìŠµì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
            # 9. LoRA ëª¨ë¸ ì €ì¥ (ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œë§Œ)
            logger.info("\nğŸ’¾ LoRA ëª¨ë¸ ì €ì¥...")
            save_dir = Path("checkpoints")
            save_dir.mkdir(exist_ok=True)
            
            # LoRA ì–´ëŒ‘í„° ì €ì¥
            lora_path = save_dir / "qwen_grpo_lora"
            
            # Accelerate unwrapìœ¼ë¡œ ì›ë³¸ LoRA ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
            unwrapped_model = accelerator.unwrap_model(qwen_model.model)
            
            # LoRA ì–´ëŒ‘í„°ë§Œ ì €ì¥
            unwrapped_model.save_pretrained(lora_path)
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            metadata_path = save_dir / "training_metadata.json"
            import json
            metadata = {
                'baseline_reward': avg_baseline,
                'trained_reward': avg_trained,
                'improvement': avg_trained - avg_baseline,
                'lora_config': {
                    'r': 16,
                    'alpha': 32,
                    'dropout': 0.1
                },
                'training_config': {
                    'learning_rate': config.learning_rate,
                    'batch_size': config.batch_size,
                    'num_epochs': 8
                }
            }
            
            with open(metadata_path, 'w') as f:
                json.dump(metadata, f, indent=2)
            
            logger.info(f"âœ… LoRA ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {lora_path}")
            logger.info(f"âœ… ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ: {metadata_path}")
            
            # LoRA íŒŒë¼ë¯¸í„° ì •ë³´ ì¶œë ¥
            lora_info = qwen_model.get_lora_trainable_params()
            logger.info(f"ğŸ“Š LoRA í•™ìŠµ íŒŒë¼ë¯¸í„°: {lora_info['trainable_params']:,}")
            logger.info(f"ğŸ“Š ì „ì²´ íŒŒë¼ë¯¸í„°: {lora_info['all_params']:,}")
            logger.info(f"ğŸ“Š í•™ìŠµ ë¹„ìœ¨: {lora_info['trainable_percentage']:.2f}%")
        
        # ìµœì¢… ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            logger.info("ğŸ§¹ ìµœì¢… GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
        
        logger.info("\nğŸ‰ QWEN GRPO í•™ìŠµ ì™„ë£Œ!")
        
    except Exception as e:
        logger.error(f"âŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        
        # ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            logger.info("ğŸ§¹ ì—ëŸ¬ ë°œìƒ í›„ GPU ë©”ëª¨ë¦¬ ì •ë¦¬")
        
        return False
    
    return True

if __name__ == "__main__":
    success = main()
    if success:
        logger.info("\nğŸ‰ í”„ë¡œê·¸ë¨ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        logger.error("\nâŒ í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)

            