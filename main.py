#!/usr/bin/env python3
"""
Simple Multi-GPU GRPO Training
GPU 0: QWEN LoRA Training
GPU 1: CLIP Reward Calculation  
GPU 2: Stable Diffusion 3 Image Generation
"""

import torch
import logging
import os
from typing import List, Dict
import time

from qwen import QWENModel, QWENGRPOConfig
from clip_reward import CLIPReward

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('grpo_training.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def load_stable_diffusion_pipeline(device="cuda:2"):
    """Stable Diffusion 3 íŒŒì´í”„ë¼ì¸ ë¡œë“œ (GPU 2ë²ˆ ì „ìš©)"""
    try:
        from diffusers import StableDiffusion3Pipeline
        
        logger.info(f"ğŸ¨ SD3 íŒŒì´í”„ë¼ì¸ ë¡œë”© ì¤‘... ({device})")
        
        pipe = StableDiffusion3Pipeline.from_pretrained(
            "stabilityai/stable-diffusion-3-medium-diffusers",
            torch_dtype=torch.float16,
            use_safetensors=True
        )
        
        # ì§€ì •ëœ GPUë¡œ ì´ë™
        pipe = pipe.to(device)
        logger.info(f"âœ… SD3 íŒŒì´í”„ë¼ì¸ì„ {device}ë¡œ ì´ë™")
        
        # ë©”ëª¨ë¦¬ ìµœì í™”
        pipe.enable_model_cpu_offload()
        pipe.enable_attention_slicing()
        pipe.set_progress_bar_config(disable=True)
        
        logger.info("âœ… Stable Diffusion 3 íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì™„ë£Œ")
        return pipe
        
    except Exception as e:
        logger.error(f"âŒ SD3 íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise

def get_training_prompts():
    """í•™ìŠµìš© í”„ë¡¬í”„íŠ¸ ë°ì´í„°ì…‹"""
    import random
    
    selected_prompts = [
        "a beautiful cat sitting on a chair",
        "sunset over mountains with golden light", 
        "abstract art painting with vibrant colors",
        "portrait of a woman with flowing hair",
        "futuristic city skyline at night",
        "red apple on blue table with green background",
        "transparent glass sphere floating in purple space",
        "crowded marketplace with many people and colorful stalls"
    ]
    
    random.shuffle(selected_prompts)
    return selected_prompts

class SimpleGRPOTrainer:
    """ê°„ë‹¨í•œ ë©€í‹° GPU GRPO íŠ¸ë ˆì´ë„ˆ"""
    
    def __init__(self, config: QWENGRPOConfig):
        self.config = config
        
        # GPU ê°€ìš©ì„± í™•ì¸
        if not torch.cuda.is_available():
            logger.warning("âš ï¸ CUDA ì‚¬ìš© ë¶ˆê°€ - CPUë¡œ ì‹¤í–‰")
            self.use_gpu = False
        elif torch.cuda.device_count() < 3:
            logger.warning(f"âš ï¸ GPU {torch.cuda.device_count()}ê°œë§Œ ì‚¬ìš© ê°€ëŠ¥ (ê¶Œì¥: 3ê°œ)")
            self.use_gpu = True
        else:
            self.use_gpu = True
            logger.info(f"ğŸ¯ ì‚¬ìš© ê°€ëŠ¥í•œ GPU: {torch.cuda.device_count()}ê°œ")
            for i in range(min(3, torch.cuda.device_count())):
                logger.info(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
        
        # ëª¨ë¸ë“¤ ì´ˆê¸°í™”
        self._init_models()
        
    def _init_models(self):
        """ê° GPUë³„ ëª¨ë¸ ì´ˆê¸°í™”"""
        logger.info("ğŸ”§ ëª¨ë¸ë“¤ ì´ˆê¸°í™” ì¤‘...")
        
        # GPU 0: QWEN LoRA ëª¨ë¸
        qwen_device = "cuda:0" if self.use_gpu else "cpu"
        logger.info(f"ğŸ§  {qwen_device}: QWEN LoRA ëª¨ë¸ ë¡œë”©...")
        self.qwen_model = QWENModel(
            model_name="Qwen/Qwen2-VL-7B-Instruct",
            device=qwen_device,
            temperature=0.7,
            grpo_config=self.config,
            is_main_process=True
        )
        logger.info(f"âœ… QWEN ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({qwen_device})")
        
        # GPU 1: CLIP ë¦¬ì›Œë“œ ëª¨ë¸  
        clip_device = "cuda:1" if self.use_gpu and torch.cuda.device_count() > 1 else "cuda:0" if self.use_gpu else "cpu"
        logger.info(f"ğŸ¯ {clip_device}: CLIP ë¦¬ì›Œë“œ ëª¨ë¸ ë¡œë”©...")
        self.reward_model = CLIPReward(device=clip_device)
        logger.info(f"âœ… CLIP ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({clip_device})")
        
        # GPU 2: Stable Diffusion 3
        sd_device = "cuda:2" if self.use_gpu and torch.cuda.device_count() > 2 else "cuda:0" if self.use_gpu else "cpu"
        logger.info(f"ğŸ¨ {sd_device}: SD3 íŒŒì´í”„ë¼ì¸ ë¡œë”©...")
        self.sd_pipeline = load_stable_diffusion_pipeline(device=sd_device)
        logger.info(f"âœ… SD3 íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì™„ë£Œ ({sd_device})")
        
        logger.info("ğŸ¯ ëª¨ë“  ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ!")
        
    def generate_enhanced_prompts(self, user_prompt: str, num_rollouts: int) -> List[tuple]:
        """í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ ìƒì„± (GPU 0ì—ì„œ ì‹¤í–‰)"""
        logger.info(f"ğŸ§  í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ ìƒì„± ({num_rollouts}ê°œ)")
        
        enhanced_data = []
        
        for i in range(num_rollouts):
            try:
                enhanced_prompt, log_prob = self.qwen_model.generate_grpo_enhanced_prompt(user_prompt)
                enhanced_data.append((enhanced_prompt, log_prob))
                logger.info(f"  ìƒì„± {i+1}/{num_rollouts}: '{enhanced_prompt[:50]}...' (log_prob: {log_prob:.4f})")
            except Exception as e:
                logger.error(f"  ìƒì„± {i+1} ì‹¤íŒ¨: {e}")
                continue
        
        logger.info(f"âœ… {len(enhanced_data)}ê°œ í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ")
        return enhanced_data
    
    def generate_images(self, enhanced_prompts: List[str]) -> List:
        """ì´ë¯¸ì§€ ìƒì„± (GPU 2ì—ì„œ ì‹¤í–‰)"""
        logger.info(f"ğŸ¨ ì´ë¯¸ì§€ ìƒì„± ({len(enhanced_prompts)}ê°œ)")
        
        images = []
        
        for i, prompt in enumerate(enhanced_prompts):
            try:
                result = self.sd_pipeline(
                    prompt=prompt,
                    num_inference_steps=28,
                    guidance_scale=7.0,
                    height=1024,
                    width=1024
                )
                image = result.images[0]
                images.append(image)
                logger.info(f"  ì´ë¯¸ì§€ {i+1}/{len(enhanced_prompts)} ìƒì„± ì™„ë£Œ")
            except Exception as e:
                logger.error(f"  ì´ë¯¸ì§€ {i+1} ìƒì„± ì‹¤íŒ¨: {e}")
                # ë”ë¯¸ ì´ë¯¸ì§€ ì¶”ê°€
                from PIL import Image
                images.append(Image.new('RGB', (1024, 1024), color='black'))
        
        logger.info(f"âœ… {len(images)}ê°œ ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ")
        return images
    
    def calculate_rewards(self, user_prompt: str, enhanced_prompts: List[str], images: List) -> List[float]:
        """ë¦¬ì›Œë“œ ê³„ì‚° (GPU 1ì—ì„œ ì‹¤í–‰)"""
        logger.info(f"ğŸ¯ ë¦¬ì›Œë“œ ê³„ì‚° ({len(images)}ê°œ)")
        
        rewards = []
        
        for i, (enhanced_prompt, image) in enumerate(zip(enhanced_prompts, images)):
            try:
                reward = self.reward_model.calculate_reward(
                    user_prompt, enhanced_prompt, image
                )
                rewards.append(reward)
                logger.info(f"  ë¦¬ì›Œë“œ {i+1}/{len(images)}: {reward:.4f}")
            except Exception as e:
                logger.error(f"  ë¦¬ì›Œë“œ {i+1} ê³„ì‚° ì‹¤íŒ¨: {e}")
                rewards.append(0.1)  # ê¸°ë³¸ ë¦¬ì›Œë“œ
        
        avg_reward = sum(rewards) / len(rewards) if rewards else 0.0
        logger.info(f"âœ… ë¦¬ì›Œë“œ ê³„ì‚° ì™„ë£Œ - í‰ê· : {avg_reward:.4f}")
        return rewards
    
    def collect_rollouts(self, prompts: List[str]) -> List[Dict]:
        """ë¡¤ì•„ì›ƒ ìˆ˜ì§‘"""
        all_experiences = []
        
        for prompt_idx, user_prompt in enumerate(prompts):
            logger.info(f"\nğŸ“ í”„ë¡¬í”„íŠ¸ {prompt_idx + 1}/{len(prompts)}: '{user_prompt}'")
            
            # 1ë‹¨ê³„: í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ ìƒì„± (GPU 0)
            enhanced_data = self.generate_enhanced_prompts(user_prompt, self.config.num_rollouts)
            
            if not enhanced_data:
                logger.warning(f"âš ï¸ í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹¤íŒ¨, ê±´ë„ˆë›°ê¸°")
                continue
            
            enhanced_prompts = [data[0] for data in enhanced_data]
            log_probs = [data[1] for data in enhanced_data]
            
            # 2ë‹¨ê³„: ì´ë¯¸ì§€ ìƒì„± (GPU 2)  
            images = self.generate_images(enhanced_prompts)
            
            # 3ë‹¨ê³„: ë¦¬ì›Œë“œ ê³„ì‚° (GPU 1)
            rewards = self.calculate_rewards(user_prompt, enhanced_prompts, images)
            
            # ê²½í—˜ ì €ì¥
            for enhanced_prompt, log_prob, reward in zip(enhanced_prompts, log_probs, rewards):
                experience = {
                    'user_prompt': user_prompt,
                    'enhanced_prompt': enhanced_prompt,
                    'log_prob': log_prob,
                    'reward': reward,
                    'info': {
                        'original_prompt': user_prompt,
                        'enhanced_prompt': enhanced_prompt,
                        'original_reward': reward,
                        'enhanced_reward': reward
                    }
                }
                all_experiences.append(experience)
        
        logger.info(f"\nğŸ“Š ì´ ìˆ˜ì§‘ëœ ê²½í—˜: {len(all_experiences)}ê°œ")
        return all_experiences
    
    def update_policy(self, experiences: List[Dict]) -> Dict:
        """ì •ì±… ì—…ë°ì´íŠ¸ (GPU 0ì—ì„œ ì‹¤í–‰)"""
        logger.info(f"ğŸ”„ ì •ì±… ì—…ë°ì´íŠ¸ ({len(experiences)}ê°œ ê²½í—˜)")
        
        metrics = self.qwen_model.update_grpo_policy(experiences)
        
        logger.info(f"âœ… ì •ì±… ì—…ë°ì´íŠ¸ ì™„ë£Œ")
        return metrics
    
    def train(self, num_epochs: int = 5):
        """ë©”ì¸ í•™ìŠµ ë£¨í”„"""
        logger.info(f"ğŸš€ Simple GRPO í•™ìŠµ ì‹œì‘ ({num_epochs} ì—í¬í¬)")
        logger.info("=" * 60)
        
        training_prompts = get_training_prompts()
        
        for epoch in range(num_epochs):
            logger.info(f"\nğŸ¯ ì—í¬í¬ {epoch + 1}/{num_epochs}")
            
            try:
                # ë¡¤ì•„ì›ƒ ìˆ˜ì§‘
                experiences = self.collect_rollouts(training_prompts)
                
                if not experiences:
                    logger.warning(f"âš ï¸ ì—í¬í¬ {epoch + 1}: ê²½í—˜ ì—†ìŒ, ê±´ë„ˆë›°ê¸°")
                    continue
                
                # ì •ì±… ì—…ë°ì´íŠ¸
                metrics = self.update_policy(experiences)
                
                # ë©”íŠ¸ë¦­ ë¡œê¹…
                avg_reward = sum(exp['reward'] for exp in experiences) / len(experiences)
                logger.info(f"ğŸ“Š ì—í¬í¬ {epoch + 1} ê²°ê³¼:")
                logger.info(f"  - í‰ê·  ë¦¬ì›Œë“œ: {avg_reward:.4f}")
                logger.info(f"  - ê²½í—˜ ìˆ˜: {len(experiences)}")
                
                if metrics:
                    for key, value in metrics.items():
                        logger.info(f"  - {key}: {value:.4f}")
                
                # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                if self.use_gpu:
                    for gpu_id in range(min(3, torch.cuda.device_count())):
                        with torch.cuda.device(gpu_id):
                            torch.cuda.empty_cache()
                
            except Exception as e:
                logger.error(f"âŒ ì—í¬í¬ {epoch + 1} ì‹¤íŒ¨: {e}")
                continue
        
        logger.info("ğŸ‰ Simple GRPO í•™ìŠµ ì™„ë£Œ!")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger.info("ğŸš€ Simple Multi-GPU GRPO í•™ìŠµ ì‹œì‘")
    logger.info("=" * 60)
    logger.info("GPU í• ë‹¹:")
    logger.info("  GPU 0: QWEN LoRA Training")
    logger.info("  GPU 1: CLIP Reward Calculation")
    logger.info("  GPU 2: Stable Diffusion 3 Image Generation")
    logger.info("=" * 60)
    
    # ì„¤ì •
    config = QWENGRPOConfig(
        learning_rate=5e-7,
        batch_size=2,
        num_rollouts=2,
        max_prompt_length=77,
        max_new_tokens=25,
        temperature=1.2,
        top_p=0.9,
        top_k=100,
        kl_coef=0.02,
        clip_ratio=0.2,
        entropy_coef=0.01,
        save_images=True,
        log_dir="simple_grpo_results"
    )
    
    try:
        # íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
        trainer = SimpleGRPOTrainer(config)
        
        # í•™ìŠµ ì‹¤í–‰
        trainer.train(num_epochs=3)
        
    except Exception as e:
        logger.error(f"âŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                with torch.cuda.device(i):
                    torch.cuda.empty_cache()
            logger.info("ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")

if __name__ == "__main__":
    main()
