#!/usr/bin/env python3
"""
Simple Multi-GPU GRPO Training
GPU 0: QWEN LoRA Training
GPU 1: CLIP Reward Calculation  
GPU 2: Stable Diffusion 3 Image Generation
"""

import torch
import logging
import os
from typing import List, Dict
import time
import matplotlib.pyplot as plt
import numpy as np
from datetime import datetime

from qwen import QWENModel, QWENGRPOConfig
from clip_reward import CLIPReward

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('grpo_training.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def load_stable_diffusion_pipeline(device="cuda:2"):
    """Stable Diffusion 3 íŒŒì´í”„ë¼ì¸ ë¡œë“œ (GPU 2ë²ˆ ì „ìš©)"""
    try:
        from diffusers import StableDiffusion3Pipeline
        
        logger.info(f"ğŸ¨ SD3 íŒŒì´í”„ë¼ì¸ ë¡œë”© ì¤‘... ({device})")
        
        pipe = StableDiffusion3Pipeline.from_pretrained(
            "stabilityai/stable-diffusion-3-medium-diffusers",
            torch_dtype=torch.float16,
            use_safetensors=True
        )
        
        # ì§€ì •ëœ GPUë¡œ ì´ë™
        pipe = pipe.to(device)
        logger.info(f"âœ… SD3 íŒŒì´í”„ë¼ì¸ì„ {device}ë¡œ ì´ë™")
        
        # ë©”ëª¨ë¦¬ ìµœì í™”
        pipe.enable_model_cpu_offload()
        pipe.enable_attention_slicing()
        pipe.set_progress_bar_config(disable=True)
        
        logger.info("âœ… Stable Diffusion 3 íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì™„ë£Œ")
        return pipe
        
    except Exception as e:
        logger.error(f"âŒ SD3 íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise

def get_training_prompts():
    """í•™ìŠµìš© í”„ë¡¬í”„íŠ¸ ë°ì´í„°ì…‹"""
    import random
    
    selected_prompts = [
        "a beautiful cat sitting on a chair",
        "sunset over mountains with golden light", 
        "abstract art painting with vibrant colors",
        "portrait of a woman with flowing hair",
        "futuristic city skyline at night",
        "red apple on blue table with green background",
        "transparent glass sphere floating in purple space",
        "crowded marketplace with many people and colorful stalls"
    ]
    
    random.shuffle(selected_prompts)
    return selected_prompts

class SimpleGRPOTrainer:
    """ê°„ë‹¨í•œ ë©€í‹° GPU GRPO íŠ¸ë ˆì´ë„ˆ"""
    
    def __init__(self, config: QWENGRPOConfig):
        self.config = config
        
        # ë¡œê¹… ë””ë ‰í† ë¦¬ ì„¤ì •
        self.log_dir = config.log_dir
        os.makedirs(self.log_dir, exist_ok=True)
        # episodes í´ë”ë§Œ ìƒì„± (ì´ë¯¸ì§€ì™€ í”Œë¡¯ ëª¨ë‘ ì—¬ê¸°ì— ì €ì¥)
        os.makedirs(os.path.join(self.log_dir, "episodes"), exist_ok=True)
        
        # í•™ìŠµ ë©”íŠ¸ë¦­ ì¶”ì 
        self.training_metrics = {
            'epoch_rewards': [],
            'policy_losses': [],
            'kl_divergences': [],
            'advantages': [],
            'epoch_times': []
        }
        
        # GPU ê°€ìš©ì„± í™•ì¸
        if not torch.cuda.is_available():
            logger.warning("âš ï¸ CUDA ì‚¬ìš© ë¶ˆê°€ - CPUë¡œ ì‹¤í–‰")
            self.use_gpu = False
        elif torch.cuda.device_count() < 3:
            logger.warning(f"âš ï¸ GPU {torch.cuda.device_count()}ê°œë§Œ ì‚¬ìš© ê°€ëŠ¥ (ê¶Œì¥: 3ê°œ)")
            self.use_gpu = True
        else:
            self.use_gpu = True
            logger.info(f"ğŸ¯ ì‚¬ìš© ê°€ëŠ¥í•œ GPU: {torch.cuda.device_count()}ê°œ")
            for i in range(min(3, torch.cuda.device_count())):
                logger.info(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
        
        # ëª¨ë¸ë“¤ ì´ˆê¸°í™”
        self._init_models()
        
    def _init_models(self):
        """ê° GPUë³„ ëª¨ë¸ ì´ˆê¸°í™” - ë©”ëª¨ë¦¬ ìµœì í™”"""
        logger.info("ğŸ”§ ëª¨ë¸ë“¤ ì´ˆê¸°í™” ì¤‘... (ë©”ëª¨ë¦¬ ìµœì í™”)")
        
        # GPU 0: QWEN LoRA ëª¨ë¸ë§Œ - 7B ëª¨ë¸ ì‚¬ìš©
        qwen_device = "cuda:0" if self.use_gpu else "cpu"
        logger.info(f"ğŸ§  {qwen_device}: QWEN 7B LoRA ëª¨ë¸ ë¡œë”©...")
        self.qwen_model = QWENModel(
            model_name="Qwen/Qwen2-VL-7B-Instruct",  # 2B â†’ 7Bë¡œ ë³€ê²½
            device=qwen_device,
            temperature=0.7,
            grpo_config=self.config
        )
        logger.info(f"âœ… QWEN 7B ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({qwen_device})")
        
        # GPU 1: CLIP ë¦¬ì›Œë“œ ëª¨ë¸ë§Œ
        clip_device = "cuda:1" if self.use_gpu and torch.cuda.device_count() > 1 else "cuda:0" if self.use_gpu else "cpu"
        logger.info(f"ğŸ¯ {clip_device}: CLIP ë¦¬ì›Œë“œ ëª¨ë¸ ë¡œë”©...")
        self.reward_model = CLIPReward(device=clip_device)
        self.clip_device = clip_device
        logger.info(f"âœ… CLIP ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({clip_device})")
        
        # GPU 2: Stable Diffusion 3ë§Œ (ì´ë¯¸ì§€ ìƒì„± ì „ìš©)
        sd_device = "cuda:2" if self.use_gpu and torch.cuda.device_count() > 2 else "cuda:0" if self.use_gpu else "cpu"
        logger.info(f"ğŸ¨ {sd_device}: SD3 íŒŒì´í”„ë¼ì¸ ë¡œë”©... (ì´ë¯¸ì§€ ìƒì„± ì „ìš©)")
        self.sd_pipeline = load_stable_diffusion_pipeline(device=sd_device)
        self.sd_device = sd_device
        logger.info(f"âœ… SD3 íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì™„ë£Œ ({sd_device})")
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if self.use_gpu:
            for i in range(torch.cuda.device_count()):
                with torch.cuda.device(i):
                    torch.cuda.empty_cache()
            logger.info("ğŸ§¹ ì´ˆê¸°í™” í›„ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
        
        logger.info("ğŸ¯ ëª¨ë“  ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ!")
        logger.info("ğŸ“‹ GPU í• ë‹¹:")
        logger.info(f"  GPU 0: QWEN 7B LoRA ëª¨ë¸ ({qwen_device})")
        logger.info(f"  GPU 1: CLIP ë¦¬ì›Œë“œ ëª¨ë¸ + Reference 7B ëª¨ë¸ ({clip_device})")
        logger.info(f"  GPU 2: SD3 ì´ë¯¸ì§€ ìƒì„± ({sd_device})")
        logger.info("ğŸ”„ ì´ë¯¸ì§€ëŠ” GPU 2ì—ì„œ ìƒì„± í›„ GPU 1ë¡œ ì´ë™í•˜ì—¬ ë¦¬ì›Œë“œ ê³„ì‚°")
        logger.info("ğŸ”„ Reference ëª¨ë¸ì€ GPU 1ì—ì„œ KL penalty ê³„ì‚°")
        
    def generate_enhanced_prompts(self, user_prompt: str, num_rollouts: int) -> List[tuple]:
        """í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ ìƒì„± (GPU 0ì—ì„œ ì‹¤í–‰)"""
        logger.info(f"ğŸ§  í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ ìƒì„± ({num_rollouts}ê°œ)")
        
        enhanced_data = []
        
        for i in range(num_rollouts):
            try:
                enhanced_prompt, log_prob = self.qwen_model.generate_grpo_enhanced_prompt(user_prompt)
                enhanced_data.append((enhanced_prompt, log_prob))
                logger.info(f"  ìƒì„± {i+1}/{num_rollouts}: '{enhanced_prompt[:50]}...' (log_prob: {log_prob:.4f})")
            except Exception as e:
                logger.error(f"  ìƒì„± {i+1} ì‹¤íŒ¨: {e}")
                continue
        
        logger.info(f"âœ… {len(enhanced_data)}ê°œ í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ")
        return enhanced_data
    
    def generate_images(self, enhanced_prompts: List[str]) -> List:
        """ì´ë¯¸ì§€ ìƒì„± (GPU 2ì—ì„œ ì‹¤í–‰) - ë‹¨ì¼ ì²˜ë¦¬"""
        logger.info(f"ğŸ¨ ì´ë¯¸ì§€ ìƒì„± ({len(enhanced_prompts)}ê°œ)")
        
        images = []
        
        for i, prompt in enumerate(enhanced_prompts):
            try:
                result = self.sd_pipeline(
                    prompt=prompt,
                    num_inference_steps=28,
                    guidance_scale=7.0,
                    height=1024,
                    width=1024
                )
                image = result.images[0]
                images.append(image)
                logger.info(f"  ì´ë¯¸ì§€ {i+1}/{len(enhanced_prompts)} ìƒì„± ì™„ë£Œ")
            except Exception as e:
                logger.error(f"  ì´ë¯¸ì§€ {i+1} ìƒì„± ì‹¤íŒ¨: {e}")
                # ë”ë¯¸ ì´ë¯¸ì§€ ì¶”ê°€
                from PIL import Image
                images.append(Image.new('RGB', (1024, 1024), color='black'))
        
        logger.info(f"âœ… {len(images)}ê°œ ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ")
        return images
    
    def generate_images_batch(self, enhanced_prompts: List[str]) -> List:
        """ë°°ì¹˜ ì´ë¯¸ì§€ ìƒì„± (GPU 2ì—ì„œ ì‹¤í–‰) - ë°°ì¹˜ ìµœì í™”"""
        logger.info(f"ğŸ¨ ë°°ì¹˜ ì´ë¯¸ì§€ ìƒì„± ({len(enhanced_prompts)}ê°œ)")
        
        images = []
        
        try:
            # SD3ëŠ” ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ì§€ì›í•˜ì§€ë§Œ ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ê°œë³„ ì²˜ë¦¬
            for i, prompt in enumerate(enhanced_prompts):
                try:
                    result = self.sd_pipeline(
                        prompt=prompt,
                        num_inference_steps=28,
                        guidance_scale=7.0,
                        height=1024,
                        width=1024
                    )
                    image = result.images[0]
                    images.append(image)
                    logger.info(f"  ë°°ì¹˜ ì´ë¯¸ì§€ {i+1}/{len(enhanced_prompts)} ìƒì„± ì™„ë£Œ")
                except Exception as e:
                    logger.error(f"  ë°°ì¹˜ ì´ë¯¸ì§€ {i+1} ìƒì„± ì‹¤íŒ¨: {e}")
                    # ë”ë¯¸ ì´ë¯¸ì§€ ì¶”ê°€
                    from PIL import Image
                    images.append(Image.new('RGB', (1024, 1024), color='black'))
        
        except Exception as e:
            logger.error(f"âŒ ë°°ì¹˜ ì´ë¯¸ì§€ ìƒì„± ì „ì²´ ì‹¤íŒ¨: {e}")
            # ì „ì²´ ì‹¤íŒ¨ì‹œ ë”ë¯¸ ì´ë¯¸ì§€ë“¤
            from PIL import Image
            images = [Image.new('RGB', (1024, 1024), color='black') for _ in enhanced_prompts]
        
        logger.info(f"âœ… ë°°ì¹˜ {len(images)}ê°œ ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ")
        return images
    
    def calculate_rewards(self, user_prompt: str, enhanced_prompts: List[str], images: List) -> List[float]:
        """ë¦¬ì›Œë“œ ê³„ì‚° (GPU 1ì—ì„œ ì‹¤í–‰) - ë‹¨ì¼ ì²˜ë¦¬"""
        logger.info(f"ğŸ¯ ë¦¬ì›Œë“œ ê³„ì‚° ({len(images)}ê°œ)")
        
        rewards = []
        
        for i, (enhanced_prompt, image) in enumerate(zip(enhanced_prompts, images)):
            try:
                reward = self.reward_model.calculate_reward(
                    user_prompt, enhanced_prompt, image
                )
                rewards.append(reward)
                logger.info(f"  ë¦¬ì›Œë“œ {i+1}/{len(images)}: {reward:.4f}")
            except Exception as e:
                logger.error(f"  ë¦¬ì›Œë“œ {i+1} ê³„ì‚° ì‹¤íŒ¨: {e}")
                rewards.append(0.1)  # ê¸°ë³¸ ë¦¬ì›Œë“œ
        
        avg_reward = sum(rewards) / len(rewards) if rewards else 0.0
        logger.info(f"âœ… ë¦¬ì›Œë“œ ê³„ì‚° ì™„ë£Œ - í‰ê· : {avg_reward:.4f}")
        return rewards
    
    def calculate_rewards_batch(self, user_prompt: str, enhanced_prompts: List[str], images: List) -> List[float]:
        """ë°°ì¹˜ ë¦¬ì›Œë“œ ê³„ì‚° - ì´ë¯¸ì§€ë¥¼ CLIP GPUë¡œ ì´ë™"""
        logger.info(f"ğŸ¯ ë°°ì¹˜ ë¦¬ì›Œë“œ ê³„ì‚° ({len(images)}ê°œ) - Original User Prompt ì‚¬ìš©")
        logger.info(f"ğŸ“ ì‚¬ìš©ëœ Original Prompt: '{user_prompt}'")
        logger.info(f"ğŸ”„ ì´ë¯¸ì§€ë¥¼ GPU {self.sd_device} â†’ {self.clip_device}ë¡œ ì´ë™")
        
        try:
            # ì´ë¯¸ì§€ë“¤ì€ ì´ë¯¸ PIL Image í˜•íƒœì´ë¯€ë¡œ ì§ì ‘ CLIPìœ¼ë¡œ ì „ë‹¬ ê°€ëŠ¥
            # (PIL ImageëŠ” CPU ë©”ëª¨ë¦¬ì— ìˆìœ¼ë¯€ë¡œ GPU ê°„ ì´ë™ ë¶ˆí•„ìš”)
            
            # CLIP ë°°ì¹˜ ì²˜ë¦¬ ì‚¬ìš© - original user promptë¡œ ê³„ì‚°
            rewards = self.reward_model.calculate_batch_rewards(
                user_prompt,  # â­ ì¤‘ìš”: original user prompt ì‚¬ìš© (enhanced ì•„ë‹˜)
                enhanced_prompts,
                images  # PIL Images - CLIPì—ì„œ ìë™ìœ¼ë¡œ ì ì ˆí•œ GPUë¡œ ì²˜ë¦¬
            )
            
            avg_reward = sum(rewards) / len(rewards) if rewards else 0.0
            logger.info(f"âœ… ë°°ì¹˜ ë¦¬ì›Œë“œ ê³„ì‚° ì™„ë£Œ - í‰ê· : {avg_reward:.4f}")
            logger.info(f"ğŸ” CLIP ìœ ì‚¬ë„ëŠ” Original User Prompt '{user_prompt}'ì™€ ìƒì„±ëœ ì´ë¯¸ì§€ ê°„ ê³„ì‚°ë¨")
            logger.info(f"ğŸ“Š ì´ë¯¸ì§€ ì²˜ë¦¬: SD3 GPU {self.sd_device} â†’ CLIP GPU {self.clip_device}")
            
            return rewards
            
        except Exception as e:
            logger.error(f"âŒ ë°°ì¹˜ ë¦¬ì›Œë“œ ê³„ì‚° ì‹¤íŒ¨: {e}")
            # ì—ëŸ¬ ì‹œ ê°œë³„ ê³„ì‚°ìœ¼ë¡œ fallback
            logger.info("ğŸ”„ ê°œë³„ ë¦¬ì›Œë“œ ê³„ì‚°ìœ¼ë¡œ fallback")
            return self.calculate_rewards(user_prompt, enhanced_prompts, images)
    
    def calculate_batch_clip_rewards(self, user_prompt: str, enhanced_prompts: List[str], images: List) -> List[float]:
        """ë°°ì¹˜ë¡œ CLIP ë¦¬ì›Œë“œ ê³„ì‚°"""
        if not images or self.reward_model is None:
            return [0.0] * len(enhanced_prompts)
        
        try:
            # ë°°ì¹˜ë¡œ ë¦¬ì›Œë“œ ê³„ì‚°
            rewards = []
            for i, image in enumerate(images):
                enhanced_prompt = enhanced_prompts[i] if i < len(enhanced_prompts) else user_prompt
                reward = self.reward_model.calculate_reward(user_prompt, enhanced_prompt, image)
                rewards.append(reward)
            
            return rewards
            
        except Exception as e:
            logger.error(f"âŒ ë°°ì¹˜ CLIP ë¦¬ì›Œë“œ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return [0.0] * len(enhanced_prompts)
    
    def calculate_detailed_rewards_batch(self, user_prompt: str, enhanced_prompts: List[str], images: List) -> List[dict]:
        """ë°°ì¹˜ë¡œ ìƒì„¸ ë¦¬ì›Œë“œ ê³„ì‚° (CLIP, Aesthetic, Semantic êµ¬ë¶„)"""
        if not images or self.reward_model is None:
            return [{'total_reward': 0.0, 'clip_score': 0.0, 'aesthetic_score': 0.0, 
                    'semantic_similarity': 0.0, 'clip_penalty': 0.0, 'semantic_penalty': 0.0}] * len(enhanced_prompts)
        
        try:
            detailed_rewards = []
            for i, (enhanced_prompt, image) in enumerate(zip(enhanced_prompts, images)):
                # CLIPReward.calculate_reward()ë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ì²´ ë¦¬ì›Œë“œ ê³„ì‚°
                total_reward = self.reward_model.calculate_reward(user_prompt, enhanced_prompt, image)
                
                # ê°œë³„ êµ¬ì„± ìš”ì†Œ ê³„ì‚° (ë¶„ì„ìš©)
                # 1. CLIP ìœ ì‚¬ë„ ì§ì ‘ ê³„ì‚°
                inputs = self.reward_model.processor(
                    text=[user_prompt], 
                    images=[image], 
                    return_tensors="pt", 
                    padding=True
                ).to(self.reward_model.device)
                
                with torch.no_grad():
                    outputs = self.reward_model.model(**inputs)
                    image_embeds = outputs.image_embeds
                    text_embeds = outputs.text_embeds
                    image_embeds = image_embeds / image_embeds.norm(dim=-1, keepdim=True)
                    text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)
                    clip_similarity = torch.sum(text_embeds * image_embeds, dim=-1).item()
                
                # 2. Aesthetic score ì§ì ‘ ê³„ì‚°
                pixel_values = (
                    self.reward_model.aesthetic_predictor_preprocessor(images=image, return_tensors="pt")
                    .pixel_values.to(torch.float16).to(self.reward_model.device)
                )
                with torch.inference_mode():
                    aesthetic_score = self.reward_model.aesthetic_predictor_model(pixel_values).logits.squeeze().item()
                
                # 3. Semantic similarity ì§ì ‘ ê³„ì‚°
                original_embed = self.reward_model.sentence_transformer_model.encode(user_prompt, convert_to_tensor=True)
                enhanced_embed = self.reward_model.sentence_transformer_model.encode(enhanced_prompt, convert_to_tensor=True)
                from sentence_transformers import util
                semantic_similarity = util.cos_sim(original_embed, enhanced_embed).item()
                
                # 4. íŒ¨ë„í‹° ê³„ì‚° (CLIPRewardì™€ ë™ì¼í•œ ë°©ì‹)
                clip_pen_threshold = 0.28
                aesthetic_weight_factor = 20.0
                semantic_sim_threshold = 0.7
                
                clip_penalty = aesthetic_weight_factor * min(clip_similarity - clip_pen_threshold, 0)
                semantic_penalty = -10 * (semantic_sim_threshold - semantic_similarity) if semantic_similarity < semantic_sim_threshold else 0.0
                
                detailed_reward = {
                    'total_reward': total_reward,  # CLIPRewardì—ì„œ ê³„ì‚°ëœ ê°’ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    'clip_score': clip_similarity,  # CLIP ìœ ì‚¬ë„ (0~1)
                    'aesthetic_score': aesthetic_score,  # ë¯¸ì  ì ìˆ˜
                    'semantic_similarity': semantic_similarity,  # ì˜ë¯¸ì  ìœ ì‚¬ë„ (0~1)
                    'clip_penalty': clip_penalty,  # CLIP íŒ¨ë„í‹° (ìŒìˆ˜ ë˜ëŠ” 0)
                    'semantic_penalty': semantic_penalty  # ì˜ë¯¸ì  íŒ¨ë„í‹° (ìŒìˆ˜ ë˜ëŠ” 0)
                }
                detailed_rewards.append(detailed_reward)
            
            return detailed_rewards
            
        except Exception as e:
            logger.error(f"âŒ ìƒì„¸ ë¦¬ì›Œë“œ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return [{'total_reward': 0.0, 'clip_score': 0.0, 'aesthetic_score': 0.0, 
                    'semantic_similarity': 0.0, 'clip_penalty': 0.0, 'semantic_penalty': 0.0}] * len(enhanced_prompts)
    
    def calculate_aesthetic_score(self, image) -> float:
        """ì´ë¯¸ì§€ ë¯¸ì  í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)"""
        try:
            # PIL Imageë¥¼ numpy arrayë¡œ ë³€í™˜
            import numpy as np
            img_array = np.array(image)
            
            # ìƒ‰ìƒ ë‹¤ì–‘ì„± ê³„ì‚°
            if len(img_array.shape) == 3:
                color_variance = np.var(img_array, axis=(0, 1)).mean()
                color_diversity = min(1.0, color_variance / 1000.0)
            else:
                color_diversity = 0.5
            
            # ëŒ€ë¹„ ê³„ì‚°
            gray = np.mean(img_array, axis=2) if len(img_array.shape) == 3 else img_array
            contrast = np.std(gray) / 255.0
            
            # ì „ì²´ì ì¸ ë°ê¸° ë¶„í¬
            brightness = np.mean(gray) / 255.0
            brightness_score = 1.0 - abs(brightness - 0.5) * 2  # ì¤‘ê°„ ë°ê¸°ê°€ ìµœì 
            
            # ì¢…í•© ì ìˆ˜
            aesthetic_score = (color_diversity * 0.4 + contrast * 0.4 + brightness_score * 0.2)
            return min(1.0, max(0.0, aesthetic_score))
            
        except Exception as e:
            logger.warning(f"âš ï¸ ë¯¸ì  ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5  # ê¸°ë³¸ê°’
    
    def calculate_semantic_similarity(self, user_prompt: str, enhanced_prompt: str, image) -> float:
        """ì˜ë¯¸ì  ìœ ì‚¬ì„± ê³„ì‚°"""
        try:
            # CLIPì„ ì‚¬ìš©í•´ì„œ ì›ë³¸ í”„ë¡¬í”„íŠ¸ì™€ í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ì˜ ì´ë¯¸ì§€ ìœ ì‚¬ì„± ë¹„êµ
            if self.reward_model is None:
                return 0.5
            
            original_similarity = self.reward_model.calculate_reward(user_prompt, user_prompt, image)
            enhanced_similarity = self.reward_model.calculate_reward(enhanced_prompt, enhanced_prompt, image)
            
            # ì›ë³¸ í”„ë¡¬í”„íŠ¸ì™€ì˜ ìœ ì‚¬ì„±ì„ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°
            # í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ê°€ ì›ë³¸ ì˜ë¯¸ë¥¼ ìœ ì§€í•˜ë©´ì„œ ê°œì„ ë˜ì—ˆëŠ”ì§€ í‰ê°€
            semantic_score = original_similarity * 0.7 + enhanced_similarity * 0.3
            
            return min(1.0, max(0.0, semantic_score))
            
        except Exception as e:
            logger.warning(f"âš ï¸ ì˜ë¯¸ì  ìœ ì‚¬ì„± ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5  # ê¸°ë³¸ê°’
    
    def make_safe_filename(self, text: str, max_length: int = 50) -> str:
        """í…ìŠ¤íŠ¸ë¥¼ ì•ˆì „í•œ íŒŒì¼ëª…ìœ¼ë¡œ ë³€í™˜"""
        import re
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ê³µë°±ì„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€ê²½
        safe_text = re.sub(r'[<>:"/\\|?*]', '', text)
        safe_text = re.sub(r'\s+', '_', safe_text)
        safe_text = safe_text.strip('_')
        
        # ê¸¸ì´ ì œí•œ
        if len(safe_text) > max_length:
            safe_text = safe_text[:max_length].rstrip('_')
        
        # ë¹ˆ ë¬¸ìì—´ ë°©ì§€
        if not safe_text:
            safe_text = "unknown_prompt"
        
        return safe_text
    
    def collect_rollouts(self, prompts: List[str]) -> List[Dict]:
        """ë°°ì¹˜ ë¡¤ì•„ì›ƒ ìˆ˜ì§‘ (Group-relative ë°©ì‹)"""
        all_experiences = []
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        batch_size = min(len(prompts), self.config.batch_size)
        
        for batch_start in range(0, len(prompts), batch_size):
            batch_prompts = prompts[batch_start:batch_start + batch_size]
            logger.info(f"\nğŸ“¦ ë°°ì¹˜ {batch_start//batch_size + 1}: {len(batch_prompts)}ê°œ í”„ë¡¬í”„íŠ¸")
            
            batch_experiences = self.collect_batch_rollouts(batch_prompts)
            all_experiences.extend(batch_experiences)
        
        logger.info(f"\nğŸ“Š ì´ ìˆ˜ì§‘ëœ ê²½í—˜: {len(all_experiences)}ê°œ")
        return all_experiences
    
    def collect_batch_rollouts(self, batch_prompts: List[str]) -> List[Dict]:
        """ë‹¨ì¼ ë°°ì¹˜ ë¡¤ì•„ì›ƒ ìˆ˜ì§‘"""
        batch_experiences = []
        
        for prompt_idx, user_prompt in enumerate(batch_prompts):
            logger.info(f"ğŸ“ í”„ë¡¬í”„íŠ¸ {prompt_idx + 1}/{len(batch_prompts)}: '{user_prompt}'")
            
            # 1ë‹¨ê³„: í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ ìƒì„± (GPU 0)
            enhanced_data = self.generate_enhanced_prompts(user_prompt, self.config.num_rollouts)
            
            if not enhanced_data:
                logger.warning(f"âš ï¸ í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹¤íŒ¨, ê±´ë„ˆë›°ê¸°")
                continue
            
            enhanced_prompts = [data[0] for data in enhanced_data]
            log_probs = [data[1] for data in enhanced_data]
            
            # 2ë‹¨ê³„: ë°°ì¹˜ ì´ë¯¸ì§€ ìƒì„± (GPU 2)  
            images = self.generate_images_batch(enhanced_prompts)
            
            # 3ë‹¨ê³„: ë°°ì¹˜ ë¦¬ì›Œë“œ ê³„ì‚° (GPU 1) - original user prompt ì‚¬ìš©
            rewards = self.calculate_rewards_batch(user_prompt, enhanced_prompts, images)
            
            # ê²½í—˜ ì €ì¥
            for enhanced_prompt, log_prob, reward in zip(enhanced_prompts, log_probs, rewards):
                experience = {
                    'user_prompt': user_prompt,
                    'enhanced_prompt': enhanced_prompt,
                    'log_prob': log_prob,
                    'reward': reward,
                    'info': {
                        'original_prompt': user_prompt,
                        'enhanced_prompt': enhanced_prompt,
                        'clip_reward': reward  # CLIPì€ original user promptë¡œ ê³„ì‚°ë¨
                    }
                }
                batch_experiences.append(experience)
        
        return batch_experiences
    
    def collect_rollouts_with_logging(self, prompts: List[str], epoch: int) -> List[Dict]:
        """ë¡œê¹… ê¸°ëŠ¥ì´ í¬í•¨ëœ ë°°ì¹˜ ë¡¤ì•„ì›ƒ ìˆ˜ì§‘"""
        all_experiences = []
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        batch_size = min(len(prompts), self.config.batch_size)
        
        for batch_start in range(0, len(prompts), batch_size):
            batch_prompts = prompts[batch_start:batch_start + batch_size]
            logger.info(f"\nğŸ“¦ ì—í¬í¬ {epoch} ë°°ì¹˜ {batch_start//batch_size + 1}: {len(batch_prompts)}ê°œ í”„ë¡¬í”„íŠ¸")
            
            batch_experiences = []
            
            for prompt_idx, user_prompt in enumerate(batch_prompts):
                logger.info(f"ğŸ“ í”„ë¡¬í”„íŠ¸ {prompt_idx + 1}/{len(batch_prompts)}: '{user_prompt}'")
                
                # 1ë‹¨ê³„: í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ ìƒì„± (GPU 0)
                enhanced_data = self.generate_enhanced_prompts(user_prompt, self.config.num_rollouts)
                
                if not enhanced_data:
                    logger.warning(f"âš ï¸ í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹¤íŒ¨, ê±´ë„ˆë›°ê¸°")
                    continue
                
                enhanced_prompts = [data[0] for data in enhanced_data]
                log_probs = [data[1] for data in enhanced_data]
                
                # 2ë‹¨ê³„: ë°°ì¹˜ ì´ë¯¸ì§€ ìƒì„± (GPU 2)  
                images = self.generate_images_batch(enhanced_prompts)
                
                # 3ë‹¨ê³„: ë°°ì¹˜ ë¦¬ì›Œë“œ ê³„ì‚° (GPU 1) - original user prompt ì‚¬ìš©
                rewards = self.calculate_rewards_batch(user_prompt, enhanced_prompts, images)
                
                # 4ë‹¨ê³„: ì´ë¯¸ì§€ ì €ì¥ (ë¡œê¹…)
                if self.config.save_images:
                    self.save_episode_images(epoch, user_prompt, enhanced_prompts, images, rewards)
                
                # ê²½í—˜ ì €ì¥
                for enhanced_prompt, log_prob, reward in zip(enhanced_prompts, log_probs, rewards):
                    experience = {
                        'user_prompt': user_prompt,
                        'enhanced_prompt': enhanced_prompt,
                        'log_prob': log_prob,
                        'reward': reward,
                        'info': {
                            'original_prompt': user_prompt,
                            'enhanced_prompt': enhanced_prompt,
                            'clip_reward': reward,  # CLIPì€ original user promptë¡œ ê³„ì‚°ë¨
                            'epoch': epoch
                        }
                    }
                    batch_experiences.append(experience)
            
            all_experiences.extend(batch_experiences)
        
        logger.info(f"\nğŸ“Š ì—í¬í¬ {epoch} ì´ ìˆ˜ì§‘ëœ ê²½í—˜: {len(all_experiences)}ê°œ")
        return all_experiences
    
    def update_policy(self, experiences: List[Dict]) -> Dict:
        """Group-relative ì •ì±… ì—…ë°ì´íŠ¸ (GPU 0ì—ì„œ ì‹¤í–‰)"""
        logger.info(f"ğŸ”„ Group-relative ì •ì±… ì—…ë°ì´íŠ¸ ({len(experiences)}ê°œ ê²½í—˜)")
        
        # Group-relative baseline ê³„ì‚°
        group_baseline_metrics = self.calculate_group_relative_baseline(experiences)
        
        # í–¥ìƒëœ ê²½í—˜ ë°ì´í„°ë¡œ ì •ì±… ì—…ë°ì´íŠ¸
        enhanced_experiences = self.apply_group_relative_advantages(experiences, group_baseline_metrics)
        
        # QWEN ëª¨ë¸ ì—…ë°ì´íŠ¸
        metrics = self.qwen_model.update_grpo_policy(enhanced_experiences)
        
        # Group-relative ë©”íŠ¸ë¦­ ì¶”ê°€
        metrics.update(group_baseline_metrics)
        
        logger.info(f"âœ… Group-relative ì •ì±… ì—…ë°ì´íŠ¸ ì™„ë£Œ")
        return metrics
    
    def calculate_group_relative_baseline(self, experiences: List[Dict]) -> Dict:
        """Group-relative baseline ê³„ì‚°"""
        if not experiences:
            return {}
        
        # ê·¸ë£¹ë³„ ë¦¬ì›Œë“œ ìˆ˜ì§‘
        user_prompt_groups = {}
        for exp in experiences:
            user_prompt = exp['user_prompt']
            if user_prompt not in user_prompt_groups:
                user_prompt_groups[user_prompt] = []
            user_prompt_groups[user_prompt].append(exp['reward'])
        
        # ê·¸ë£¹ë³„ í†µê³„ ê³„ì‚°
        group_stats = {}
        all_rewards = []
        
        for user_prompt, rewards in user_prompt_groups.items():
            group_mean = sum(rewards) / len(rewards)
            group_std = (sum((r - group_mean) ** 2 for r in rewards) / len(rewards)) ** 0.5
            
            group_stats[user_prompt] = {
                'mean': group_mean,
                'std': group_std,
                'count': len(rewards),
                'rewards': rewards
            }
            all_rewards.extend(rewards)
        
        # ì „ì²´ í†µê³„
        global_mean = sum(all_rewards) / len(all_rewards)
        global_std = (sum((r - global_mean) ** 2 for r in all_rewards) / len(all_rewards)) ** 0.5
        
        logger.info(f"ğŸ“Š Group-relative Baseline í†µê³„:")
        logger.info(f"  ì „ì²´ í‰ê· : {global_mean:.4f} Â± {global_std:.4f}")
        logger.info(f"  ê·¸ë£¹ ìˆ˜: {len(group_stats)}")
        
        for prompt, stats in group_stats.items():
            logger.info(f"  '{prompt[:30]}...': {stats['mean']:.4f} Â± {stats['std']:.4f} (n={stats['count']})")
        
        return {
            'global_mean': global_mean,
            'global_std': global_std,
            'group_stats': group_stats,
            'num_groups': len(group_stats)
        }
    
    def apply_group_relative_advantages(self, experiences: List[Dict], baseline_metrics: Dict) -> List[Dict]:
        """Group-relative advantage ì ìš©"""
        if not baseline_metrics:
            return experiences
        
        enhanced_experiences = []
        group_stats = baseline_metrics['group_stats']
        global_mean = baseline_metrics['global_mean']
        
        for exp in experiences:
            user_prompt = exp['user_prompt']
            reward = exp['reward']
            
            # Group-relative advantage ê³„ì‚°
            if user_prompt in group_stats:
                group_mean = group_stats[user_prompt]['mean']
                group_advantage = reward - group_mean
            else:
                group_advantage = reward - global_mean
            
            # Global advantageë„ ê³„ì‚°
            global_advantage = reward - global_mean
            
            # ê²½í—˜ ë°ì´í„°ì— advantage ì •ë³´ ì¶”ê°€
            enhanced_exp = exp.copy()
            enhanced_exp['group_advantage'] = group_advantage
            enhanced_exp['global_advantage'] = global_advantage
            enhanced_exp['group_baseline'] = group_stats.get(user_prompt, {}).get('mean', global_mean)
            enhanced_exp['global_baseline'] = global_mean
            
            enhanced_experiences.append(enhanced_exp)
        
        logger.info(f"âœ… Group-relative advantage ì ìš© ì™„ë£Œ")
        return enhanced_experiences
    
    def save_episode_images(self, epoch: int, user_prompt: str, enhanced_prompts: List[str], 
                           images: List, rewards: List[float]):
        """ì—í”¼ì†Œë“œë³„ ìƒì„¸ ì´ë¯¸ì§€ ë° ë¹„êµ ë¶„ì„ ì €ì¥"""
        try:
            # ì›ë³¸ í”„ë¡¬í”„íŠ¸ë³„ í´ë” ìƒì„± (ì•ˆì „í•œ í´ë”ëª…ìœ¼ë¡œ ë³€í™˜)
            safe_prompt = self.make_safe_filename(user_prompt)
            prompt_dir = os.path.join(self.log_dir, "episodes", safe_prompt)
            os.makedirs(prompt_dir, exist_ok=True)  # í”„ë¡¬í”„íŠ¸ë³„ í•˜ìœ„ í´ë”ëŠ” í•„ìš”ì‹œì—ë§Œ ìƒì„±
            
            # 1. ì›ë³¸ í”„ë¡¬í”„íŠ¸ë¡œ ì´ë¯¸ì§€ ìƒì„± (ë¹„êµìš©)
            logger.info(f"ğŸ” ì›ë³¸ í”„ë¡¬í”„íŠ¸ë¡œ ë¹„êµ ì´ë¯¸ì§€ ìƒì„±: '{user_prompt}'")
            original_images = self.generate_images_batch([user_prompt])
            original_rewards = self.calculate_detailed_rewards_batch(user_prompt, [user_prompt], original_images)
            
            # 2. í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ ì´ë¯¸ì§€ë“¤ê³¼ í•¨ê»˜ ì €ì¥
            for i, (enhanced_prompt, enhanced_image, enhanced_reward) in enumerate(zip(enhanced_prompts, images, rewards)):
                # ì´ë¯¸ì§€ ì €ì¥
                original_image = original_images[0] if original_images else None
                original_reward = original_rewards[0] if original_rewards else {}
                enhanced_reward_detailed = self.calculate_detailed_rewards_batch(user_prompt, [enhanced_prompt], [enhanced_image])[0]
                
                # ì›ë³¸ ì´ë¯¸ì§€ ì €ì¥
                if original_image:
                    original_path = os.path.join(prompt_dir, f"epoch_{epoch}_sample_{i}_original.png")
                    original_image.save(original_path)
                
                # í–¥ìƒëœ ì´ë¯¸ì§€ ì €ì¥
                enhanced_path = os.path.join(prompt_dir, f"epoch_{epoch}_sample_{i}_enhanced.png")
                enhanced_image.save(enhanced_path)
                
                # ìƒì„¸ ë¶„ì„ í…ìŠ¤íŠ¸ ì €ì¥
                analysis_path = os.path.join(prompt_dir, f"epoch_{epoch}_sample_{i}_analysis.txt")
                with open(analysis_path, 'w', encoding='utf-8') as f:
                    f.write("=" * 60 + "\n")
                    f.write(f"EPISODE {epoch} - SAMPLE {i} ANALYSIS\n")
                    f.write("=" * 60 + "\n\n")
                    
                    f.write("ğŸ“ PROMPTS:\n")
                    f.write(f"Original Prompt: {user_prompt}\n")
                    f.write(f"Enhanced Prompt: {enhanced_prompt}\n\n")
                    
                    f.write("ğŸ¨ IMAGES:\n")
                    f.write(f"Original Image: epoch_{epoch}_sample_{i}_original.png\n")
                    f.write(f"Enhanced Image: epoch_{epoch}_sample_{i}_enhanced.png\n\n")
                    
                    f.write("ğŸ“Š REWARD ANALYSIS:\n")
                    f.write("-" * 40 + "\n")
                    
                    # ì›ë³¸ ë¦¬ì›Œë“œ ë¶„ì„
                    if original_reward:
                        f.write("Original Prompt â†’ Generated Image:\n")
                        f.write(f"  Total Reward: {original_reward.get('total_reward', 0.0):.4f}\n")
                        f.write(f"  CLIP Score: {original_reward.get('clip_score', 0.0):.4f}\n")
                        f.write(f"  Aesthetic Score: {original_reward.get('aesthetic_score', 0.0):.4f}\n")
                        f.write(f"  Semantic Similarity: {original_reward.get('semantic_similarity', 0.0):.4f}\n")
                        f.write(f"  CLIP Penalty: {original_reward.get('clip_penalty', 0.0):.4f}\n")
                        f.write(f"  Semantic Penalty: {original_reward.get('semantic_penalty', 0.0):.4f}\n\n")
                    
                    # í–¥ìƒëœ ë¦¬ì›Œë“œ ë¶„ì„
                    f.write("Enhanced Prompt â†’ Generated Image:\n")
                    f.write(f"  Total Reward: {enhanced_reward_detailed.get('total_reward', 0.0):.4f}\n")
                    f.write(f"  CLIP Score: {enhanced_reward_detailed.get('clip_score', 0.0):.4f}\n")
                    f.write(f"  Aesthetic Score: {enhanced_reward_detailed.get('aesthetic_score', 0.0):.4f}\n")
                    f.write(f"  Semantic Similarity: {enhanced_reward_detailed.get('semantic_similarity', 0.0):.4f}\n")
                    f.write(f"  CLIP Penalty: {enhanced_reward_detailed.get('clip_penalty', 0.0):.4f}\n")
                    f.write(f"  Semantic Penalty: {enhanced_reward_detailed.get('semantic_penalty', 0.0):.4f}\n\n")
                    
                    # ê°œì„ ë„ ë¶„ì„
                    if original_reward:
                        f.write("ğŸš€ IMPROVEMENT ANALYSIS:\n")
                        f.write("-" * 40 + "\n")
                        total_improvement = enhanced_reward_detailed.get('total_reward', 0.0) - original_reward.get('total_reward', 0.0)
                        clip_improvement = enhanced_reward_detailed.get('clip_score', 0.0) - original_reward.get('clip_score', 0.0)
                        aesthetic_improvement = enhanced_reward_detailed.get('aesthetic_score', 0.0) - original_reward.get('aesthetic_score', 0.0)
                        semantic_improvement = enhanced_reward_detailed.get('semantic_similarity', 0.0) - original_reward.get('semantic_similarity', 0.0)
                        
                        f.write(f"Total Reward Change: {total_improvement:+.4f}\n")
                        f.write(f"CLIP Score Change: {clip_improvement:+.4f}\n")
                        f.write(f"Aesthetic Score Change: {aesthetic_improvement:+.4f}\n")
                        f.write(f"Semantic Similarity Change: {semantic_improvement:+.4f}\n\n")
                        
                        # ê°œì„  ìš”ì•½
                        if total_improvement > 0:
                            f.write("âœ… ENHANCEMENT SUCCESSFUL - Overall improvement achieved\n")
                        else:
                            f.write("âŒ ENHANCEMENT FAILED - Overall degradation occurred\n")
                    
                    f.write(f"\nTimestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                    f.write(f"Epoch: {epoch}\n")
            
            logger.info(f"ğŸ’¾ ì—í”¼ì†Œë“œ {epoch} ìƒì„¸ ë¶„ì„ ì €ì¥ ì™„ë£Œ: {len(images)}ê°œ ìƒ˜í”Œ")
            
        except Exception as e:
            logger.error(f"âŒ ì—í”¼ì†Œë“œ ì´ë¯¸ì§€ ì €ì¥ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
    
    def plot_training_metrics(self, epoch: int):
        """í•™ìŠµ ë©”íŠ¸ë¦­ í”Œë¡¯ ìƒì„± - episodes/ í´ë”ì— ì§€ì† ì—…ë°ì´íŠ¸"""
        try:
            # episodes í´ë” (ì´ë¯¸ __init__ì—ì„œ ìƒì„±ë¨)
            episodes_dir = os.path.join(self.log_dir, "episodes")
            
            if not self.training_metrics['epoch_rewards']:
                return
            
            fig, axes = plt.subplots(2, 2, figsize=(16, 12))
            fig.suptitle(f'GRPO Training Progress - Updated at Epoch {epoch}', fontsize=16, fontweight='bold')
            
            # 1. ë¦¬ì›Œë“œ ì¶”ì´ (íŠ¸ë Œë“œ ë¼ì¸ ì¶”ê°€)
            epochs_list = list(range(1, len(self.training_metrics['epoch_rewards']) + 1))
            axes[0, 0].plot(epochs_list, self.training_metrics['epoch_rewards'], 'b-o', linewidth=2, markersize=6)
            
            # íŠ¸ë Œë“œ ë¼ì¸ ì¶”ê°€
            if len(epochs_list) > 1:
                z = np.polyfit(epochs_list, self.training_metrics['epoch_rewards'], 1)
                p = np.poly1d(z)
                axes[0, 0].plot(epochs_list, p(epochs_list), "r--", alpha=0.8, 
                               label=f'Trend: {z[0]:.4f}x + {z[1]:.4f}')
                axes[0, 0].legend()
            
            axes[0, 0].set_title('Average Reward Over Time', fontsize=14, fontweight='bold')
            axes[0, 0].set_xlabel('Epoch', fontsize=12)
            axes[0, 0].set_ylabel('Average Reward', fontsize=12)
            axes[0, 0].grid(True, alpha=0.3)
            
            # 2. Policy Loss ì¶”ì´
            if self.training_metrics['policy_losses']:
                axes[0, 1].plot(epochs_list, self.training_metrics['policy_losses'], 'r-o', linewidth=2, markersize=6)
                axes[0, 1].set_title('Policy Loss Over Time', fontsize=14, fontweight='bold')
                axes[0, 1].set_xlabel('Epoch', fontsize=12)
                axes[0, 1].set_ylabel('Policy Loss', fontsize=12)
                axes[0, 1].grid(True, alpha=0.3)
            
            # 3. KL Divergence ì¶”ì´
            if self.training_metrics['kl_divergences']:
                axes[1, 0].plot(epochs_list, self.training_metrics['kl_divergences'], 'g-o', linewidth=2, markersize=6)
                axes[1, 0].set_title('KL Divergence Over Time', fontsize=14, fontweight='bold')
                axes[1, 0].set_xlabel('Epoch', fontsize=12)
                axes[1, 0].set_ylabel('KL Divergence', fontsize=12)
                axes[1, 0].grid(True, alpha=0.3)
            
            # 4. Advantage ë¶„í¬ (ìµœê·¼ ì—í”¼ì†Œë“œ)
            if self.training_metrics['advantages']:
                recent_advantages = self.training_metrics['advantages'][-50:]  # ìµœê·¼ 50ê°œ
                axes[1, 1].hist(recent_advantages, bins=20, alpha=0.7, color='skyblue', edgecolor='black')
                axes[1, 1].axvline(np.mean(recent_advantages), color='red', linestyle='--', 
                                  label=f'Mean: {np.mean(recent_advantages):.3f}')
                axes[1, 1].legend()
                axes[1, 1].set_title('Recent Advantage Distribution', fontsize=14, fontweight='bold')
                axes[1, 1].set_xlabel('Advantage Value', fontsize=12)
                axes[1, 1].set_ylabel('Frequency', fontsize=12)
                axes[1, 1].grid(True, alpha=0.3)
            else:
                # Advantageê°€ ì—†ìœ¼ë©´ ë¦¬ì›Œë“œ íˆìŠ¤í† ë¦¬ ë°” ì°¨íŠ¸
                axes[1, 1].bar(epochs_list, self.training_metrics['epoch_rewards'], 
                              alpha=0.7, color='lightgreen', edgecolor='black')
                axes[1, 1].set_title('Reward History (Bar Chart)', fontsize=14, fontweight='bold')
                axes[1, 1].set_xlabel('Epoch', fontsize=12)
                axes[1, 1].set_ylabel('Reward', fontsize=12)
                axes[1, 1].grid(True, alpha=0.3)
            
            plt.tight_layout()
            
            # episodes í´ë”ì— ë©”ì¸ í”Œë¡¯ ì €ì¥ (í•­ìƒ ê°™ì€ íŒŒì¼ëª…ìœ¼ë¡œ ì—…ë°ì´íŠ¸)
            main_plot_path = os.path.join(episodes_dir, "training_progress.png")
            plt.savefig(main_plot_path, dpi=300, bbox_inches='tight')
            
            # ì—í¬í¬ë³„ ë°±ì—…ë„ ì €ì¥
            backup_plot_path = os.path.join(episodes_dir, f"training_progress_epoch_{epoch}.png")
            plt.savefig(backup_plot_path, dpi=300, bbox_inches='tight')
            
            plt.close()
            
            logger.info(f"ğŸ“Š í•™ìŠµ ì§„í–‰ í”Œë¡¯ ì—…ë°ì´íŠ¸: {main_plot_path}")
            logger.info(f"ğŸ“Š ì—í¬í¬ ë°±ì—… ì €ì¥: {backup_plot_path}")
            
        except Exception as e:
            logger.error(f"âŒ í”Œë¡¯ ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
    
    def log_epoch_metrics(self, epoch: int, experiences: List[Dict], metrics: Dict):
        """ì—í”¼ì†Œë“œ ë©”íŠ¸ë¦­ ë¡œê¹…"""
        # ë¦¬ì›Œë“œ í†µê³„
        rewards = [exp['reward'] for exp in experiences]
        avg_reward = sum(rewards) / len(rewards) if rewards else 0.0
        
        # ë©”íŠ¸ë¦­ ì €ì¥
        self.training_metrics['epoch_rewards'].append(avg_reward)
        if 'policy_loss' in metrics:
            self.training_metrics['policy_losses'].append(metrics['policy_loss'])
        if 'kl_div' in metrics:
            self.training_metrics['kl_divergences'].append(metrics['kl_div'])
        
        # Advantage ì €ì¥
        for exp in experiences:
            if 'group_advantage' in exp:
                self.training_metrics['advantages'].append(exp['group_advantage'])
        
        # ìƒì„¸ ë¡œê·¸ ì¶œë ¥
        logger.info(f"ğŸ“ˆ ì—í”¼ì†Œë“œ {epoch} ìƒì„¸ ë©”íŠ¸ë¦­:")
        logger.info(f"  ğŸ“Š í‰ê·  ë¦¬ì›Œë“œ: {avg_reward:.4f}")
        logger.info(f"  ğŸ“Š ë¦¬ì›Œë“œ ë²”ìœ„: {min(rewards):.4f} ~ {max(rewards):.4f}")
        logger.info(f"  ğŸ“Š ê²½í—˜ ìˆ˜: {len(experiences)}")
        
        if metrics:
            for key, value in metrics.items():
                logger.info(f"  ğŸ“Š {key}: {value:.4f}")
        
        # CSV ë¡œê·¸ ì €ì¥
        csv_path = os.path.join(self.log_dir, "training_log.csv")
        if not os.path.exists(csv_path):
            with open(csv_path, 'w') as f:
                f.write("epoch,avg_reward,min_reward,max_reward,num_experiences,policy_loss,kl_div\n")
        
        with open(csv_path, 'a') as f:
            policy_loss = metrics.get('policy_loss', 0.0)
            kl_div = metrics.get('kl_div', 0.0)
            f.write(f"{epoch},{avg_reward:.4f},{min(rewards):.4f},{max(rewards):.4f},"
                   f"{len(experiences)},{policy_loss:.4f},{kl_div:.4f}\n")
    
    def train(self, num_epochs: int = 5):
        """ë©”ì¸ í•™ìŠµ ë£¨í”„"""
        logger.info(f"ğŸš€ Simple GRPO í•™ìŠµ ì‹œì‘ ({num_epochs} ì—í¬í¬)")
        logger.info("=" * 60)
        
        training_prompts = get_training_prompts()
        
        for epoch in range(num_epochs):
            epoch_start_time = time.time()
            logger.info(f"\nğŸ¯ ì—í¬í¬ {epoch + 1}/{num_epochs}")
            
            try:
                # ë¡¤ì•„ì›ƒ ìˆ˜ì§‘ (ì´ë¯¸ì§€ ì €ì¥ í¬í•¨)
                experiences = self.collect_rollouts_with_logging(training_prompts, epoch + 1)
                
                if not experiences:
                    logger.warning(f"âš ï¸ ì—í¬í¬ {epoch + 1}: ê²½í—˜ ì—†ìŒ, ê±´ë„ˆë›°ê¸°")
                    continue
                
                # ì •ì±… ì—…ë°ì´íŠ¸ (ê·¸ë¼ë””ì–¸íŠ¸ ì—…ë°ì´íŠ¸ í¬í•¨)
                try:
                    logger.info(f"ğŸ”„ ì—í¬í¬ {epoch + 1}: ì •ì±… ì—…ë°ì´íŠ¸ ì‹œì‘...")
                    metrics = self.update_policy(experiences)
                    logger.info(f"âœ… ì—í¬í¬ {epoch + 1}: ì •ì±… ì—…ë°ì´íŠ¸ ì™„ë£Œ")
                    
                except Exception as update_error:
                    logger.error(f"ğŸ’¥ ì—í¬í¬ {epoch + 1}: ê·¸ë¼ë””ì–¸íŠ¸ ì—…ë°ì´íŠ¸ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ!")
                    logger.error(f"ğŸš¨ ì˜¤ë¥˜ ë‚´ìš©: {update_error}")
                    import traceback
                    traceback.print_exc()
                    
                    # ëª¨ë¸ ìƒíƒœ ì €ì¥ ì‹œë„
                    try:
                        emergency_save_path = os.path.join(self.log_dir, f"emergency_save_epoch_{epoch + 1}")
                        os.makedirs(emergency_save_path, exist_ok=True)
                        self.qwen_model.save_lora_model(emergency_save_path)
                        logger.info(f"ğŸ’¾ ì‘ê¸‰ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {emergency_save_path}")
                    except Exception as save_error:
                        logger.error(f"âŒ ì‘ê¸‰ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {save_error}")
                    
                    # í•™ìŠµ ì¢…ë£Œ
                    logger.error(f"ğŸ›‘ ê·¸ë¼ë””ì–¸íŠ¸ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜ë¡œ ì¸í•œ í•™ìŠµ ì¡°ê¸° ì¢…ë£Œ (ì—í¬í¬ {epoch + 1}/{num_epochs})")
                    logger.info(f"ğŸ“Š ì™„ë£Œëœ ì—í¬í¬: {epoch}/{num_epochs}")
                    break  # í•™ìŠµ ë£¨í”„ ì¢…ë£Œ
                
                # ì—í¬í¬ ì‹œê°„ ê¸°ë¡
                epoch_time = time.time() - epoch_start_time
                self.training_metrics['epoch_times'].append(epoch_time)
                
                # ìƒì„¸ ë©”íŠ¸ë¦­ ë¡œê¹…
                self.log_epoch_metrics(epoch + 1, experiences, metrics)
                
                # í”Œë¡¯ ìƒì„± (ë§¤ ì—í¬í¬ë§ˆë‹¤)
                self.plot_training_metrics(epoch + 1)
                
                # ì ê·¹ì ì¸ GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                if self.use_gpu:
                    # ëª¨ë“  GPUì— ëŒ€í•´ ë©”ëª¨ë¦¬ ì •ë¦¬
                    for gpu_id in range(torch.cuda.device_count()):
                        with torch.cuda.device(gpu_id):
                            torch.cuda.empty_cache()
                    
                    # ì¶”ê°€ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œë„
                    import gc
                    gc.collect()
                    torch.cuda.synchronize()
                
                logger.info(f"â±ï¸ ì—í¬í¬ {epoch + 1} ì™„ë£Œ ì‹œê°„: {epoch_time:.2f}ì´ˆ")
                
            except Exception as e:
                logger.error(f"âŒ ì—í¬í¬ {epoch + 1} ì¼ë°˜ ì˜¤ë¥˜: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        # ìµœì¢… ëª¨ë¸ ì €ì¥
        try:
            final_save_path = os.path.join(self.log_dir, "final_model")
            os.makedirs(final_save_path, exist_ok=True)
            self.qwen_model.save_lora_model(final_save_path)
            logger.info(f"ğŸ’¾ ìµœì¢… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {final_save_path}")
        except Exception as save_error:
            logger.error(f"âŒ ìµœì¢… ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {save_error}")
        
        # í•™ìŠµ ì™„ë£Œ ìš”ì•½
        total_epochs_completed = len(self.training_metrics['epoch_rewards'])
        total_training_time = sum(self.training_metrics['epoch_times']) if self.training_metrics['epoch_times'] else 0
        
        logger.info("ğŸ‰ Simple GRPO í•™ìŠµ ì™„ë£Œ!")
        logger.info("=" * 60)
        logger.info("ğŸ“Š í•™ìŠµ ìš”ì•½:")
        logger.info(f"  ì™„ë£Œëœ ì—í¬í¬: {total_epochs_completed}/{num_epochs}")
        logger.info(f"  ì´ í•™ìŠµ ì‹œê°„: {total_training_time:.2f}ì´ˆ ({total_training_time/60:.1f}ë¶„)")
        
        if self.training_metrics['epoch_rewards']:
            avg_reward = sum(self.training_metrics['epoch_rewards']) / len(self.training_metrics['epoch_rewards'])
            best_reward = max(self.training_metrics['epoch_rewards'])
            logger.info(f"  í‰ê·  ë¦¬ì›Œë“œ: {avg_reward:.4f}")
            logger.info(f"  ìµœê³  ë¦¬ì›Œë“œ: {best_reward:.4f}")
        
        logger.info(f"  ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {self.log_dir}")
        logger.info("=" * 60)

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger.info("ğŸš€ Simple Multi-GPU GRPO í•™ìŠµ ì‹œì‘")
    logger.info("=" * 60)
    logger.info("GPU í• ë‹¹:")
    logger.info("  GPU 0: QWEN LoRA Training")
    logger.info("  GPU 1: CLIP Reward Calculation")
    logger.info("  GPU 2: Stable Diffusion 3 Image Generation")
    logger.info("=" * 60)
    
    # ì„¤ì • - QWEN 7B + ê·¹í•œ ë©”ëª¨ë¦¬ ìµœì í™”
    config = QWENGRPOConfig(
        learning_rate=1e-4,  # 7B ëª¨ë¸ì— ì í•©í•œ í•™ìŠµë¥ 
        batch_size=1,  # ê·¹í•œ ë©”ëª¨ë¦¬ ì ˆì•½ (2 â†’ 1)
        num_rollouts=1,  # ê·¹í•œ ë©”ëª¨ë¦¬ ì ˆì•½ (2 â†’ 1)
        max_prompt_length=77,
        max_new_tokens=20,  # í† í° ìˆ˜ ë” ì œí•œ (30 â†’ 20)
        temperature=1.2,
        top_p=0.9,
        top_k=100,
        kl_coef=0.02,
        clip_ratio=0.2,
        entropy_coef=0.01,
        save_images=True,
        log_dir="grpo_7b_results"  # 7B ëª¨ë¸ ì „ìš© ê²°ê³¼ ë””ë ‰í† ë¦¬
    )
    
    try:
        # íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
        trainer = SimpleGRPOTrainer(config)
        
        # í•™ìŠµ ì‹¤í–‰
        trainer.train(num_epochs=15)
        
    except Exception as e:
        logger.error(f"âŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                with torch.cuda.device(i):
                    torch.cuda.empty_cache()
            logger.info("ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")

if __name__ == "__main__":
    main()
