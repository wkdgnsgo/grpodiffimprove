#!/usr/bin/env python3
"""
Simple Multi-GPU GRPO Training
GPU 0: QWEN LoRA Training
GPU 1: CLIP Reward Calculation  
GPU 2: Stable Diffusion 3 Image Generation
"""

import torch
import logging
import os
from typing import List, Dict
import time
import matplotlib.pyplot as plt
import numpy as np
from datetime import datetime

from qwen import QWENModel, QWENGRPOConfig
from clip_reward import CLIPReward

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('grpo_training.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def load_stable_diffusion_pipeline(device="cuda:2"):
    """Stable Diffusion 3 íŒŒì´í”„ë¼ì¸ ë¡œë“œ (GPU 2ë²ˆ ì „ìš©)"""
    try:
        from diffusers import StableDiffusion3Pipeline
        
        logger.info(f"ğŸ¨ SD3 íŒŒì´í”„ë¼ì¸ ë¡œë”© ì¤‘... ({device})")
        
        pipe = StableDiffusion3Pipeline.from_pretrained(
            "stabilityai/stable-diffusion-3-medium-diffusers",
            torch_dtype=torch.float16,
            use_safetensors=True
        )
        
        # ì§€ì •ëœ GPUë¡œ ì´ë™
        pipe = pipe.to(device)
        logger.info(f"âœ… SD3 íŒŒì´í”„ë¼ì¸ì„ {device}ë¡œ ì´ë™")
        
        # ë©”ëª¨ë¦¬ ìµœì í™”
        pipe.enable_model_cpu_offload()
        pipe.enable_attention_slicing()
        pipe.set_progress_bar_config(disable=True)
        
        logger.info("âœ… Stable Diffusion 3 íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì™„ë£Œ")
        return pipe
        
    except Exception as e:
        logger.error(f"âŒ SD3 íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise

def get_training_prompts():
    """í•™ìŠµìš© í”„ë¡¬í”„íŠ¸ ë°ì´í„°ì…‹"""
    import random
    
    selected_prompts = [
        "a beautiful cat sitting on a chair",
        "sunset over mountains with golden light", 
        "abstract art painting with vibrant colors",
        "portrait of a woman with flowing hair",
        "futuristic city skyline at night",
        "red apple on blue table with green background",
        "transparent glass sphere floating in purple space",
        "crowded marketplace with many people and colorful stalls"
    ]
    
    random.shuffle(selected_prompts)
    return selected_prompts

class SimpleGRPOTrainer:
    """ê°„ë‹¨í•œ ë©€í‹° GPU GRPO íŠ¸ë ˆì´ë„ˆ"""
    
    def __init__(self, config: QWENGRPOConfig):
        self.config = config
        
        # ë¡œê¹… ë””ë ‰í† ë¦¬ ì„¤ì •
        self.log_dir = config.log_dir
        os.makedirs(self.log_dir, exist_ok=True)
        os.makedirs(os.path.join(self.log_dir, "images"), exist_ok=True)
        os.makedirs(os.path.join(self.log_dir, "plots"), exist_ok=True)
        
        # í•™ìŠµ ë©”íŠ¸ë¦­ ì¶”ì 
        self.training_metrics = {
            'epoch_rewards': [],
            'policy_losses': [],
            'kl_divergences': [],
            'advantages': [],
            'epoch_times': []
        }
        
        # GPU ê°€ìš©ì„± í™•ì¸
        if not torch.cuda.is_available():
            logger.warning("âš ï¸ CUDA ì‚¬ìš© ë¶ˆê°€ - CPUë¡œ ì‹¤í–‰")
            self.use_gpu = False
        elif torch.cuda.device_count() < 3:
            logger.warning(f"âš ï¸ GPU {torch.cuda.device_count()}ê°œë§Œ ì‚¬ìš© ê°€ëŠ¥ (ê¶Œì¥: 3ê°œ)")
            self.use_gpu = True
        else:
            self.use_gpu = True
            logger.info(f"ğŸ¯ ì‚¬ìš© ê°€ëŠ¥í•œ GPU: {torch.cuda.device_count()}ê°œ")
            for i in range(min(3, torch.cuda.device_count())):
                logger.info(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
        
        # ëª¨ë¸ë“¤ ì´ˆê¸°í™”
        self._init_models()
        
    def _init_models(self):
        """ê° GPUë³„ ëª¨ë¸ ì´ˆê¸°í™” - ë©”ëª¨ë¦¬ ìµœì í™”"""
        logger.info("ğŸ”§ ëª¨ë¸ë“¤ ì´ˆê¸°í™” ì¤‘... (ë©”ëª¨ë¦¬ ìµœì í™”)")
        
        # GPU 0: QWEN LoRA ëª¨ë¸ë§Œ - ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©
        qwen_device = "cuda:0" if self.use_gpu else "cpu"
        logger.info(f"ğŸ§  {qwen_device}: QWEN LoRA ëª¨ë¸ ë¡œë”©... (ë” ì‘ì€ ëª¨ë¸)")
        self.qwen_model = QWENModel(
            model_name="Qwen/Qwen2-VL-2B-Instruct",  # 7B â†’ 2Bë¡œ ë³€ê²½
            device=qwen_device,
            temperature=0.7,
            grpo_config=self.config
        )
        logger.info(f"âœ… QWEN 2B ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({qwen_device})")
        
        # GPU 1: CLIP ë¦¬ì›Œë“œ ëª¨ë¸ë§Œ
        clip_device = "cuda:1" if self.use_gpu and torch.cuda.device_count() > 1 else "cuda:0" if self.use_gpu else "cpu"
        logger.info(f"ğŸ¯ {clip_device}: CLIP ë¦¬ì›Œë“œ ëª¨ë¸ ë¡œë”©...")
        self.reward_model = CLIPReward(device=clip_device)
        self.clip_device = clip_device
        logger.info(f"âœ… CLIP ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({clip_device})")
        
        # GPU 2: Stable Diffusion 3ë§Œ (ì´ë¯¸ì§€ ìƒì„± ì „ìš©)
        sd_device = "cuda:2" if self.use_gpu and torch.cuda.device_count() > 2 else "cuda:0" if self.use_gpu else "cpu"
        logger.info(f"ğŸ¨ {sd_device}: SD3 íŒŒì´í”„ë¼ì¸ ë¡œë”©... (ì´ë¯¸ì§€ ìƒì„± ì „ìš©)")
        self.sd_pipeline = load_stable_diffusion_pipeline(device=sd_device)
        self.sd_device = sd_device
        logger.info(f"âœ… SD3 íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì™„ë£Œ ({sd_device})")
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if self.use_gpu:
            for i in range(torch.cuda.device_count()):
                with torch.cuda.device(i):
                    torch.cuda.empty_cache()
            logger.info("ğŸ§¹ ì´ˆê¸°í™” í›„ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
        
        logger.info("ğŸ¯ ëª¨ë“  ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ!")
        logger.info("ğŸ“‹ GPU í• ë‹¹:")
        logger.info(f"  GPU 0: QWEN 2B LoRA ëª¨ë¸ ({qwen_device})")
        logger.info(f"  GPU 1: CLIP ë¦¬ì›Œë“œ ëª¨ë¸ + Reference ëª¨ë¸ ({clip_device})")
        logger.info(f"  GPU 2: SD3 ì´ë¯¸ì§€ ìƒì„± ({sd_device})")
        logger.info("ğŸ”„ ì´ë¯¸ì§€ëŠ” GPU 2ì—ì„œ ìƒì„± í›„ GPU 1ë¡œ ì´ë™í•˜ì—¬ ë¦¬ì›Œë“œ ê³„ì‚°")
        logger.info("ğŸ”„ Reference ëª¨ë¸ì€ GPU 1ì—ì„œ KL penalty ê³„ì‚°")
        
    def generate_enhanced_prompts(self, user_prompt: str, num_rollouts: int) -> List[tuple]:
        """í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ ìƒì„± (GPU 0ì—ì„œ ì‹¤í–‰)"""
        logger.info(f"ğŸ§  í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ ìƒì„± ({num_rollouts}ê°œ)")
        
        enhanced_data = []
        
        for i in range(num_rollouts):
            try:
                enhanced_prompt, log_prob = self.qwen_model.generate_grpo_enhanced_prompt(user_prompt)
                enhanced_data.append((enhanced_prompt, log_prob))
                logger.info(f"  ìƒì„± {i+1}/{num_rollouts}: '{enhanced_prompt[:50]}...' (log_prob: {log_prob:.4f})")
            except Exception as e:
                logger.error(f"  ìƒì„± {i+1} ì‹¤íŒ¨: {e}")
                continue
        
        logger.info(f"âœ… {len(enhanced_data)}ê°œ í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ")
        return enhanced_data
    
    def generate_images(self, enhanced_prompts: List[str]) -> List:
        """ì´ë¯¸ì§€ ìƒì„± (GPU 2ì—ì„œ ì‹¤í–‰) - ë‹¨ì¼ ì²˜ë¦¬"""
        logger.info(f"ğŸ¨ ì´ë¯¸ì§€ ìƒì„± ({len(enhanced_prompts)}ê°œ)")
        
        images = []
        
        for i, prompt in enumerate(enhanced_prompts):
            try:
                result = self.sd_pipeline(
                    prompt=prompt,
                    num_inference_steps=28,
                    guidance_scale=7.0,
                    height=1024,
                    width=1024
                )
                image = result.images[0]
                images.append(image)
                logger.info(f"  ì´ë¯¸ì§€ {i+1}/{len(enhanced_prompts)} ìƒì„± ì™„ë£Œ")
            except Exception as e:
                logger.error(f"  ì´ë¯¸ì§€ {i+1} ìƒì„± ì‹¤íŒ¨: {e}")
                # ë”ë¯¸ ì´ë¯¸ì§€ ì¶”ê°€
                from PIL import Image
                images.append(Image.new('RGB', (1024, 1024), color='black'))
        
        logger.info(f"âœ… {len(images)}ê°œ ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ")
        return images
    
    def generate_images_batch(self, enhanced_prompts: List[str]) -> List:
        """ë°°ì¹˜ ì´ë¯¸ì§€ ìƒì„± (GPU 2ì—ì„œ ì‹¤í–‰) - ë°°ì¹˜ ìµœì í™”"""
        logger.info(f"ğŸ¨ ë°°ì¹˜ ì´ë¯¸ì§€ ìƒì„± ({len(enhanced_prompts)}ê°œ)")
        
        images = []
        
        try:
            # SD3ëŠ” ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ì§€ì›í•˜ì§€ë§Œ ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ê°œë³„ ì²˜ë¦¬
            for i, prompt in enumerate(enhanced_prompts):
                try:
                    result = self.sd_pipeline(
                        prompt=prompt,
                        num_inference_steps=28,
                        guidance_scale=7.0,
                        height=1024,
                        width=1024
                    )
                    image = result.images[0]
                    images.append(image)
                    logger.info(f"  ë°°ì¹˜ ì´ë¯¸ì§€ {i+1}/{len(enhanced_prompts)} ìƒì„± ì™„ë£Œ")
                except Exception as e:
                    logger.error(f"  ë°°ì¹˜ ì´ë¯¸ì§€ {i+1} ìƒì„± ì‹¤íŒ¨: {e}")
                    # ë”ë¯¸ ì´ë¯¸ì§€ ì¶”ê°€
                    from PIL import Image
                    images.append(Image.new('RGB', (1024, 1024), color='black'))
        
        except Exception as e:
            logger.error(f"âŒ ë°°ì¹˜ ì´ë¯¸ì§€ ìƒì„± ì „ì²´ ì‹¤íŒ¨: {e}")
            # ì „ì²´ ì‹¤íŒ¨ì‹œ ë”ë¯¸ ì´ë¯¸ì§€ë“¤
            from PIL import Image
            images = [Image.new('RGB', (1024, 1024), color='black') for _ in enhanced_prompts]
        
        logger.info(f"âœ… ë°°ì¹˜ {len(images)}ê°œ ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ")
        return images
    
    def calculate_rewards(self, user_prompt: str, enhanced_prompts: List[str], images: List) -> List[float]:
        """ë¦¬ì›Œë“œ ê³„ì‚° (GPU 1ì—ì„œ ì‹¤í–‰) - ë‹¨ì¼ ì²˜ë¦¬"""
        logger.info(f"ğŸ¯ ë¦¬ì›Œë“œ ê³„ì‚° ({len(images)}ê°œ)")
        
        rewards = []
        
        for i, (enhanced_prompt, image) in enumerate(zip(enhanced_prompts, images)):
            try:
                reward = self.reward_model.calculate_reward(
                    user_prompt, enhanced_prompt, image
                )
                rewards.append(reward)
                logger.info(f"  ë¦¬ì›Œë“œ {i+1}/{len(images)}: {reward:.4f}")
            except Exception as e:
                logger.error(f"  ë¦¬ì›Œë“œ {i+1} ê³„ì‚° ì‹¤íŒ¨: {e}")
                rewards.append(0.1)  # ê¸°ë³¸ ë¦¬ì›Œë“œ
        
        avg_reward = sum(rewards) / len(rewards) if rewards else 0.0
        logger.info(f"âœ… ë¦¬ì›Œë“œ ê³„ì‚° ì™„ë£Œ - í‰ê· : {avg_reward:.4f}")
        return rewards
    
    def calculate_rewards_batch(self, user_prompt: str, enhanced_prompts: List[str], images: List) -> List[float]:
        """ë°°ì¹˜ ë¦¬ì›Œë“œ ê³„ì‚° - ì´ë¯¸ì§€ë¥¼ CLIP GPUë¡œ ì´ë™"""
        logger.info(f"ğŸ¯ ë°°ì¹˜ ë¦¬ì›Œë“œ ê³„ì‚° ({len(images)}ê°œ) - Original User Prompt ì‚¬ìš©")
        logger.info(f"ğŸ“ ì‚¬ìš©ëœ Original Prompt: '{user_prompt}'")
        logger.info(f"ğŸ”„ ì´ë¯¸ì§€ë¥¼ GPU {self.sd_device} â†’ {self.clip_device}ë¡œ ì´ë™")
        
        try:
            # ì´ë¯¸ì§€ë“¤ì€ ì´ë¯¸ PIL Image í˜•íƒœì´ë¯€ë¡œ ì§ì ‘ CLIPìœ¼ë¡œ ì „ë‹¬ ê°€ëŠ¥
            # (PIL ImageëŠ” CPU ë©”ëª¨ë¦¬ì— ìˆìœ¼ë¯€ë¡œ GPU ê°„ ì´ë™ ë¶ˆí•„ìš”)
            
            # CLIP ë°°ì¹˜ ì²˜ë¦¬ ì‚¬ìš© - original user promptë¡œ ê³„ì‚°
            rewards = self.reward_model.calculate_batch_rewards(
                user_prompt,  # â­ ì¤‘ìš”: original user prompt ì‚¬ìš© (enhanced ì•„ë‹˜)
                enhanced_prompts,
                images  # PIL Images - CLIPì—ì„œ ìë™ìœ¼ë¡œ ì ì ˆí•œ GPUë¡œ ì²˜ë¦¬
            )
            
            avg_reward = sum(rewards) / len(rewards) if rewards else 0.0
            logger.info(f"âœ… ë°°ì¹˜ ë¦¬ì›Œë“œ ê³„ì‚° ì™„ë£Œ - í‰ê· : {avg_reward:.4f}")
            logger.info(f"ğŸ” CLIP ìœ ì‚¬ë„ëŠ” Original User Prompt '{user_prompt}'ì™€ ìƒì„±ëœ ì´ë¯¸ì§€ ê°„ ê³„ì‚°ë¨")
            logger.info(f"ğŸ“Š ì´ë¯¸ì§€ ì²˜ë¦¬: SD3 GPU {self.sd_device} â†’ CLIP GPU {self.clip_device}")
            
            return rewards
            
        except Exception as e:
            logger.error(f"âŒ ë°°ì¹˜ ë¦¬ì›Œë“œ ê³„ì‚° ì‹¤íŒ¨: {e}")
            # ì—ëŸ¬ ì‹œ ê°œë³„ ê³„ì‚°ìœ¼ë¡œ fallback
            logger.info("ğŸ”„ ê°œë³„ ë¦¬ì›Œë“œ ê³„ì‚°ìœ¼ë¡œ fallback")
            return self.calculate_rewards(user_prompt, enhanced_prompts, images)
    
    def collect_rollouts(self, prompts: List[str]) -> List[Dict]:
        """ë°°ì¹˜ ë¡¤ì•„ì›ƒ ìˆ˜ì§‘ (Group-relative ë°©ì‹)"""
        all_experiences = []
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        batch_size = min(len(prompts), self.config.batch_size)
        
        for batch_start in range(0, len(prompts), batch_size):
            batch_prompts = prompts[batch_start:batch_start + batch_size]
            logger.info(f"\nğŸ“¦ ë°°ì¹˜ {batch_start//batch_size + 1}: {len(batch_prompts)}ê°œ í”„ë¡¬í”„íŠ¸")
            
            batch_experiences = self.collect_batch_rollouts(batch_prompts)
            all_experiences.extend(batch_experiences)
        
        logger.info(f"\nğŸ“Š ì´ ìˆ˜ì§‘ëœ ê²½í—˜: {len(all_experiences)}ê°œ")
        return all_experiences
    
    def collect_batch_rollouts(self, batch_prompts: List[str]) -> List[Dict]:
        """ë‹¨ì¼ ë°°ì¹˜ ë¡¤ì•„ì›ƒ ìˆ˜ì§‘"""
        batch_experiences = []
        
        for prompt_idx, user_prompt in enumerate(batch_prompts):
            logger.info(f"ğŸ“ í”„ë¡¬í”„íŠ¸ {prompt_idx + 1}/{len(batch_prompts)}: '{user_prompt}'")
            
            # 1ë‹¨ê³„: í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ ìƒì„± (GPU 0)
            enhanced_data = self.generate_enhanced_prompts(user_prompt, self.config.num_rollouts)
            
            if not enhanced_data:
                logger.warning(f"âš ï¸ í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹¤íŒ¨, ê±´ë„ˆë›°ê¸°")
                continue
            
            enhanced_prompts = [data[0] for data in enhanced_data]
            log_probs = [data[1] for data in enhanced_data]
            
            # 2ë‹¨ê³„: ë°°ì¹˜ ì´ë¯¸ì§€ ìƒì„± (GPU 2)  
            images = self.generate_images_batch(enhanced_prompts)
            
            # 3ë‹¨ê³„: ë°°ì¹˜ ë¦¬ì›Œë“œ ê³„ì‚° (GPU 1) - original user prompt ì‚¬ìš©
            rewards = self.calculate_rewards_batch(user_prompt, enhanced_prompts, images)
            
            # ê²½í—˜ ì €ì¥
            for enhanced_prompt, log_prob, reward in zip(enhanced_prompts, log_probs, rewards):
                experience = {
                    'user_prompt': user_prompt,
                    'enhanced_prompt': enhanced_prompt,
                    'log_prob': log_prob,
                    'reward': reward,
                    'info': {
                        'original_prompt': user_prompt,
                        'enhanced_prompt': enhanced_prompt,
                        'clip_reward': reward  # CLIPì€ original user promptë¡œ ê³„ì‚°ë¨
                    }
                }
                batch_experiences.append(experience)
        
        return batch_experiences
    
    def collect_rollouts_with_logging(self, prompts: List[str], epoch: int) -> List[Dict]:
        """ë¡œê¹… ê¸°ëŠ¥ì´ í¬í•¨ëœ ë°°ì¹˜ ë¡¤ì•„ì›ƒ ìˆ˜ì§‘"""
        all_experiences = []
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        batch_size = min(len(prompts), self.config.batch_size)
        
        for batch_start in range(0, len(prompts), batch_size):
            batch_prompts = prompts[batch_start:batch_start + batch_size]
            logger.info(f"\nğŸ“¦ ì—í¬í¬ {epoch} ë°°ì¹˜ {batch_start//batch_size + 1}: {len(batch_prompts)}ê°œ í”„ë¡¬í”„íŠ¸")
            
            batch_experiences = []
            
            for prompt_idx, user_prompt in enumerate(batch_prompts):
                logger.info(f"ğŸ“ í”„ë¡¬í”„íŠ¸ {prompt_idx + 1}/{len(batch_prompts)}: '{user_prompt}'")
                
                # 1ë‹¨ê³„: í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ ìƒì„± (GPU 0)
                enhanced_data = self.generate_enhanced_prompts(user_prompt, self.config.num_rollouts)
                
                if not enhanced_data:
                    logger.warning(f"âš ï¸ í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹¤íŒ¨, ê±´ë„ˆë›°ê¸°")
                    continue
                
                enhanced_prompts = [data[0] for data in enhanced_data]
                log_probs = [data[1] for data in enhanced_data]
                
                # 2ë‹¨ê³„: ë°°ì¹˜ ì´ë¯¸ì§€ ìƒì„± (GPU 2)  
                images = self.generate_images_batch(enhanced_prompts)
                
                # 3ë‹¨ê³„: ë°°ì¹˜ ë¦¬ì›Œë“œ ê³„ì‚° (GPU 1) - original user prompt ì‚¬ìš©
                rewards = self.calculate_rewards_batch(user_prompt, enhanced_prompts, images)
                
                # 4ë‹¨ê³„: ì´ë¯¸ì§€ ì €ì¥ (ë¡œê¹…)
                if self.config.save_images:
                    self.save_episode_images(epoch, user_prompt, enhanced_prompts, images, rewards)
                
                # ê²½í—˜ ì €ì¥
                for enhanced_prompt, log_prob, reward in zip(enhanced_prompts, log_probs, rewards):
                    experience = {
                        'user_prompt': user_prompt,
                        'enhanced_prompt': enhanced_prompt,
                        'log_prob': log_prob,
                        'reward': reward,
                        'info': {
                            'original_prompt': user_prompt,
                            'enhanced_prompt': enhanced_prompt,
                            'clip_reward': reward,  # CLIPì€ original user promptë¡œ ê³„ì‚°ë¨
                            'epoch': epoch
                        }
                    }
                    batch_experiences.append(experience)
            
            all_experiences.extend(batch_experiences)
        
        logger.info(f"\nğŸ“Š ì—í¬í¬ {epoch} ì´ ìˆ˜ì§‘ëœ ê²½í—˜: {len(all_experiences)}ê°œ")
        return all_experiences
    
    def update_policy(self, experiences: List[Dict]) -> Dict:
        """Group-relative ì •ì±… ì—…ë°ì´íŠ¸ (GPU 0ì—ì„œ ì‹¤í–‰)"""
        logger.info(f"ğŸ”„ Group-relative ì •ì±… ì—…ë°ì´íŠ¸ ({len(experiences)}ê°œ ê²½í—˜)")
        
        # Group-relative baseline ê³„ì‚°
        group_baseline_metrics = self.calculate_group_relative_baseline(experiences)
        
        # í–¥ìƒëœ ê²½í—˜ ë°ì´í„°ë¡œ ì •ì±… ì—…ë°ì´íŠ¸
        enhanced_experiences = self.apply_group_relative_advantages(experiences, group_baseline_metrics)
        
        # QWEN ëª¨ë¸ ì—…ë°ì´íŠ¸
        metrics = self.qwen_model.update_grpo_policy(enhanced_experiences)
        
        # Group-relative ë©”íŠ¸ë¦­ ì¶”ê°€
        metrics.update(group_baseline_metrics)
        
        logger.info(f"âœ… Group-relative ì •ì±… ì—…ë°ì´íŠ¸ ì™„ë£Œ")
        return metrics
    
    def calculate_group_relative_baseline(self, experiences: List[Dict]) -> Dict:
        """Group-relative baseline ê³„ì‚°"""
        if not experiences:
            return {}
        
        # ê·¸ë£¹ë³„ ë¦¬ì›Œë“œ ìˆ˜ì§‘
        user_prompt_groups = {}
        for exp in experiences:
            user_prompt = exp['user_prompt']
            if user_prompt not in user_prompt_groups:
                user_prompt_groups[user_prompt] = []
            user_prompt_groups[user_prompt].append(exp['reward'])
        
        # ê·¸ë£¹ë³„ í†µê³„ ê³„ì‚°
        group_stats = {}
        all_rewards = []
        
        for user_prompt, rewards in user_prompt_groups.items():
            group_mean = sum(rewards) / len(rewards)
            group_std = (sum((r - group_mean) ** 2 for r in rewards) / len(rewards)) ** 0.5
            
            group_stats[user_prompt] = {
                'mean': group_mean,
                'std': group_std,
                'count': len(rewards),
                'rewards': rewards
            }
            all_rewards.extend(rewards)
        
        # ì „ì²´ í†µê³„
        global_mean = sum(all_rewards) / len(all_rewards)
        global_std = (sum((r - global_mean) ** 2 for r in all_rewards) / len(all_rewards)) ** 0.5
        
        logger.info(f"ğŸ“Š Group-relative Baseline í†µê³„:")
        logger.info(f"  ì „ì²´ í‰ê· : {global_mean:.4f} Â± {global_std:.4f}")
        logger.info(f"  ê·¸ë£¹ ìˆ˜: {len(group_stats)}")
        
        for prompt, stats in group_stats.items():
            logger.info(f"  '{prompt[:30]}...': {stats['mean']:.4f} Â± {stats['std']:.4f} (n={stats['count']})")
        
        return {
            'global_mean': global_mean,
            'global_std': global_std,
            'group_stats': group_stats,
            'num_groups': len(group_stats)
        }
    
    def apply_group_relative_advantages(self, experiences: List[Dict], baseline_metrics: Dict) -> List[Dict]:
        """Group-relative advantage ì ìš©"""
        if not baseline_metrics:
            return experiences
        
        enhanced_experiences = []
        group_stats = baseline_metrics['group_stats']
        global_mean = baseline_metrics['global_mean']
        
        for exp in experiences:
            user_prompt = exp['user_prompt']
            reward = exp['reward']
            
            # Group-relative advantage ê³„ì‚°
            if user_prompt in group_stats:
                group_mean = group_stats[user_prompt]['mean']
                group_advantage = reward - group_mean
            else:
                group_advantage = reward - global_mean
            
            # Global advantageë„ ê³„ì‚°
            global_advantage = reward - global_mean
            
            # ê²½í—˜ ë°ì´í„°ì— advantage ì •ë³´ ì¶”ê°€
            enhanced_exp = exp.copy()
            enhanced_exp['group_advantage'] = group_advantage
            enhanced_exp['global_advantage'] = global_advantage
            enhanced_exp['group_baseline'] = group_stats.get(user_prompt, {}).get('mean', global_mean)
            enhanced_exp['global_baseline'] = global_mean
            
            enhanced_experiences.append(enhanced_exp)
        
        logger.info(f"âœ… Group-relative advantage ì ìš© ì™„ë£Œ")
        return enhanced_experiences
    
    def save_episode_images(self, epoch: int, user_prompt: str, enhanced_prompts: List[str], 
                           images: List, rewards: List[float]):
        """ì—í”¼ì†Œë“œë³„ ì´ë¯¸ì§€ ì €ì¥"""
        try:
            epoch_dir = os.path.join(self.log_dir, "images", f"epoch_{epoch}")
            os.makedirs(epoch_dir, exist_ok=True)
            
            for i, (enhanced_prompt, image, reward) in enumerate(zip(enhanced_prompts, images, rewards)):
                # ì´ë¯¸ì§€ ì €ì¥
                image_path = os.path.join(epoch_dir, f"image_{i}_reward_{reward:.3f}.png")
                image.save(image_path)
                
                # í”„ë¡¬í”„íŠ¸ ì •ë³´ ì €ì¥
                info_path = os.path.join(epoch_dir, f"image_{i}_info.txt")
                with open(info_path, 'w', encoding='utf-8') as f:
                    f.write(f"Original Prompt: {user_prompt}\n")
                    f.write(f"Enhanced Prompt: {enhanced_prompt}\n")
                    f.write(f"CLIP Reward: {reward:.4f}\n")
                    f.write(f"Epoch: {epoch}\n")
                    f.write(f"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            
            logger.info(f"ğŸ’¾ ì—í”¼ì†Œë“œ {epoch} ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ: {len(images)}ê°œ")
            
        except Exception as e:
            logger.error(f"âŒ ì´ë¯¸ì§€ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def plot_training_metrics(self, epoch: int):
        """í•™ìŠµ ë©”íŠ¸ë¦­ í”Œë¡¯ ìƒì„±"""
        try:
            if not self.training_metrics['epoch_rewards']:
                return
            
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle(f'GRPO Training Metrics - Epoch {epoch}', fontsize=16)
            
            # 1. ë¦¬ì›Œë“œ ì¶”ì´
            axes[0, 0].plot(self.training_metrics['epoch_rewards'], 'b-o', linewidth=2, markersize=6)
            axes[0, 0].set_title('Average Reward per Epoch')
            axes[0, 0].set_xlabel('Epoch')
            axes[0, 0].set_ylabel('Average Reward')
            axes[0, 0].grid(True, alpha=0.3)
            
            # 2. Policy Loss ì¶”ì´
            if self.training_metrics['policy_losses']:
                axes[0, 1].plot(self.training_metrics['policy_losses'], 'r-o', linewidth=2, markersize=6)
                axes[0, 1].set_title('Policy Loss per Epoch')
                axes[0, 1].set_xlabel('Epoch')
                axes[0, 1].set_ylabel('Policy Loss')
                axes[0, 1].grid(True, alpha=0.3)
            
            # 3. KL Divergence ì¶”ì´
            if self.training_metrics['kl_divergences']:
                axes[1, 0].plot(self.training_metrics['kl_divergences'], 'g-o', linewidth=2, markersize=6)
                axes[1, 0].set_title('KL Divergence per Epoch')
                axes[1, 0].set_xlabel('Epoch')
                axes[1, 0].set_ylabel('KL Divergence')
                axes[1, 0].grid(True, alpha=0.3)
            
            # 4. Advantage ë¶„í¬ (ìµœê·¼ ì—í”¼ì†Œë“œ)
            if self.training_metrics['advantages']:
                recent_advantages = self.training_metrics['advantages'][-50:]  # ìµœê·¼ 50ê°œ
                axes[1, 1].hist(recent_advantages, bins=20, alpha=0.7, color='purple', edgecolor='black')
                axes[1, 1].set_title('Recent Advantage Distribution')
                axes[1, 1].set_xlabel('Advantage Value')
                axes[1, 1].set_ylabel('Frequency')
                axes[1, 1].grid(True, alpha=0.3)
            
            plt.tight_layout()
            
            # í”Œë¡¯ ì €ì¥
            plot_path = os.path.join(self.log_dir, "plots", f"training_metrics_epoch_{epoch}.png")
            plt.savefig(plot_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info(f"ğŸ“Š ì—í”¼ì†Œë“œ {epoch} ë©”íŠ¸ë¦­ í”Œë¡¯ ì €ì¥ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ í”Œë¡¯ ìƒì„± ì‹¤íŒ¨: {e}")
    
    def log_epoch_metrics(self, epoch: int, experiences: List[Dict], metrics: Dict):
        """ì—í”¼ì†Œë“œ ë©”íŠ¸ë¦­ ë¡œê¹…"""
        # ë¦¬ì›Œë“œ í†µê³„
        rewards = [exp['reward'] for exp in experiences]
        avg_reward = sum(rewards) / len(rewards) if rewards else 0.0
        
        # ë©”íŠ¸ë¦­ ì €ì¥
        self.training_metrics['epoch_rewards'].append(avg_reward)
        if 'policy_loss' in metrics:
            self.training_metrics['policy_losses'].append(metrics['policy_loss'])
        if 'kl_div' in metrics:
            self.training_metrics['kl_divergences'].append(metrics['kl_div'])
        
        # Advantage ì €ì¥
        for exp in experiences:
            if 'group_advantage' in exp:
                self.training_metrics['advantages'].append(exp['group_advantage'])
        
        # ìƒì„¸ ë¡œê·¸ ì¶œë ¥
        logger.info(f"ğŸ“ˆ ì—í”¼ì†Œë“œ {epoch} ìƒì„¸ ë©”íŠ¸ë¦­:")
        logger.info(f"  ğŸ“Š í‰ê·  ë¦¬ì›Œë“œ: {avg_reward:.4f}")
        logger.info(f"  ğŸ“Š ë¦¬ì›Œë“œ ë²”ìœ„: {min(rewards):.4f} ~ {max(rewards):.4f}")
        logger.info(f"  ğŸ“Š ê²½í—˜ ìˆ˜: {len(experiences)}")
        
        if metrics:
            for key, value in metrics.items():
                logger.info(f"  ğŸ“Š {key}: {value:.4f}")
        
        # CSV ë¡œê·¸ ì €ì¥
        csv_path = os.path.join(self.log_dir, "training_log.csv")
        if not os.path.exists(csv_path):
            with open(csv_path, 'w') as f:
                f.write("epoch,avg_reward,min_reward,max_reward,num_experiences,policy_loss,kl_div\n")
        
        with open(csv_path, 'a') as f:
            policy_loss = metrics.get('policy_loss', 0.0)
            kl_div = metrics.get('kl_div', 0.0)
            f.write(f"{epoch},{avg_reward:.4f},{min(rewards):.4f},{max(rewards):.4f},"
                   f"{len(experiences)},{policy_loss:.4f},{kl_div:.4f}\n")
    
    def train(self, num_epochs: int = 5):
        """ë©”ì¸ í•™ìŠµ ë£¨í”„"""
        logger.info(f"ğŸš€ Simple GRPO í•™ìŠµ ì‹œì‘ ({num_epochs} ì—í¬í¬)")
        logger.info("=" * 60)
        
        training_prompts = get_training_prompts()
        
        for epoch in range(num_epochs):
            epoch_start_time = time.time()
            logger.info(f"\nğŸ¯ ì—í¬í¬ {epoch + 1}/{num_epochs}")
            
            try:
                # ë¡¤ì•„ì›ƒ ìˆ˜ì§‘ (ì´ë¯¸ì§€ ì €ì¥ í¬í•¨)
                experiences = self.collect_rollouts_with_logging(training_prompts, epoch + 1)
                
                if not experiences:
                    logger.warning(f"âš ï¸ ì—í¬í¬ {epoch + 1}: ê²½í—˜ ì—†ìŒ, ê±´ë„ˆë›°ê¸°")
                    continue
                
                # ì •ì±… ì—…ë°ì´íŠ¸
                metrics = self.update_policy(experiences)
                
                # ì—í¬í¬ ì‹œê°„ ê¸°ë¡
                epoch_time = time.time() - epoch_start_time
                self.training_metrics['epoch_times'].append(epoch_time)
                
                # ìƒì„¸ ë©”íŠ¸ë¦­ ë¡œê¹…
                self.log_epoch_metrics(epoch + 1, experiences, metrics)
                
                # í”Œë¡¯ ìƒì„± (ë§¤ ì—í¬í¬ë§ˆë‹¤)
                self.plot_training_metrics(epoch + 1)
                
                # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                if self.use_gpu:
                    for gpu_id in range(min(3, torch.cuda.device_count())):
                        with torch.cuda.device(gpu_id):
                            torch.cuda.empty_cache()
                
                logger.info(f"â±ï¸ ì—í¬í¬ {epoch + 1} ì™„ë£Œ ì‹œê°„: {epoch_time:.2f}ì´ˆ")
                
            except Exception as e:
                logger.error(f"âŒ ì—í¬í¬ {epoch + 1} ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        logger.info("ğŸ‰ Simple GRPO í•™ìŠµ ì™„ë£Œ!")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger.info("ğŸš€ Simple Multi-GPU GRPO í•™ìŠµ ì‹œì‘")
    logger.info("=" * 60)
    logger.info("GPU í• ë‹¹:")
    logger.info("  GPU 0: QWEN LoRA Training")
    logger.info("  GPU 1: CLIP Reward Calculation")
    logger.info("  GPU 2: Stable Diffusion 3 Image Generation")
    logger.info("=" * 60)
    
    # ì„¤ì • - ê³ ì„±ëŠ¥ LoRA ìµœì í™”
    config = QWENGRPOConfig(
        learning_rate=2e-4,  # ë” ë†’ì€ í•™ìŠµë¥  (í™•ì¥ëœ LoRAì— ë§ì¶¤)
        batch_size=3,  # ë°°ì¹˜ í¬ê¸° ì¦ê°€ (ë©”ëª¨ë¦¬ ì—¬ìœ ë¶„ í™œìš©)
        num_rollouts=3,  # ë¡¤ì•„ì›ƒ ìˆ˜ ì¦ê°€
        max_prompt_length=77,
        max_new_tokens=30,  # í† í° ìˆ˜ ì¦ê°€
        temperature=1.2,
        top_p=0.9,
        top_k=100,
        kl_coef=0.02,
        clip_ratio=0.2,
        entropy_coef=0.01,
        save_images=True,
        log_dir="grpo_enhanced_results"  # ìƒˆë¡œìš´ ê²°ê³¼ ë””ë ‰í† ë¦¬
    )
    
    try:
        # íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
        trainer = SimpleGRPOTrainer(config)
        
        # í•™ìŠµ ì‹¤í–‰
        trainer.train(num_epochs=3)
        
    except Exception as e:
        logger.error(f"âŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                with torch.cuda.device(i):
                    torch.cuda.empty_cache()
            logger.info("ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")

if __name__ == "__main__":
    main()
