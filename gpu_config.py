"""
Multi-GPU Configuration for GRPO Training
=========================================

GPU 3ê°œ (1, 2, 3ë²ˆ)ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ í™œìš©í•˜ê¸° ìœ„í•œ ì„¤ì • ëª¨ë“ˆì…ë‹ˆë‹¤.

GPU ë¶„ë°° ì „ëµ:
- GPU 1: QWEN VL ëª¨ë¸ (ì •ì±… ë„¤íŠ¸ì›Œí¬)
- GPU 2: Stable Diffusion 3 (ì´ë¯¸ì§€ ìƒì„±)
- GPU 3: CLIP ëª¨ë¸ (ë³´ìƒ ê³„ì‚°)

Author: AI Assistant
Date: 2025-01-22
"""

import os
import torch
import logging
from typing import Dict, List

logger = logging.getLogger(__name__)

class MultiGPUConfig:
    """ë©€í‹° GPU ì„¤ì • ë° ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, gpu_ids: List[int] = [1, 2, 3]):
        """
        Multi-GPU ì„¤ì • ì´ˆê¸°í™”
        
        Args:
            gpu_ids: ì‚¬ìš©í•  GPU ID ë¦¬ìŠ¤íŠ¸
        """
        self.gpu_ids = gpu_ids
        self.setup_environment()
        self.device_map = self.create_device_map()
        
    def setup_environment(self):
        """í™˜ê²½ ë³€ìˆ˜ ë° CUDA ì„¤ì •"""
        # GPU í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
        gpu_env_vars = {
            'CUDA_VISIBLE_DEVICES': ','.join(map(str, self.gpu_ids)),
            'CUDA_LAUNCH_BLOCKING': '0',  # ë¹„ë™ê¸° ì‹¤í–‰ í™œì„±í™”
            'TORCH_CUDA_ARCH_LIST': '8.0;8.6;8.9;9.0',  # ìµœì‹  ì•„í‚¤í…ì²˜ ì§€ì›
            'PYTORCH_CUDA_ALLOC_CONF': 'max_split_size_mb:512',  # ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”
            'NCCL_DEBUG': 'INFO',  # Multi-GPU í†µì‹  ë””ë²„ê¹…
            'NCCL_IB_DISABLE': '1',  # InfiniBand ë¹„í™œì„±í™” (í•„ìš”ì‹œ)
        }
        
        # í™˜ê²½ ë³€ìˆ˜ ì ìš©
        for key, value in gpu_env_vars.items():
            os.environ[key] = value
            logger.info(f"ğŸ”§ {key} = {value}")
        
        logger.info("ğŸš€ Multi-GPU environment configured")
        
    def create_device_map(self) -> Dict[str, str]:
        """ëª¨ë¸ë³„ GPU ë””ë°”ì´ìŠ¤ ë§µ ìƒì„±"""
        if not torch.cuda.is_available():
            logger.warning("âš ï¸ CUDA not available, falling back to CPU")
            return {
                'qwen': 'cpu',
                'sd3': 'cpu', 
                'clip': 'cpu'
            }
        
        available_gpus = torch.cuda.device_count()
        logger.info(f"ğŸ“± Available GPUs: {available_gpus}")
        
        if available_gpus >= 3:
            device_map = {
                'qwen': f'cuda:0',  # ì‹¤ì œë¡œëŠ” GPU 1
                'sd3': f'cuda:1',   # ì‹¤ì œë¡œëŠ” GPU 2
                'clip': f'cuda:2'   # ì‹¤ì œë¡œëŠ” GPU 3
            }
        elif available_gpus == 2:
            device_map = {
                'qwen': f'cuda:0',
                'sd3': f'cuda:1', 
                'clip': f'cuda:0'  # QWENê³¼ ê³µìœ 
            }
        else:
            device_map = {
                'qwen': f'cuda:0',
                'sd3': f'cuda:0',
                'clip': f'cuda:0'
            }
        
        logger.info(f"ğŸ—ºï¸ Device mapping: {device_map}")
        return device_map
    
    def get_device(self, model_name: str) -> str:
        """ëª¨ë¸ë³„ ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
        return self.device_map.get(model_name, 'cuda:0')
    
    def print_gpu_status(self):
        """GPU ìƒíƒœ ì¶œë ¥"""
        if not torch.cuda.is_available():
            logger.info("âŒ CUDA not available")
            return
            
        logger.info(f"ğŸ” GPU Status:")
        for i in range(torch.cuda.device_count()):
            gpu_name = torch.cuda.get_device_name(i)
            memory_total = torch.cuda.get_device_properties(i).total_memory / 1024**3
            memory_allocated = torch.cuda.memory_allocated(i) / 1024**3
            memory_cached = torch.cuda.memory_reserved(i) / 1024**3
            
            logger.info(f"  GPU {i} ({gpu_name}):")
            logger.info(f"    Total Memory: {memory_total:.1f} GB")
            logger.info(f"    Allocated: {memory_allocated:.1f} GB")
            logger.info(f"    Cached: {memory_cached:.1f} GB")
            logger.info(f"    Free: {memory_total - memory_cached:.1f} GB")
    
    def clear_gpu_memory(self):
        """GPU ë©”ëª¨ë¦¬ ì •ë¦¬"""
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                with torch.cuda.device(i):
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
            logger.info("ğŸ§¹ GPU memory cleared")
    
    def set_memory_fraction(self, fraction: float = 0.9):
        """GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì œí•œ"""
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                torch.cuda.set_per_process_memory_fraction(fraction, device=i)
            logger.info(f"ğŸ“Š GPU memory fraction set to {fraction}")

# ì „ì—­ ì„¤ì • ì¸ìŠ¤í„´ìŠ¤
gpu_config = MultiGPUConfig([1, 2, 3])

def setup_multi_gpu():
    """ë©€í‹° GPU ì„¤ì • í•¨ìˆ˜"""
    global gpu_config
    gpu_config.clear_gpu_memory()
    gpu_config.set_memory_fraction(0.85)  # 85% ë©”ëª¨ë¦¬ ì‚¬ìš©
    gpu_config.print_gpu_status()
    return gpu_config

def get_device_for_model(model_name: str) -> str:
    """ëª¨ë¸ë³„ ë””ë°”ì´ìŠ¤ ë°˜í™˜ í•¨ìˆ˜"""
    return gpu_config.get_device(model_name) 